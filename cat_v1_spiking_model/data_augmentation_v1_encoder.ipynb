{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters, RunningStats\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop\n",
    "\n",
    "from data import (\n",
    "    prepare_v1_dataloaders,\n",
    "    SyntheticDataset,\n",
    "    BatchPatchesDataLoader,\n",
    "    MixedBatchLoader,\n",
    "    PerSampleStoredDataset,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # \"device\": \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    # \"crop\": True,\n",
    "    \"batch_size\": 64,\n",
    "    \"stim_normalize_mean\": 46.236,\n",
    "    \"stim_normalize_std\": 21.196,\n",
    "    \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "    \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data loaders\n",
    "v1_dataloaders = prepare_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "stim, resp = next(iter(v1_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{stim.shape=}, {resp.shape=}\"\n",
    "    f\"\\n{stim.min()=}, {stim.max()=}\"\n",
    "    f\"\\n{resp.min()=}, {resp.max()=}\"\n",
    "    f\"\\n{stim.mean()=}, {stim.std()=}\"\n",
    "    f\"\\n{resp.mean()=}, {resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "from data_orig import prepare_spiking_data_loaders\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "### config only for the encoder\n",
    "spiking_data_loaders_config = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "encoder_config = {\n",
    "    \"init_mu_range\": 0.55,\n",
    "    \"init_sigma\": 0.4,\n",
    "    \"input_kern\": 19,\n",
    "    \"hidden_kern\": 17,\n",
    "    \"hidden_channels\": 32,\n",
    "    \"gamma_input\": 1.0,\n",
    "    \"gamma_readout\": 2.439,\n",
    "    \"grid_mean_predictor\": None,\n",
    "    \"layers\": 5\n",
    "}\n",
    "\n",
    "### encoder\n",
    "_dataloaders = prepare_spiking_data_loaders(**spiking_data_loaders_config)\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_config,\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=2,\n",
    ").float()\n",
    "del _dataloaders\n",
    "\n",
    "### load pretrained core\n",
    "pretrained_core = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"spiking_scratch_tunecore_68Y_model.pth\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "encoder.load_state_dict(pretrained_core, strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = \"test\" ## TODO: when \"train\", turn of shuffle\n",
    "save_stats = False\n",
    "\n",
    "trans_to_apply = [\n",
    "    {\n",
    "        \"name\": \"original\",\n",
    "        \"stim\": lambda x: x,\n",
    "        \"resp\": lambda x: x,\n",
    "        \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part),\n",
    "        \"sample_idx\": 0,\n",
    "        \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    },\n",
    "    # { ### noise to resp\n",
    "    #     \"name\": \"01noise_resp\n",
    "    #     \"stim\": lambda x: x,\n",
    "    #     \"resp\": lambda x: x + torch.randn(x.shape) * 0.1,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_01noise_resp\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### flip stim\n",
    "    #     \"name\": \"flip_stim\",\n",
    "    #     \"stim\": lambda x: x.flip(2),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_flip_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"01rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 1, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_01rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"02rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 2, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_02rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"03rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 3, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_03rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "]\n",
    "\n",
    "### create dirs\n",
    "for tran_to_apply in trans_to_apply:\n",
    "    if os.path.exists(tran_to_apply[\"save_dir\"]) and len(os.listdir(tran_to_apply[\"save_dir\"])) > 0:\n",
    "        print(f\"[WARNING]: {tran_to_apply['save_dir']} already exists and is not empty.\")\n",
    "    os.makedirs(tran_to_apply[\"save_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(v1_dataloaders[data_part])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### run\n",
    "    for batch_idx, (stim, _) in enumerate(v1_dataloaders[data_part]):\n",
    "        for tran_to_apply in trans_to_apply:\n",
    "            stim = tran_to_apply[\"stim\"](stim.to(config[\"device\"]))\n",
    "\n",
    "            ### forward\n",
    "            resp = encoder(stim)\n",
    "            resp = tran_to_apply[\"resp\"](resp)\n",
    "            if save_stats:\n",
    "                tran_to_apply[\"stats\"].update(resp)\n",
    "\n",
    "            ### save\n",
    "            for i in range(stim.shape[0]):\n",
    "                sample_path = os.path.join(tran_to_apply[\"save_dir\"], f\"{tran_to_apply['sample_idx']}.pickle\")\n",
    "                with open(sample_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"stim\": stim[i].cpu(),\n",
    "                        \"resp\": resp[i].cpu(),\n",
    "                    }, f)\n",
    "                tran_to_apply[\"sample_idx\"] += 1\n",
    "        \n",
    "        ### log\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{n_batches}\")\n",
    "\n",
    "## save stats\n",
    "if save_stats:\n",
    "    for tran_to_apply in trans_to_apply:\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", f\"responses_mean_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_mean().cpu().numpy(),\n",
    "        )\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", f\"responses_std_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_std().cpu().numpy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_mean_original.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_std_original.npy\"))).float()\n",
    "\n",
    "dataset = PerSampleStoredDataset(\n",
    "    dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"train\"),\n",
    "    stim_transform=lambda x: x,\n",
    "    resp_transform=csng.utils.Normalize(\n",
    "        mean=resp_mean,\n",
    "        std=resp_std,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = RunningStats(num_components=50 * 50, lib=\"torch\", device=\"cpu\")\n",
    "for b, (s, r) in enumerate(dataloader):\n",
    "    stats.update(s.view(s.shape[0], -1))\n",
    "    if b % 50 == 0:\n",
    "        print(f\"Batch {b} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_mean(), stats.get_std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
