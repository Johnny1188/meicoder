{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.GAN import GAN\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop\n",
    "\n",
    "from mypkg.visualization import LivePlot\n",
    "\n",
    "from data import (\n",
    "    prepare_v1_dataloaders,\n",
    "    SyntheticDataset,\n",
    "    BatchPatchesDataLoader,\n",
    "    MixedBatchLoader,\n",
    "    PerSampleStoredDataset,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1 dataset (spiking model of cat V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    # \"crop\": True,\n",
    "    # \"batch_size\": 64,\n",
    "    \"batch_size\": 20,\n",
    "    \"stim_normalize_mean\": 46.236,\n",
    "    \"stim_normalize_std\": 21.196,\n",
    "    \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "    \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data loaders\n",
    "dataloaders[\"v1_data\"] = prepare_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "stim_sample, resp_sample = next(iter(dataloaders[\"v1_data\"][\"val\"]))\n",
    "print(\n",
    "    f\"{stim_sample.shape=}, {resp_sample.shape=}\"\n",
    "    f\"\\n{stim_sample.min()=}, {stim_sample.max()=}\"\n",
    "    f\"\\n{resp_sample.min()=}, {resp_sample.max()=}\"\n",
    "    f\"\\n{stim_sample.mean()=}, {stim_sample.std()=}\"\n",
    "    f\"\\n{resp_sample.mean()=}, {resp_sample.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim_sample[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim_sample[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp_sample[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data (different stimuli dataset -> encoder -> neuronal responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data\", \"responses_mean.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data\", \"responses_std.npy\"))).float()\n",
    "\n",
    "config[\"data\"][\"syn_data\"] = {\n",
    "    \"dataset\": {\n",
    "        \"stim_transform\": transforms.Normalize(\n",
    "            mean=114.457,\n",
    "            std=51.356,\n",
    "        ),\n",
    "        \"resp_transform\": csng.utils.Normalize(\n",
    "            mean=resp_mean,\n",
    "            std=resp_std,\n",
    "        ),\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 20,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets = {\n",
    "    \"train\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"train\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"val\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"val\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"test\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"test\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataloaders[\"syn_data\"] = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=syn_datasets[\"train\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        dataset=syn_datasets[\"val\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=syn_datasets[\"test\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim_sample, syn_resp_sample = next(iter(dataloaders[\"syn_data\"][\"val\"]))\n",
    "print(\n",
    "    f\"{syn_stim_sample.shape=}, {syn_resp_sample.shape=}\"\n",
    "    f\"\\n{syn_stim_sample.min()=}, {syn_stim_sample.max()=}\"\n",
    "    f\"\\n{syn_resp_sample.min()=}, {syn_resp_sample.max()=}\"\n",
    "    f\"\\n{syn_stim_sample.mean()=}, {syn_stim_sample.std()=}\"\n",
    "    f\"\\n{syn_resp_sample.mean()=}, {syn_resp_sample.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim_sample.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(syn_stim_sample.cpu()[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(syn_resp_sample.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data (V1 data stimuli -> encoder -> neuronal responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_mean_original.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_std_original.npy\"))).float()\n",
    "\n",
    "config[\"data\"][\"syn_data_v1_enc\"] = {\n",
    "    \"dataset\": {\n",
    "        \"stim_transform\": transforms.Normalize(\n",
    "            mean=0,\n",
    "            std=1,\n",
    "        ),\n",
    "        \"resp_transform\": csng.utils.Normalize(\n",
    "            mean=resp_mean,\n",
    "            std=resp_std,\n",
    "        ),\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 20,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets_v1_encoder = {\n",
    "    \"train\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"train\"),\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataset\"]\n",
    "    ),\n",
    "    \"val\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"val\"),\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataset\"]\n",
    "    ),\n",
    "    \"test\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"test\"),\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataset\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataloaders[\"syn_data_v1_enc\"] = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=syn_datasets_v1_encoder[\"train\"],\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        dataset=syn_datasets_v1_encoder[\"val\"],\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=syn_datasets_v1_encoder[\"test\"],\n",
    "        **config[\"data\"][\"syn_data_v1_enc\"][\"dataloader\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim_v1_enc, syn_resp_v1_enc = next(iter(dataloaders[\"syn_data_v1_enc\"][\"val\"]))\n",
    "print(\n",
    "    f\"{syn_stim_v1_enc.shape=}, {syn_resp_v1_enc.shape=}\"\n",
    "    f\"\\n{syn_stim_v1_enc.min()=}, {syn_stim_v1_enc.max()=}\"\n",
    "    f\"\\n{syn_resp_v1_enc.min()=}, {syn_resp_v1_enc.max()=}\"\n",
    "    f\"\\n{syn_stim_v1_enc.mean()=}, {syn_stim_v1_enc.std()=}\"\n",
    "    f\"\\n{syn_resp_v1_enc.mean()=}, {syn_resp_v1_enc.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim_v1_enc.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(syn_stim_v1_enc.cpu()[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(syn_resp_v1_enc.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, opter, loss_fn, config, l1_reg_mul=0, l2_reg_mul=0, verbose=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "    ### run\n",
    "    for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "        ### data\n",
    "        stim = stim.to(config[\"device\"])\n",
    "        resp = resp.to(config[\"device\"])\n",
    "        \n",
    "        ### train\n",
    "        opter.zero_grad()\n",
    "        stim_pred = model(resp)\n",
    "        loss = loss_fn(stim_pred, stim)\n",
    "\n",
    "        ### regularization\n",
    "        if l1_reg_mul != 0:\n",
    "            l1_reg = sum(p.abs().sum() for n, p in model.named_parameters() if p.requires_grad and \"weight\" in n)\n",
    "            loss += l1_reg_mul * l1_reg\n",
    "        if l2_reg_mul != 0:\n",
    "            l2_reg = sum(p.pow(2.0).sum() for n, p in model.named_parameters() if p.requires_grad and \"weight\" in n)\n",
    "            loss += l2_reg_mul * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        opter.step()\n",
    "        \n",
    "        ### log\n",
    "        train_loss += loss.item()\n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            print(f\"Training progress: [{batch_idx}/{n_batches} ({100. * batch_idx / n_batches:.0f}%)]\"\n",
    "                  f\"  Loss: {loss.item():.6f}\")\n",
    "        batch_idx += 1\n",
    "\n",
    "    train_loss /= n_batches\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, dataloader, loss_fn, config):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "            stim = stim.to(config[\"device\"])\n",
    "            resp = resp.to(config[\"device\"])\n",
    "            \n",
    "            stim_pred = model(resp)\n",
    "            loss = loss_fn(stim_pred, stim)\n",
    "            \n",
    "            ### log\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(dataloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, dataloaders, use_data_names, only_v1_data_eval=True):\n",
    "    if only_v1_data_eval:\n",
    "        val_dataloader = dataloaders[\"v1_data\"][\"val\"]\n",
    "\n",
    "    ### get dataloaders to mix\n",
    "    dataloaders_to_mix = []\n",
    "    for data_name in use_data_names:\n",
    "        dataloaders_to_mix.append(dataloaders[data_name])\n",
    "\n",
    "    if len(dataloaders_to_mix) > 1:\n",
    "        train_dataloader = MixedBatchLoader(\n",
    "            dataloaders=[dl[\"train\"] for dl in dataloaders_to_mix],\n",
    "            mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "            device=config[\"device\"],\n",
    "        )\n",
    "        if not only_v1_data_eval:\n",
    "            val_dataloader = MixedBatchLoader(\n",
    "                dataloaders=[dl[\"val\"] for dl in dataloaders_to_mix],\n",
    "                mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "                device=config[\"device\"],\n",
    "            )\n",
    "    elif len(dataloaders_to_mix) == 1:\n",
    "        train_dataloader = dataloaders_to_mix[0][\"train\"]\n",
    "        if not only_v1_data_eval:\n",
    "            val_dataloader = dataloaders_to_mix[0][\"val\"]\n",
    "    else:\n",
    "        raise ValueError(\"No data to train on.\")\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"decoder\"] = {\n",
    "    \"model\": {\n",
    "        \"G_kwargs\": {\n",
    "            \"in_shape\": resp_sample.shape[1:],\n",
    "            # \"layers\": [\n",
    "            #     (\"fc\", 637),\n",
    "            #     (\"unflatten\", 1, (13, 7, 7)),\n",
    "            #     (\"deconv\", 256, 7, 2, 0),\n",
    "            #     (\"deconv\", 128, 5, 2, 0),\n",
    "            #     (\"deconv\", 64, 5, 1, 0),\n",
    "            #     (\"deconv\", 64, 4, 1, 0),\n",
    "            #     (\"deconv\", 1, 3, 1, 0),\n",
    "            # ],\n",
    "            \"layers\": [\n",
    "                (\"fc\", 384),  # CNN Baseline\n",
    "                (\"unflatten\", 1, (6, 8, 8)),  # CNN Baseline\n",
    "                (\"deconv\", 256, 7, 2, 0),\n",
    "                (\"deconv\", 128, 5, 2, 0),\n",
    "                (\"deconv\", 64, 4, 1, 0),  # CNN Baseline\n",
    "                (\"deconv\", 1, 3, 1, 0),\n",
    "            ],\n",
    "            \"act_fn\": nn.ReLU,\n",
    "            # \"out_act_fn\": nn.Tanh,\n",
    "            \"out_act_fn\": nn.Identity,\n",
    "            \"dropout\": 0.2,\n",
    "            \"batch_norm\": True,\n",
    "        },\n",
    "        \"D_kwargs\": {\n",
    "            \"in_shape\": crop_stim(stim_sample).shape[1:],\n",
    "            \"layers\": [\n",
    "                (\"conv\", 128, 4, 1, 2),\n",
    "                # (\"conv\", 128, 4, 2, 1),\n",
    "                (\"conv\", 128, 4, 1, 0),\n",
    "                # (\"conv\", 64, 4, 1, 0),\n",
    "                (\"conv\", 64, 4, 1, 0),\n",
    "                (\"conv\", 32, 3, 1, 0),\n",
    "                (\"fc\", 1),\n",
    "            ],\n",
    "            \"act_fn\": nn.ReLU,\n",
    "            \"out_act_fn\": nn.Sigmoid,\n",
    "            # \"out_act_fn\": nn.Identity,\n",
    "            \"dropout\": 0.3,\n",
    "            \"batch_norm\": True,\n",
    "        },\n",
    "        \"G_optim_kwargs\": {\"lr\": 1e-4, \"betas\": (0.5, 0.999)},\n",
    "        \"D_optim_kwargs\": {\"lr\": 5e-5, \"betas\": (0.5, 0.999)},\n",
    "    },\n",
    "    \"stim_loss_fn\": SSIMLoss(\n",
    "        window=config[\"stim_crop_win\"],\n",
    "        log_loss=True,\n",
    "        inp_normalized=True,\n",
    "    ),\n",
    "    # \"l1_reg_mul\": 0,\n",
    "    # \"l2_reg_mul\": 5e-5,\n",
    "    \"n_epochs\": 150,\n",
    "    \"save_run\": True,\n",
    "}\n",
    "\n",
    "gan = GAN(**config[\"decoder\"][\"model\"]).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare checkpointing\n",
    "if config[\"decoder\"][\"save_run\"]:\n",
    "    ### save config\n",
    "    run_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    config[\"dir\"] = os.path.join(DATA_PATH, \"models\", \"gan\", run_name)\n",
    "    os.makedirs(config[\"dir\"], exist_ok=True)\n",
    "    with open(os.path.join(config[\"dir\"], \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4, default=str)\n",
    "    os.makedirs(os.path.join(config[\"dir\"], \"samples\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(config[\"dir\"], \"ckpt\"), exist_ok=True)\n",
    "    make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "        config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    "    )\n",
    "\n",
    "    print(f\"Run name: {run_name}\\nRun dir: {config['dir']}\")\n",
    "else:\n",
    "    make_sample_path = lambda epoch, prefix: None\n",
    "    print(\"WARNING: Not saving the run and the config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load ckpt\n",
    "run_name = \"2023-08-27_14-13-45\"\n",
    "ckpt = torch.load(os.path.join(DATA_PATH, \"models\", \"gan\", run_name, \"ckpt\", \"decoder_15.pt\"))\n",
    "\n",
    "history = ckpt[\"history\"]\n",
    "config = ckpt[\"config\"]\n",
    "best = ckpt[\"best\"]\n",
    "\n",
    "gan = GAN(**config[\"decoder\"][\"model\"]).to(config[\"device\"])\n",
    "gan.load_state_dict(ckpt[\"decoder\"])\n",
    "\n",
    "make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "    config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show model\n",
    "with torch.no_grad():\n",
    "    print(\n",
    "        f\"Output shapes:\"\n",
    "        f\"\\n\\tG: {gan(resp_sample.to(config['device'])).cpu().shape}\"\n",
    "        f\"\\n\\tD: {gan.D(crop_stim(stim_sample).to(config['device'])).cpu().shape}\"\n",
    "    )\n",
    "print(\n",
    "    f\"Number of parameters:\"\n",
    "    f\"\\n\\tG: {count_parameters(gan.G)}\"\n",
    "    f\"\\n\\tD: {count_parameters(gan.D)}\"\n",
    ")\n",
    "gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, save_to=None):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_to:\n",
    "        fig.savefig(save_to)\n",
    "    ### save fig\n",
    "    if config[\"decoder\"][\"save_run\"]:\n",
    "        fig.savefig(os.path.join(config[\"dir\"], f\"losses_{epoch}.png\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_freq = 100\n",
    "data_names = list(dataloaders.keys())\n",
    "print(f\"{data_names=}\")\n",
    "\n",
    "history = {k: [] for k in (\n",
    "    \"val_loss\", \"D_loss\", \"G_loss\", \"G_loss_stim\", \"G_loss_adv\",\n",
    "    \"D_mean_abs_grad_first_layer\", \"D_mean_abs_grad_last_layer\",\n",
    "    \"G_mean_abs_grad_first_layer\", \"G_mean_abs_grad_last_layer\"\n",
    ")}\n",
    "live_plot = LivePlot(\n",
    "    figsize=(22, 24),\n",
    "    groups=[k for k in history.keys() if k not in [\"val_loss\"]],\n",
    "    use_seaborn=True,\n",
    ")\n",
    "best = {\"val_loss\": np.inf, \"epoch\": 0, \"decoder\": None}\n",
    "s, e = len(history[\"val_loss\"]), config[\"decoder\"][\"n_epochs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "gan.train()\n",
    "for epoch in range(s, e):\n",
    "    print(f\"[{epoch + 1}/{config['decoder']['n_epochs']}]\")\n",
    "\n",
    "    ### get data\n",
    "    train_dataloader, val_dataloader = get_dataloaders(\n",
    "        config=config,\n",
    "        dataloaders=dataloaders,\n",
    "        use_data_names=data_names,\n",
    "        only_v1_data_eval=config[\"only_v1_data_eval\"],\n",
    "    )\n",
    "\n",
    "    ### training epoch\n",
    "    for batch_idx, (stim, resp) in enumerate(train_dataloader):\n",
    "        resp = resp.to(config[\"device\"])\n",
    "        stim = stim.to(config[\"device\"])\n",
    "\n",
    "        ### update discriminator\n",
    "        gan.D_optim.zero_grad()\n",
    "        real_stim_pred = gan.D(crop_stim(stim))\n",
    "        ### add noise to labels (uniform distribution between 0.9 and 1)\n",
    "        noisy_real_stim_labels = torch.rand_like(real_stim_pred) * 0.05 + 0.95\n",
    "        # noisy_real_stim_labels = torch.ones_like(real_stim_pred)\n",
    "        real_stim_loss = torch.mean((real_stim_pred - noisy_real_stim_labels)**2) / 2.\n",
    "        # real_stim_loss = torch.mean((real_stim_pred - 1.)**2) / 2.\n",
    "\n",
    "        stim_pred = gan.G(resp)\n",
    "        fake_stim_pred = gan.D(crop_stim(stim_pred.detach()))\n",
    "        ### add noise to labels (uniform distribution between 0 and 0.1)\n",
    "        noisy_fake_stim_labels = torch.rand_like(fake_stim_pred) * 0.05\n",
    "        # noisy_fake_stim_labels = torch.zeros_like(fake_stim_pred)\n",
    "        fake_stim_loss = torch.mean((fake_stim_pred - noisy_fake_stim_labels)**2) / 2.\n",
    "        # fake_stim_loss = torch.mean(fake_stim_pred**2) / 2.\n",
    "\n",
    "        D_loss = real_stim_loss + fake_stim_loss\n",
    "        D_loss.backward()\n",
    "\n",
    "        ### clip gradients\n",
    "        for p in gan.D.parameters():\n",
    "            p.grad.data.clamp_(-1., 1.)\n",
    "\n",
    "        history[\"D_mean_abs_grad_first_layer\"].append(torch.mean(torch.abs(gan.D.layers[0].weight.grad)).item())\n",
    "        history[\"D_mean_abs_grad_last_layer\"].append(torch.mean(torch.abs(gan.D.layers[-2].weight.grad)).item())\n",
    "        gan.D_optim.step()\n",
    "\n",
    "        ### update generator\n",
    "        gan.G_optim.zero_grad()\n",
    "        stim_pred = gan.G(resp)\n",
    "        fake_stim_pred = gan.D(crop_stim(stim_pred))\n",
    "\n",
    "        G_loss_adv = torch.mean((fake_stim_pred - 1.)**2)\n",
    "        G_loss_stim = config[\"decoder\"][\"stim_loss_fn\"](stim_pred, stim) / 2\n",
    "        G_loss = G_loss_adv + G_loss_stim\n",
    "        G_loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        for p in gan.G.parameters():\n",
    "            p.grad.data.clamp_(-1., 1.)\n",
    "\n",
    "        history[\"G_mean_abs_grad_first_layer\"].append(torch.mean(torch.abs(gan.G.layers[0].weight.grad)).item())\n",
    "        history[\"G_mean_abs_grad_last_layer\"].append(torch.mean(torch.abs(gan.G.layers[-2].weight.grad)).item())\n",
    "        gan.G_optim.step()\n",
    "\n",
    "        ### log\n",
    "        history[\"D_loss\"].append(D_loss.item())\n",
    "        history[\"G_loss\"].append(G_loss.item())\n",
    "        history[\"G_loss_stim\"].append(G_loss_stim.item())\n",
    "        history[\"G_loss_adv\"].append(G_loss_adv.item())\n",
    "\n",
    "        if batch_idx % log_freq == 0 and batch_idx > 0:\n",
    "            print(\n",
    "                f\"[{epoch + 1}/{config['decoder']['n_epochs']}  {batch_idx * len(resp)}/{len(train_dataloader) * len(resp)}  \"\n",
    "                f\"({100. * batch_idx / len(train_dataloader):.0f}%)]  \"\n",
    "                f\"D-loss: {D_loss.item():.4f}  \"\n",
    "                f\"G-loss: {G_loss.item():.4f}  \"\n",
    "                f\"G-loss-stim: {G_loss_stim.item():.4f}  \"\n",
    "                f\"G-loss-adv: {G_loss_adv.item():.4f}\"\n",
    "            )\n",
    "\n",
    "            live_plot.update({\n",
    "                k: history[k][-log_freq:] for k in history.keys()\n",
    "                if k not in [\"val_loss\"]               \n",
    "            }, display=True)\n",
    "    \n",
    "    ### eval\n",
    "    val_loss = val(\n",
    "        model=gan,\n",
    "        dataloader=val_dataloader,\n",
    "        loss_fn=config[\"decoder\"][\"stim_loss_fn\"],\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    ### save best model\n",
    "    if val_loss < best[\"val_loss\"]:\n",
    "        best[\"val_loss\"] = val_loss\n",
    "        best[\"epoch\"] = epoch\n",
    "        best[\"decoder\"] = deepcopy(gan.state_dict())\n",
    "\n",
    "    ### log\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    print(f\"{val_loss=:.4f}\")\n",
    "\n",
    "    ### plot reconstructions\n",
    "    stim_pred = gan(resp_sample[:8].to(config[\"device\"])).detach()\n",
    "    plot_comparison(\n",
    "        target=crop_stim(stim_sample[:8]).cpu(),\n",
    "        pred=crop_stim(stim_pred[:8]).cpu(),\n",
    "        save_to=make_sample_path(epoch, \"\")\n",
    "    )\n",
    "\n",
    "    ### plot losses\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        plot_losses(history=history)\n",
    "\n",
    "        ### ckpt\n",
    "        if config[\"decoder\"][\"save_run\"]:\n",
    "            torch.save({\n",
    "                \"decoder\": gan.state_dict(),\n",
    "                \"history\": history,\n",
    "                \"config\": config,\n",
    "                \"best\": best,\n",
    "            }, os.path.join(config[\"dir\"], \"ckpt\", f\"decoder_{epoch}.pt\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best val loss: {best['val_loss']:.4f} at epoch {best['epoch']}\")\n",
    "\n",
    "### save final\n",
    "torch.save({\n",
    "    \"decoder\": gan.state_dict(),\n",
    "    \"history\": history,\n",
    "    \"config\": config,\n",
    "    \"best\": best,\n",
    "}, os.path.join(config[\"dir\"], f\"decoder.pt\"))\n",
    "\n",
    "### plot reconstructions of the final model\n",
    "gan.load_state_dict(best[\"decoder\"])\n",
    "stim_pred_best = gan(resp_sample.to(config[\"device\"])).detach().cpu()\n",
    "plot_comparison(\n",
    "    target=crop_stim(stim_sample[:8]).cpu(),\n",
    "    pred=crop_stim(stim_pred_best[:8]).cpu(),\n",
    "    save_to=os.path.join(config[\"dir\"], \"stim_comparison_best.png\")\n",
    ")\n",
    "\n",
    "### plot losses\n",
    "plot_losses(\n",
    "    history=history,\n",
    "    save_to=None if not config[\"decoder\"][\"save_run\"] else os.path.join(config[\"dir\"], f\"losses.png\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_plot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_axes = len(history.keys())\n",
    "fig = plt.figure(figsize=(24, 22))\n",
    "\n",
    "for k_i, k in enumerate(history.keys()):\n",
    "    ax = fig.add_subplot((n_axes // 3) + 1, 3, k_i + 1)\n",
    "    ax.plot(history[k])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_title(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
