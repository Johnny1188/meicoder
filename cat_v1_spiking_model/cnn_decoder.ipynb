{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.CNN_Decoder import CNN_Decoder\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop\n",
    "\n",
    "from data import (\n",
    "    prepare_v1_dataloaders,\n",
    "    SyntheticDataset,\n",
    "    BatchPatchesDataLoader,\n",
    "    MixedBatchLoader,\n",
    "    PerSampleStoredDataset,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    # \"crop\": True,\n",
    "    # \"batch_size\": 64,\n",
    "    \"batch_size\": 20,\n",
    "    \"stim_normalize_mean\": 46.236,\n",
    "    \"stim_normalize_std\": 21.196,\n",
    "    \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "    \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data statistics\n",
    "# v1_dataloaders = prepare_data_loaders(**config[\"data\"])\n",
    "# data_stats = get_mean_and_std(dataset=v1_dataloaders[\"train\"].dataset, verbose=True)\n",
    "# for k in data_stats:\n",
    "#     for ks in data_stats[k]:\n",
    "#         print(f\"{k}.{ks}: {data_stats[k][ks]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data loaders\n",
    "v1_dataloaders = prepare_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "stim, resp = next(iter(v1_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{stim.shape=}, {resp.shape=}\"\n",
    "    f\"\\n{stim.min()=}, {stim.max()=}\"\n",
    "    f\"\\n{resp.min()=}, {resp.max()=}\"\n",
    "    f\"\\n{stim.mean()=}, {stim.std()=}\"\n",
    "    f\"\\n{resp.mean()=}, {resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data (generated using the Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_mean_from_syn_dataset.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_std_from_syn_dataset.npy\"))).float()\n",
    "\n",
    "config[\"data\"][\"syn_data\"] = {\n",
    "    \"dataset\": {\n",
    "        \"stim_transform\": transforms.Normalize(\n",
    "            mean=114.457,\n",
    "            std=51.356,\n",
    "        ),\n",
    "        \"resp_transform\": csng.utils.Normalize(\n",
    "            mean=resp_mean,\n",
    "            std=resp_std,\n",
    "        ),\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 20,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets = {\n",
    "    \"train\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"train\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"val\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"val\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"test\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"test\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "syn_dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=syn_datasets[\"train\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        dataset=syn_datasets[\"val\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=syn_datasets[\"test\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate statistics\n",
    "\n",
    "### for stimuli\n",
    "# syn_stats = get_mean_and_std(dataset=syn_dataset, verbose=True)\n",
    "# syn_stats\n",
    "\n",
    "### for responses\n",
    "# from csng.utils import RunningStats\n",
    "\n",
    "# stats = RunningStats(num_components=10000, lib=\"torch\", device=\"cuda\")\n",
    "# for i, (s, r) in enumerate(syn_dataloader):\n",
    "#     stats.update(r)\n",
    "#     if i % 200 == 0:\n",
    "#         print(f\"{i}: {r.mean()=} {r.std()=} {stats.get_mean()=} {stats.get_std()=}\")\n",
    "\n",
    "### save\n",
    "# torch.save(stats.get_mean(), os.path.join(DATA_PATH, \"responses_mean_from_syn_dataset.pt\"))\n",
    "# torch.save(stats.get_std(), os.path.join(DATA_PATH, \"responses_std_from_syn_dataset.pt\"))\n",
    "\n",
    "\n",
    "### generate preprocessed synthetic data\n",
    "# import pickle\n",
    "# target_dir = os.path.join(DATA_PATH, \"synthetic_data\", \"processed\")\n",
    "\n",
    "# for data_split in (\"train\", \"val\", \"test\"):\n",
    "#     print(data_split)\n",
    "#     ### get the whole batch from dataloaders and save to disk\n",
    "#     sample_idx = 0\n",
    "#     for stim, resp in syn_dataloaders[data_split]:\n",
    "#         if sample_idx % 2000 == 0:\n",
    "#             print(\"  \", sample_idx)\n",
    "        \n",
    "#         for i in range(stim.shape[0]):\n",
    "#             sample_idx += 1\n",
    "#             save_to = os.path.join(target_dir, data_split, f\"{sample_idx}.pickle\")\n",
    "\n",
    "#             data = {\"stim\": stim[i].cpu(), \"resp\": resp[i].cpu()}\n",
    "#             with open(save_to, \"wb\") as f:\n",
    "#                 pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp = next(iter(syn_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{syn_stim.shape=}, {syn_resp.shape=}\"\n",
    "    f\"\\n{syn_stim.min()=}, {syn_stim.max()=}\"\n",
    "    f\"\\n{syn_resp.min()=}, {syn_resp.max()=}\"\n",
    "    f\"\\n{syn_stim.mean()=}, {syn_stim.std()=}\"\n",
    "    f\"\\n{syn_resp.mean()=}, {syn_resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(syn_stim.cpu()[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(syn_resp.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, opter, loss_fn, config, l1_reg_mul=0, l2_reg_mul=0, verbose=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "    ### run\n",
    "    for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "        ### data\n",
    "        stim = stim.to(config[\"device\"])\n",
    "        resp = resp.to(config[\"device\"])\n",
    "        \n",
    "        ### train\n",
    "        opter.zero_grad()\n",
    "        stim_pred = model(resp)\n",
    "        loss = loss_fn(stim_pred, stim)\n",
    "\n",
    "        ### regularization\n",
    "        if l1_reg_mul != 0:\n",
    "            l1_reg = sum(p.abs().sum() for n, p in model.named_parameters() if p.requires_grad and \"weight\" in n)\n",
    "            loss += l1_reg_mul * l1_reg\n",
    "        if l2_reg_mul != 0:\n",
    "            l2_reg = sum(p.pow(2.0).sum() for n, p in model.named_parameters() if p.requires_grad and \"weight\" in n)\n",
    "            loss += l2_reg_mul * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        opter.step()\n",
    "        \n",
    "        ### log\n",
    "        train_loss += loss.item()\n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            print(f\"Training progress: [{batch_idx}/{n_batches} ({100. * batch_idx / n_batches:.0f}%)]\"\n",
    "                  f\"  Loss: {loss.item():.6f}\")\n",
    "        batch_idx += 1\n",
    "\n",
    "    train_loss /= n_batches\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, dataloader, loss_fn, config):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "            stim = stim.to(config[\"device\"])\n",
    "            resp = resp.to(config[\"device\"])\n",
    "            \n",
    "            stim_pred = model(resp)\n",
    "            loss = loss_fn(stim_pred, stim)\n",
    "            \n",
    "            ### log\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(dataloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, v1_dataloaders, syn_dataloaders, only_v1_data_eval=True):\n",
    "    if \"v1_data\" in config[\"data\"] and \"syn_data\" in config[\"data\"]:\n",
    "        train_dataloader = MixedBatchLoader(\n",
    "            dataloaders=[v1_dataloaders[\"train\"], syn_dataloaders[\"train\"]],\n",
    "            mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "            device=config[\"device\"],\n",
    "        )\n",
    "        if only_v1_data_eval:\n",
    "            val_dataloader = v1_dataloaders[\"val\"]\n",
    "        else:\n",
    "            val_dataloader = MixedBatchLoader(\n",
    "                dataloaders=[v1_dataloaders[\"val\"], syn_dataloaders[\"val\"]],\n",
    "                mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "                device=config[\"device\"],\n",
    "            )\n",
    "    elif \"v1_data\" in config[\"data\"]:\n",
    "        train_dataloader = v1_dataloaders[\"train\"]\n",
    "        val_dataloader = v1_dataloaders[\"val\"]\n",
    "    elif \"syn_data\" in config[\"data\"]:\n",
    "        train_dataloader = syn_dataloaders[\"train\"]\n",
    "        val_dataloader = syn_dataloaders[\"val\"]\n",
    "    else:\n",
    "        raise ValueError(\"No data to train on.\")\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"decoder\"] = {\n",
    "    \"model\": {\n",
    "        \"resp_shape\": (10000,),\n",
    "        \"stim_shape\": (1, 50, 50),\n",
    "        \"layers\": [\n",
    "            (\"fc\", 384),  # CNN Baseline\n",
    "            (\"unflatten\", 1, (6, 8, 8)),  # CNN Baseline\n",
    "            # (\"fc\", 576),  # CNN Baseline large\n",
    "            # (\"unflatten\", 1, (9, 8, 8)),  # CNN Baseline large\n",
    "            (\"deconv\", 256, 7, 2, 0),\n",
    "            (\"deconv\", 64, 5, 2, 0),\n",
    "            (\"deconv\", 32, 4, 1, 0),\n",
    "            # (\"deconv\", 64, 5, 1, 1),  # CNN Baseline large\n",
    "            # (\"deconv\", 32, 4, 1, 1),  # CNN Baseline large\n",
    "            (\"deconv\", 1, 3, 1, 0),\n",
    "        ],\n",
    "        \"act_fn\": nn.ReLU,\n",
    "        \"out_act_fn\": nn.Identity,\n",
    "        \"dropout\": 0.2,\n",
    "        \"batch_norm\": True,\n",
    "    },\n",
    "    \"opter_cls\": torch.optim.Adam,\n",
    "    \"opter_kwargs\": {\n",
    "        \"lr\": 0.003,\n",
    "    },\n",
    "    # \"loss_fn\": nn.MSELoss(),\n",
    "    # \"loss_fn\": MSELossWithCrop(window=config[\"stim_crop_win\"]),\n",
    "    \"loss_fn\": SSIMLoss(\n",
    "        window=config[\"stim_crop_win\"],\n",
    "        log_loss=True,\n",
    "        inp_normalized=True,\n",
    "    ),\n",
    "    \"l1_reg_mul\": 0,\n",
    "    \"l2_reg_mul\": 1e-5,\n",
    "    \"n_epochs\": 100,\n",
    "    \"save_run\": True,\n",
    "}\n",
    "\n",
    "decoder = CNN_Decoder(**config[\"decoder\"][\"model\"]).to(config[\"device\"])\n",
    "opter = config[\"decoder\"][\"opter_cls\"](decoder.parameters(), **config[\"decoder\"][\"opter_kwargs\"])\n",
    "loss_fn = config[\"decoder\"][\"loss_fn\"]() if type(config[\"decoder\"][\"loss_fn\"]) == type else config[\"decoder\"][\"loss_fn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare checkpointing\n",
    "if config[\"decoder\"][\"save_run\"]:\n",
    "    ### save config\n",
    "    run_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    config[\"dir\"] = os.path.join(DATA_PATH, \"models\", run_name)\n",
    "    os.makedirs(config[\"dir\"], exist_ok=True)\n",
    "    with open(os.path.join(config[\"dir\"], \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4, default=str)\n",
    "    os.makedirs(os.path.join(config[\"dir\"], \"samples\"), exist_ok=True)\n",
    "    make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "        config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    "    )\n",
    "\n",
    "    print(f\"Run name: {run_name}\\nRun dir: {config['dir']}\")\n",
    "else:\n",
    "    make_sample_path = lambda epoch, prefix: None\n",
    "    print(\"WARNING: Not saving the run and the config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load ckpt\n",
    "run_name = \"2023-08-01_15-22-55\"\n",
    "ckpt = torch.load(os.path.join(DATA_PATH, \"models\", run_name, \"decoder.pt\"))\n",
    "\n",
    "decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "opter.load_state_dict(ckpt[\"opter\"])\n",
    "history = ckpt[\"history\"]\n",
    "config = ckpt[\"config\"]\n",
    "best = ckpt[\"best\"]\n",
    "\n",
    "make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "    config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(decoder(resp.to(config[\"device\"])).shape)\n",
    "print(f\"Number of parameters: {count_parameters(decoder)}\")\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, save_to=None):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history[\"train_loss\"], label=\"train\")\n",
    "    ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_to:\n",
    "        fig.savefig(save_to)\n",
    "    ### save fig\n",
    "    if config[\"decoder\"][\"save_run\"]:\n",
    "        fig.savefig(os.path.join(config[\"dir\"], f\"losses_{epoch}.png\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "best = {\"val_loss\": np.inf, \"epoch\": 0, \"model\": None}\n",
    "s, e = len(history[\"train_loss\"]), len(history[\"train_loss\"]) + config[\"decoder\"][\"n_epochs\"]\n",
    "for epoch in range(s, e):\n",
    "    print(f\"[{epoch + 1}/{e}]\")\n",
    "\n",
    "    ### train and val\n",
    "    train_dataloader, val_dataloader = get_dataloaders(\n",
    "        config=config,\n",
    "        v1_dataloaders=v1_dataloaders,\n",
    "        syn_dataloaders=syn_dataloaders,\n",
    "        only_v1_data_eval=config[\"only_v1_data_eval\"],\n",
    "    )\n",
    "    train_loss = train(\n",
    "        model=decoder,\n",
    "        dataloader=train_dataloader,\n",
    "        opter=opter,\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "        l1_reg_mul=config[\"decoder\"][\"l1_reg_mul\"],\n",
    "        l2_reg_mul=config[\"decoder\"][\"l2_reg_mul\"],\n",
    "    )\n",
    "    val_loss = val(\n",
    "        model=decoder,\n",
    "        dataloader=val_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    ### save best model\n",
    "    if val_loss < best[\"val_loss\"]:\n",
    "        best[\"val_loss\"] = val_loss\n",
    "        best[\"epoch\"] = epoch\n",
    "        best[\"model\"] = decoder.state_dict()\n",
    "\n",
    "    ### log\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    print(f\"{train_loss=:.4f}, {val_loss=:.4f}\")\n",
    "\n",
    "    ### plot reconstructions\n",
    "    stim_pred = decoder(resp[:8].to(config[\"device\"])).detach()\n",
    "    plot_comparison(target=stim[:8].cpu(), pred=stim_pred[:8].cpu(), save_to=make_sample_path(epoch, \"no_crop_\"))\n",
    "    if \"v1_data\" in config[\"data\"] and config[\"data\"][\"v1_data\"][\"crop\"] == False:\n",
    "        plot_comparison(target=crop_stim(stim[:8]).cpu(), pred=crop_stim(stim_pred[:8]).cpu(), save_to=make_sample_path(epoch, \"\"))\n",
    "\n",
    "    ### plot losses\n",
    "    if epoch % 10 == 0:\n",
    "        plot_losses(\n",
    "            history=history,\n",
    "            save_to=None if not config[\"decoder\"][\"save_run\"] else os.path.join(config[\"dir\"], f\"losses_{epoch}.png\"),\n",
    "        )\n",
    "\n",
    "    ### ckpt\n",
    "    if config[\"decoder\"][\"save_run\"]:\n",
    "        torch.save({\n",
    "            \"decoder\": decoder.state_dict(),\n",
    "            \"opter\": opter.state_dict(),\n",
    "            \"history\": history,\n",
    "            \"config\": config,\n",
    "            \"best\": best,\n",
    "        }, os.path.join(config[\"dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot reconstructions of the final model\n",
    "decoder.load_state_dict(best[\"model\"])\n",
    "stim_pred_best = decoder(resp.to(config[\"device\"])).detach().cpu()\n",
    "plot_comparison(\n",
    "    target=crop_stim(stim[:8]).cpu(),\n",
    "    pred=crop_stim(stim_pred_best[:8]).cpu(),\n",
    "    save_to=os.path.join(config[\"dir\"], \"stim_comparison_best.png\")\n",
    ")\n",
    "\n",
    "### plot losses\n",
    "plot_losses(\n",
    "    history=history,\n",
    "    save_to=None if not config[\"decoder\"][\"save_run\"] else os.path.join(config[\"dir\"], f\"losses_final.png\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
