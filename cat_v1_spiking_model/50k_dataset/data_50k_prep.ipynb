{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH='/media/jsobotka/ext_ssd/csng_data/cat_V1_spiking_model/50K_single_trial_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters, RunningStats\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop\n",
    "\n",
    "from data import (\n",
    "    prepare_50k_v1_dataloaders,\n",
    "    SyntheticDataset,\n",
    "    BatchPatchesDataLoader,\n",
    "    MixedBatchLoader,\n",
    "    PerSampleStoredDataset,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\", \"50K_single_trial_dataset\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running on cuda ...\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # \"device\": \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "subdirs = [\"train\", \"val\", \"test\"]\n",
    "train_ratio, val_ratio = 0.8, 0.125\n",
    "all_samples = sorted(os.listdir(os.path.join(DATA_PATH, \"single_trial\")))\n",
    "total_samples = len(all_samples)\n",
    "train_samples, val_samples = int(train_ratio * total_samples), int(val_ratio * total_samples)\n",
    "test_samples = total_samples - train_samples - val_samples\n",
    "print(f\"{train_samples=}, {val_samples=}, {test_samples=}\")\n",
    "\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(DATA_PATH, \"datasets\", subdir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "### split into subfolders\n",
    "for sample_idx, sample_name in enumerate(all_samples):\n",
    "    if sample_idx < train_samples:\n",
    "        subdir = subdirs[0]\n",
    "    elif sample_idx < train_samples + val_samples:\n",
    "        subdir = subdirs[1]\n",
    "    else:\n",
    "        subdir = subdirs[2]\n",
    "    \n",
    "    ### move file\n",
    "    stim = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"stimulus.npy\"))\n",
    "    exc_resp = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"V1_Exc_L23.npy\"))\n",
    "    inh_resp = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"V1_Inh_L23.npy\"))\n",
    "    # save as pickle\n",
    "    with open(os.path.join(DATA_PATH, \"datasets\", subdir, f\"{sample_name}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"stim\": stim,\n",
    "            \"exc_resp\": exc_resp,\n",
    "            \"inh_resp\": inh_resp,\n",
    "        }, f)\n",
    "    # remove sample_name directory\n",
    "    shutil.rmtree(os.path.join(DATA_PATH, \"single_trial\", sample_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move multi-trial test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(samples)=250,  target_dir='/media/jsobotka/ext_ssd/csng_data/cat_V1_spiking_model/50K_single_trial_dataset/datasets/test'\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(DATA_PATH, \"datasets\", \"test\")\n",
    "samples = sorted(os.listdir(os.path.join(DATA_PATH, \"Dataset_multitrial\", \"Dic23data\", \"multitrial\")))\n",
    "print(f\"{len(samples)=},  {target_dir=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for sample_name in samples:\n",
    "    sample_dir = os.path.join(DATA_PATH, \"Dataset_multitrial\", \"Dic23data\", \"multitrial\", sample_name)\n",
    "    \n",
    "    ### move files\n",
    "    stim = np.load(os.path.join(sample_dir, \"stimulus.npy\"))\n",
    "    all_exc_resp = []\n",
    "    all_inh_resp = []\n",
    "    for trial_dir_name in os.listdir(sample_dir):\n",
    "        if trial_dir_name == \"stimulus.npy\":\n",
    "            continue\n",
    "        exc_resp = np.load(os.path.join(sample_dir, trial_dir_name, \"V1_Exc_L23.npy\"))\n",
    "        inh_resp = np.load(os.path.join(sample_dir, trial_dir_name, \"V1_Inh_L23.npy\"))\n",
    "        all_exc_resp.append(exc_resp)\n",
    "        all_inh_resp.append(inh_resp)\n",
    "    exc_resp = np.stack(all_exc_resp, axis=0)\n",
    "    inh_resp = np.stack(all_inh_resp, axis=0)\n",
    "\n",
    "    ### save as pickle\n",
    "    with open(os.path.join(target_dir, f\"{sample_name}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"stim\": stim,\n",
    "            \"exc_resp\": exc_resp,\n",
    "            \"inh_resp\": inh_resp,\n",
    "        }, f)\n",
    "    \n",
    "    ### remove sample_name directory\n",
    "    shutil.rmtree(sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"datasets\", \"val\", f\"0000045000.pickle\"), \"rb\") as f:\n",
    "    da = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"datasets\", \"test\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 1000,\n",
    "    \"stim_keys\": (\"stim\",),\n",
    "    \"resp_keys\": (\"exc_resp\", \"inh_resp\"),\n",
    "    # \"stim_normalize_mean\": 46.143,\n",
    "    # \"stim_normalize_std\": 20.420,\n",
    "    # \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "    #     os.path.join(DATA_PATH, \"responses_mean.npy\")\n",
    "    # )).float(),\n",
    "    # \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "    #     os.path.join(DATA_PATH, \"responses_std.npy\")\n",
    "    # )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_dataloaders = prepare_50k_v1_dataloaders(**config[\"data\"][\"v1_data\"])\n",
    "dataloader = torch.utils.data.DataLoader(v1_dataloaders[\"train\"].dataset.dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "mean_inputs, std_inputs = torch.zeros(1), torch.zeros(1)\n",
    "for inp_idx, (inputs, targets) in enumerate(dataloader):\n",
    "    for c in range(inputs.size(1)):\n",
    "        mean_inputs[c] += inputs[:,c,:,:].mean((-1,-2)).mean()\n",
    "        std_inputs[c] += inputs[:,c,:,:].std((-1,-2)).mean()\n",
    "mean_inputs.div_(len(dataloader))\n",
    "std_inputs.div_(len(dataloader))\n",
    "mean_inputs, std_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 45000. Validation dataset size: 5000. Test dataset size: 0.\n",
      "0: r.mean()=tensor[] 3.475 r.std()=tensor[] 4.659 stats_all.get_mean()=tensor[46875] x∈[0.166, 30.312] μ=3.475 σ=2.705 stats_all.get_std()=tensor[46875] x∈[0.001, 0.281] μ=0.009 σ=0.017\n"
     ]
    }
   ],
   "source": [
    "v1_dataloaders = prepare_50k_v1_dataloaders(**config[\"data\"][\"v1_data\"])\n",
    "dataloader = torch.utils.data.DataLoader(v1_dataloaders[\"train\"].dataset.dataset, batch_size=1000, shuffle=True)\n",
    "stats_all = RunningStats(num_components=46875, lib=\"torch\", device=\"cpu\")\n",
    "stats_exc = RunningStats(num_components=37500, lib=\"torch\", device=\"cpu\")\n",
    "stats_inh = RunningStats(num_components=9375, lib=\"torch\", device=\"cpu\")\n",
    "for i, (s, r) in enumerate(dataloader):\n",
    "    stats_all.update(r)\n",
    "    stats_exc.update(r[:,:37500])\n",
    "    stats_inh.update(r[:,37500:])\n",
    "    if i % 200 == 0:\n",
    "        print(f\"{i}: {r.mean()=} {r.std()=} {stats_all.get_mean()=} {stats_all.get_std()=}\")\n",
    "\n",
    "## save\n",
    "torch.save(stats_all.get_mean(), os.path.join(DATA_PATH, \"responses_mean.pt\"))\n",
    "torch.save(stats_all.get_std(), os.path.join(DATA_PATH, \"responses_std.pt\"))\n",
    "torch.save(stats_exc.get_mean(), os.path.join(DATA_PATH, \"responses_exc_mean.pt\"))\n",
    "torch.save(stats_exc.get_std(), os.path.join(DATA_PATH, \"responses_exc_std.pt\"))\n",
    "torch.save(stats_inh.get_mean(), os.path.join(DATA_PATH, \"responses_inh_mean.pt\"))\n",
    "torch.save(stats_inh.get_std(), os.path.join(DATA_PATH, \"responses_inh_std.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"datasets\", \"test\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 1000,\n",
    "    \"stim_keys\": (\"stim\",),\n",
    "    \"resp_keys\": (\"exc_resp\", \"inh_resp\"),\n",
    "    \"stim_normalize_mean\": 46.143,\n",
    "    \"stim_normalize_std\": 20.420,\n",
    "    \"resp_normalize_mean\": torch.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean.pt\")\n",
    "    ),\n",
    "    \"resp_normalize_std\": torch.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std.pt\")\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 45000. Validation dataset size: 5000. Test dataset size: 250.\n"
     ]
    }
   ],
   "source": [
    "### get data loaders\n",
    "v1_dataloaders = prepare_50k_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 34, in do_one_step\n    data = pin_memory(data, device)\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### show data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stim, resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv1_dataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstim\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstim\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstim\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in pin memory thread for device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 34, in do_one_step\n    data = pin_memory(data, device)\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in pin_memory\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 70, in <listcomp>\n    return type(data)([pin_memory(sample, device) for sample in data])  # type: ignore[call-arg]\n  File \"/home/jsobotka/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 55, in pin_memory\n    return data.pin_memory(device)\nRuntimeError: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "### show data\n",
    "stim, resp = next(iter(v1_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{stim.shape=}, {resp.shape=}\"\n",
    "    f\"\\n{stim.min()=}, {stim.max()=}\"\n",
    "    f\"\\n{resp.min()=}, {resp.max()=}\"\n",
    "    f\"\\n{stim.mean()=}, {stim.std()=}\"\n",
    "    f\"\\n{resp.mean()=}, {resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp[0].view(125, 375).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "from data_orig import prepare_spiking_data_loaders\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "### config only for the encoder\n",
    "spiking_data_loaders_config = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "encoder_config = {\n",
    "    \"init_mu_range\": 0.55,\n",
    "    \"init_sigma\": 0.4,\n",
    "    \"input_kern\": 19,\n",
    "    \"hidden_kern\": 17,\n",
    "    \"hidden_channels\": 32,\n",
    "    \"gamma_input\": 1.0,\n",
    "    \"gamma_readout\": 2.439,\n",
    "    \"grid_mean_predictor\": None,\n",
    "    \"layers\": 5\n",
    "}\n",
    "\n",
    "### encoder\n",
    "_dataloaders = prepare_spiking_data_loaders(**spiking_data_loaders_config)\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_config,\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=2,\n",
    ").float()\n",
    "del _dataloaders\n",
    "\n",
    "### load pretrained core\n",
    "pretrained_core = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"spiking_scratch_tunecore_68Y_model.pth\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "encoder.load_state_dict(pretrained_core, strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = \"test\" ## TODO: when \"train\", turn off shuffle\n",
    "save_stats = False\n",
    "\n",
    "trans_to_apply = [\n",
    "    {\n",
    "        \"name\": \"original\",\n",
    "        \"stim\": lambda x: x,\n",
    "        \"resp\": lambda x: x,\n",
    "        \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part),\n",
    "        \"sample_idx\": 0,\n",
    "        \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    },\n",
    "    # { ### noise to resp\n",
    "    #     \"name\": \"01noise_resp\n",
    "    #     \"stim\": lambda x: x,\n",
    "    #     \"resp\": lambda x: x + torch.randn(x.shape) * 0.1,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_01noise_resp\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### flip stim\n",
    "    #     \"name\": \"flip_stim\",\n",
    "    #     \"stim\": lambda x: x.flip(2),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_flip_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"01rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 1, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_01rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"02rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 2, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_02rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"03rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 3, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", data_part + \"_03rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "]\n",
    "\n",
    "### create dirs\n",
    "for tran_to_apply in trans_to_apply:\n",
    "    if os.path.exists(tran_to_apply[\"save_dir\"]) and len(os.listdir(tran_to_apply[\"save_dir\"])) > 0:\n",
    "        print(f\"[WARNING]: {tran_to_apply['save_dir']} already exists and is not empty.\")\n",
    "    os.makedirs(tran_to_apply[\"save_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(v1_dataloaders[data_part])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### run\n",
    "    for batch_idx, (stim, _) in enumerate(v1_dataloaders[data_part]):\n",
    "        for tran_to_apply in trans_to_apply:\n",
    "            stim = tran_to_apply[\"stim\"](stim.to(config[\"device\"]))\n",
    "\n",
    "            ### forward\n",
    "            resp = encoder(stim)\n",
    "            resp = tran_to_apply[\"resp\"](resp)\n",
    "            if save_stats:\n",
    "                tran_to_apply[\"stats\"].update(resp)\n",
    "\n",
    "            ### save\n",
    "            for i in range(stim.shape[0]):\n",
    "                sample_path = os.path.join(tran_to_apply[\"save_dir\"], f\"{tran_to_apply['sample_idx']}.pickle\")\n",
    "                with open(sample_path, \"wb\") as f:\n",
    "                    pickle.dump({\n",
    "                        \"stim\": stim[i].cpu(),\n",
    "                        \"resp\": resp[i].cpu(),\n",
    "                    }, f)\n",
    "                tran_to_apply[\"sample_idx\"] += 1\n",
    "        \n",
    "        ### log\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{n_batches}\")\n",
    "\n",
    "## save stats\n",
    "if save_stats:\n",
    "    for tran_to_apply in trans_to_apply:\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", f\"responses_mean_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_mean().cpu().numpy(),\n",
    "        )\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", f\"responses_std_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_std().cpu().numpy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_mean_original.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"responses_std_original.npy\"))).float()\n",
    "\n",
    "dataset = PerSampleStoredDataset(\n",
    "    dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_v1_encoder\", \"train\"),\n",
    "    stim_transform=lambda x: x,\n",
    "    resp_transform=csng.utils.Normalize(\n",
    "        mean=resp_mean,\n",
    "        std=resp_std,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = RunningStats(num_components=50 * 50, lib=\"torch\", device=\"cpu\")\n",
    "for b, (s, r) in enumerate(dataloader):\n",
    "    stats.update(s.view(s.shape[0], -1))\n",
    "    if b % 50 == 0:\n",
    "        print(f\"Batch {b} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_mean(), stats.get_std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
