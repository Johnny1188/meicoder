{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.CNN_Decoder import CNN_Decoder\n",
    "from csng.GAN import GAN\n",
    "from L2O_Decoder import L2O_Decoder\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop\n",
    "\n",
    "# from orig_data import prepare_spiking_data_loaders\n",
    "from data import prepare_v1_dataloaders, SyntheticDataset, BatchPatchesDataLoader, MixedBatchLoader, PerSampleStoredDataset\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    # \"crop\": True,\n",
    "    # \"batch_size\": 64,\n",
    "    \"batch_size\": 20,\n",
    "    \"stim_normalize_mean\": 46.236,\n",
    "    \"stim_normalize_std\": 21.196,\n",
    "    \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "    \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data loaders\n",
    "v1_dataloaders = prepare_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "stim, resp = next(iter(v1_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{stim.shape=}, {resp.shape=}\"\n",
    "    f\"\\n{stim.min()=}, {stim.max()=}\"\n",
    "    f\"\\n{resp.min()=}, {resp.max()=}\"\n",
    "    f\"\\n{stim.mean()=}, {stim.std()=}\"\n",
    "    f\"\\n{resp.mean()=}, {resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data (generated using the Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_mean_from_syn_dataset.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_std_from_syn_dataset.npy\"))).float()\n",
    "\n",
    "config[\"data\"][\"syn_data\"] = {\n",
    "    \"dataset\": {\n",
    "        \"stim_transform\": transforms.Normalize(\n",
    "            mean=114.457,\n",
    "            std=51.356,\n",
    "        ),\n",
    "        \"resp_transform\": csng.utils.Normalize(\n",
    "            mean=resp_mean,\n",
    "            std=resp_std,\n",
    "        ),\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 20,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets = {\n",
    "    \"train\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"train\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"val\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"val\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"test\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"test\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "syn_dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=syn_datasets[\"train\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        dataset=syn_datasets[\"val\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=syn_datasets[\"test\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp = next(iter(syn_dataloaders[\"val\"]))\n",
    "print(\n",
    "    f\"{syn_stim.shape=}, {syn_resp.shape=}\"\n",
    "    f\"\\n{syn_stim.min()=}, {syn_stim.max()=}\"\n",
    "    f\"\\n{syn_resp.min()=}, {syn_resp.max()=}\"\n",
    "    f\"\\n{syn_stim.mean()=}, {syn_stim.std()=}\"\n",
    "    f\"\\n{syn_resp.mean()=}, {syn_resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(syn_stim.cpu()[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(syn_resp.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, loss_fn, normalize_decoded, config, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    # with torch.no_grad():\n",
    "    for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "        stim = stim.to(device)\n",
    "        resp = resp.to(device)\n",
    "\n",
    "        if resp.ndim == 3:\n",
    "            # resp = resp.mean(dim=1) # average over trials (test V1 dataset)\n",
    "            resp = resp[:, 0, :] # take only the first trial\n",
    "        \n",
    "        if model.__class__.__name__ == \"L2O_Decoder\":\n",
    "            stim_pred, _ = model.run_batch(\n",
    "                train=False,\n",
    "                stim=None,\n",
    "                resp=resp,\n",
    "                n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "                x_hat_history_iters=None,\n",
    "            )\n",
    "        else:\n",
    "            stim_pred = model(resp)\n",
    "        \n",
    "        if normalize_decoded:\n",
    "            stim_pred = normalize(stim_pred)\n",
    "        \n",
    "        loss = loss_fn(stim_pred, stim)\n",
    "        \n",
    "        ### log\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(dataloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_to_compare = {\n",
    "    # \"0% - CNN-S\": {\n",
    "    #     \"run_name\": \"...\",\n",
    "    # },\n",
    "    \"0%\": {\n",
    "    # \"0% - CNN\": {\n",
    "        \"run_name\": \"2023-08-06_20-13-46\",\n",
    "    },\n",
    "    # \"0% - CNN-L\": {\n",
    "    #     \"run_name\": \"2023-08-10_00-02-52\",\n",
    "    # },\n",
    "    # \"25% - CNN\": {\n",
    "    #     \"run_name\": \"2023-08-07_18-28-52\",\n",
    "    # },\n",
    "    # \"25% - CNN-L\": {\n",
    "    #     \"run_name\": \"2023-08-14_23-24-26\",\n",
    "    # },\n",
    "    # \"50% - CNN-S\": {\n",
    "    #     \"run_name\": \"2023-08-17_23-03-04\",\n",
    "    # },\n",
    "    # \"50% - CNN\": {\n",
    "    #     \"run_name\": \"2023-08-07_08-50-53\",\n",
    "    # },\n",
    "    # \"50% - CNN-L\": {\n",
    "    #     \"run_name\": \"2023-08-09_00-03-18\",\n",
    "    # },\n",
    "    # \"50% - CNN-L\": { # G from GAN\n",
    "    #     \"run_name\": \"2023-10-02_10-09-20\",\n",
    "    # },\n",
    "    # \"75% - CNN\": {\n",
    "    #     \"run_name\": \"2023-08-07_08-54-37\",\n",
    "    # },\n",
    "    # \"75% - CNN-L\": {\n",
    "    #     \"run_name\": \"2023-08-09_23-42-36\",\n",
    "    # },\n",
    "    # \"100% - CNN\": {\n",
    "    #     \"run_name\": \"2023-08-09_00-08-49\",\n",
    "    # },\n",
    "    \n",
    "    # \"0% - GAN\": {\n",
    "    #     \"run_name\": \"2023-08-26_16-34-36\",\n",
    "    # },\n",
    "    # \"50% - GAN\": {\n",
    "    #     \"run_name\": \"2023-08-30_09-07-13\",\n",
    "    # },\n",
    "    \n",
    "    # \"0% - L2O\": {\n",
    "    #     \"run_name\": \"2023-08-21_23-07-49\",\n",
    "    # },\n",
    "    # \"25% - L2O\": {\n",
    "    #     \"run_name\": \"2023-09-05_19-45-22\",\n",
    "    # },\n",
    "    # \"50% - L2O\": {\n",
    "    #     \"run_name\": \"2023-09-02_15-09-52\",\n",
    "    # },\n",
    "\n",
    "    \"25%\": {\n",
    "        \"run_name\": \"2023-08-25_09-09-51\",\n",
    "    },\n",
    "    \"50%\": {\n",
    "        \"run_name\": \"2023-08-25_09-07-46\",\n",
    "    },\n",
    "    \"75%\": {\n",
    "        \"run_name\": \"2023-09-14_20-14-24\",\n",
    "    },\n",
    "\n",
    "    # \"test\": {\n",
    "    #     \"run_name\": \"2023-10-02_10-09-20\",\n",
    "    # },\n",
    "}\n",
    "\n",
    "loss_fns = {\n",
    "    \"Log SSIM Loss\": SSIMLoss(\n",
    "        window=config[\"stim_crop_win\"],\n",
    "        log_loss=True,\n",
    "        inp_normalized=True,\n",
    "    ),\n",
    "    \"SSIM Loss\": SSIMLoss(\n",
    "        window=config[\"stim_crop_win\"],\n",
    "        log_loss=False,\n",
    "        inp_normalized=True,\n",
    "    ),\n",
    "    \"MSE Loss\": lambda x_hat, x: F.mse_loss(\n",
    "        standardize(crop_stim(x_hat)),\n",
    "        standardize(crop_stim(x))\n",
    "    ),\n",
    "}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config for collecting results\n",
    "plot_losses = False\n",
    "plot_reconstructions = False\n",
    "rerun_l2o_val_loss_during_training = True\n",
    "rerun_all_val_losses = False\n",
    "\n",
    "### load models\n",
    "for k in runs_to_compare.keys():\n",
    "    run_name = runs_to_compare[k][\"run_name\"]\n",
    "    print(f\"Loading {k} model (run name: {run_name})...\")\n",
    "\n",
    "    ### load ckpt\n",
    "    if \"l2o\" in k.lower():\n",
    "        ckpt = torch.load(os.path.join(DATA_PATH, \"models\", run_name, \"decoder.pt\"))\n",
    "        config = ckpt[\"config\"]\n",
    "        decoder = L2O_Decoder(**config[\"decoder\"][\"model\"]).to(device)\n",
    "    elif \"gan\" in k.lower():\n",
    "        ckpt = torch.load(os.path.join(DATA_PATH, \"models\", \"gan\", run_name, \"decoder.pt\"))\n",
    "        config = ckpt[\"config\"]\n",
    "        decoder = GAN(**config[\"decoder\"][\"model\"]).to(device)\n",
    "    else:\n",
    "        ckpt = torch.load(os.path.join(DATA_PATH, \"models\", run_name, \"decoder.pt\"))\n",
    "        config = ckpt[\"config\"]\n",
    "        decoder = CNN_Decoder(**config[\"decoder\"][\"model\"]).to(device)\n",
    "\n",
    "    history = ckpt[\"history\"]\n",
    "    config[\"stim_crop_win\"] = (slice(15, 35), slice(15, 35))\n",
    "    best = ckpt[\"best\"]\n",
    "\n",
    "    ### rerun L2O val loss with the same loss function as other decoders use\n",
    "    if rerun_all_val_losses or (rerun_l2o_val_loss_during_training and \"l2o\" in k.lower()):\n",
    "        print(\"  Rerunning val loss...\")\n",
    "        history[\"val_loss\"] = []\n",
    "        ckpt_dir = os.path.join(DATA_PATH, \"models\", run_name, \"ckpt\")\n",
    "        \n",
    "        for epoch in range(config[\"decoder\"][\"n_epochs\"]):\n",
    "            ckpt_filepath = os.path.join(ckpt_dir, f\"decoder_{epoch}.pt\")\n",
    "            if os.path.exists(ckpt_filepath):\n",
    "                ckpt = torch.load(ckpt_filepath)\n",
    "                decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "                decoder.eval()\n",
    "                val_loss = eval(\n",
    "                    model=decoder,\n",
    "                    dataloader=v1_dataloaders[\"val\"],\n",
    "                    loss_fn=loss_fns[\"Log SSIM Loss\"],\n",
    "                    normalize_decoded=True,\n",
    "                    config=config,\n",
    "                    device=device,\n",
    "                )\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                print(f\"{epoch} \", end=\"\")\n",
    "            else:\n",
    "                history[\"val_loss\"].append(np.nan)\n",
    "        print()\n",
    "\n",
    "    ### load best model\n",
    "    if \"model\" in best.keys():\n",
    "        decoder.load_state_dict(best[\"model\"])\n",
    "    elif \"decoder\" in best.keys():\n",
    "        decoder.load_state_dict(best[\"decoder\"])\n",
    "    else:\n",
    "        decoder.load_state_dict(best)\n",
    "    decoder.eval()\n",
    "\n",
    "    ### plot losses\n",
    "    if plot_losses:\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(history[\"train_loss\"], label=\"train\")\n",
    "        ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_ylim(0, None)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    ### plot reconstructions of the final model\n",
    "    if plot_reconstructions:\n",
    "        if \"l2o\" in k.lower():\n",
    "            stim_pred, x_hat_history = decoder.run_batch(\n",
    "                train=False,\n",
    "                stim=None,\n",
    "                resp=resp.to(device),\n",
    "                n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "                x_hat_history_iters=None,\n",
    "            )\n",
    "            stim_pred_best = normalize(stim_pred.detach().cpu())\n",
    "        else:\n",
    "            stim_pred_best = decoder(resp.to(device)).detach().cpu()\n",
    "        plot_comparison(\n",
    "            target=crop_stim(stim[:8]).cpu(),\n",
    "            pred=crop_stim(stim_pred_best[:8]).cpu(),\n",
    "            # save_to=os.path.join(config[\"dir\"], \"stim_comparison_best.png\")\n",
    "        )\n",
    "\n",
    "    ### eval\n",
    "    test_losses = dict()\n",
    "    for loss_fn_name, loss_fn in loss_fns.items():\n",
    "        test_losses[loss_fn_name] = eval(\n",
    "            model=decoder,\n",
    "            dataloader=v1_dataloaders[\"test\"],\n",
    "            loss_fn=loss_fn,\n",
    "            normalize_decoded=True if \"l2o\" in k.lower() else False,\n",
    "            config=config,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    ### save\n",
    "    runs_to_compare[k][\"test_losses\"] = test_losses\n",
    "    for metric in history.keys():\n",
    "        runs_to_compare[k][metric] = history[metric]\n",
    "    runs_to_compare[k][\"config\"] = config\n",
    "    runs_to_compare[k][\"best_val_loss\"] = best[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\n",
    "    https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{height:.3f}\",\n",
    "            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            ha='center', va='bottom',\n",
    "            fontsize=13,\n",
    "            rotation=90,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot losses together\n",
    "\n",
    "### config\n",
    "to_plot = \"val_loss\"\n",
    "conv_win = 10\n",
    "\n",
    "### plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for k, run_dict in runs_to_compare.items():\n",
    "    if k == \"100%\":\n",
    "        continue\n",
    "    if conv_win is not None and np.nan not in run_dict[to_plot]:\n",
    "        vals_to_plot = np.convolve(run_dict[to_plot], np.ones(conv_win) / conv_win, mode=\"valid\")\n",
    "    else:\n",
    "        vals_to_plot = run_dict[to_plot]\n",
    "    ax.plot(\n",
    "        [t for t in range(len(vals_to_plot)) if vals_to_plot[t] is not np.nan],\n",
    "        [v for v in vals_to_plot if v is not np.nan],\n",
    "        label=k,\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "if to_plot == \"train_loss\":\n",
    "    ax.set_title(\"Training log SSIM loss (V1 data + x % of synthetic data)\", fontsize=16)\n",
    "elif to_plot == \"val_loss\":\n",
    "    ax.set_title(\"Validation log SSIM loss on the V1 data\", fontsize=16)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown loss type: {to_plot}\")\n",
    "\n",
    "ax.set_xlabel(\"Epoch\", fontsize=15, labelpad=20)\n",
    "ax.set_ylabel(\"Log SSIM loss\", fontsize=15, labelpad=20)\n",
    "ax.set_ylim(0.26, None)\n",
    "# ax.set_xlim(0, 150)\n",
    "ax.legend(\n",
    "    # loc=\"upper right\",\n",
    "    # loc=\"upper center\",\n",
    "    loc=\"lower left\",\n",
    "    fontsize=13,\n",
    "    frameon=False,\n",
    "    # bbox_to_anchor=(1.0, 1.0),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    title=\"% of syn. data in training\",\n",
    "    title_fontsize=15,\n",
    "    ncol=4,\n",
    ")\n",
    "# increase width of legend lines\n",
    "leg = ax.get_legend()\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(4.0)\n",
    "\n",
    "\n",
    "# set larger font for x and y ticks\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "\n",
    "# remove top and right spines\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bar plot of test losses\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "### grouped bar plot (run_dict[\"test_losses\"] is a dict containing multiple losses)\n",
    "bar_width = 0.9\n",
    "losses_to_plot = [\n",
    "    \"SSIM Loss\",\n",
    "    \"Log SSIM Loss\",\n",
    "    \"MSE Loss\",\n",
    "]\n",
    "colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "]\n",
    "for i, (k, run_dict) in enumerate(runs_to_compare.items()):\n",
    "    for j, loss in enumerate(losses_to_plot):\n",
    "        rects = ax.bar(\n",
    "            i - bar_width / len(losses_to_plot) + j * bar_width / len(losses_to_plot),\n",
    "            run_dict[\"test_losses\"][loss],\n",
    "            width=bar_width / len(losses_to_plot),\n",
    "            color=colors[j],\n",
    "        )\n",
    "        autolabel(rects)\n",
    "\n",
    "### add legend with color explanation\n",
    "from matplotlib import patches as mpatches\n",
    "ax.legend(\n",
    "    handles=[\n",
    "        mpatches.Patch(color=colors[i], label=loss)\n",
    "        for i, loss in enumerate(losses_to_plot)\n",
    "    ],\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.14),\n",
    "    ncol=len(losses_to_plot),\n",
    "    fontsize=14,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    \"Test Losses (test dataset only with V1 data)\",\n",
    "    fontsize=18,\n",
    "    pad=70,\n",
    ")\n",
    "ax.set_xticks(range(len(runs_to_compare)))\n",
    "ax.set_xticklabels(runs_to_compare.keys())\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "ax.set_xlabel(\"Decoder\", fontsize=14, labelpad=20)\n",
    "ax.set_ylabel(\"Loss\", fontsize=14, labelpad=20)\n",
    "ax.set_ylim(0, None)\n",
    "\n",
    "# remove top and right spines\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
