{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "import wandb\n",
    "from nnfabrik.builder import get_data\n",
    "from nnfabrik.builder import get_data\n",
    "from lurz2020.datasets.mouse_loaders import static_loaders\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "from lurz2020.training.trainers import standard_trainer as trainer\n",
    "from lurz2020.utility.measures import get_correlations, get_fraction_oracles\n",
    "\n",
    "import csng\n",
    "from csng.CNN_Decoder import CNN_Decoder\n",
    "from csng.utils import RunningStats, crop, plot_comparison, standardize, normalize, get_mean_and_std, count_parameters, plot_losses\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop, Loss\n",
    "from csng.data import MixedBatchLoader\n",
    "from csng.readins import (\n",
    "    MultiReadIn,\n",
    "    HypernetReadIn,\n",
    "    ConvReadIn,\n",
    "    AttentionReadIn,\n",
    "    FCReadIn,\n",
    "    AutoEncoderReadIn,\n",
    "    Conv1dReadIn,\n",
    "    LocalizedFCReadIn,\n",
    ")\n",
    "\n",
    "from encoder import get_encoder\n",
    "from data_utils import (\n",
    "    get_mouse_v1_data,\n",
    "    append_syn_dataloaders,\n",
    "    append_data_aug_dataloaders,\n",
    "    RespGaussianNoise,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"mouse_v1_sensorium22\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"sequential\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "    \"crop_win\": (22, 36),\n",
    "}\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(device=config[\"device\"], eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "### load pretrained encoder ckpt\n",
    "encoder_ckpt = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"encoder.pt\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "\n",
    "### get temporary dataloaders for the encoder\n",
    "_dataloaders = get_data(\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_fn\"],\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_config\"]\n",
    ")\n",
    "\n",
    "### init encoder\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_ckpt[\"config\"][\"encoder\"][\"model_config\"],\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=encoder_ckpt[\"config\"][\"seed\"],\n",
    ").float()\n",
    "encoder.load_state_dict(encoder_ckpt[\"encoder_state\"], strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### validate encoder is working (corr on val set should be ~ 0.32)\n",
    "train_correlation = get_correlations(encoder, _dataloaders[\"train\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "validation_correlation = get_correlations(encoder, _dataloaders[\"validation\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "test_correlation = get_correlations(encoder, _dataloaders[\"test\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "\n",
    "print(\n",
    "    f\"Correlation (train set):      {train_correlation:.3f}\"\n",
    "    f\"\\nCorrelation (validation set): {validation_correlation:.3f}\"\n",
    "    f\"\\nCorrelation (test set):       {test_correlation:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mouse V1 dataset (Sensorium 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prep data config\n",
    "filenames = [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "    # \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # mouse 1\n",
    "    # \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # sensorium+ (mouse 2)\n",
    "    \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 3)\n",
    "    # \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 4)\n",
    "    # \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 5)\n",
    "    # \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 6)\n",
    "    # \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 7)\n",
    "]\n",
    "for f_idx, f_name in enumerate(filenames):\n",
    "    filenames[f_idx] = os.path.join(DATA_PATH, f_name)\n",
    "\n",
    "config[\"data\"].update({\n",
    "    \"paths\": filenames,\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": filenames,\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 0.25, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"normalize_neuron_coords\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get dataloaders and cell coordinates\n",
    "dataloaders, neuron_coords = get_mouse_v1_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "sample_data_key = dataloaders[\"mouse_v1\"][\"val\"].data_keys[0]\n",
    "datapoint = next(iter(dataloaders[\"mouse_v1\"][\"val\"].dataloaders[0]))\n",
    "stim, resp = datapoint.images, datapoint.responses\n",
    "pupil_center = datapoint.pupil_center\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {stim.shape}\"\n",
    "    f\"\\n  min={stim.min().item():.3f}  max={stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={stim.mean().item():.3f}  std={stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {resp.shape}\"\n",
    "    f\"\\n  min={resp.min().item():.3f}  max={resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={resp.mean().item():.3f}  std={resp.std().item():.3f}\"\n",
    "    \"\\nneuronal coordinates:\"\n",
    "    f\"\\n  {neuron_coords[sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[sample_data_key].min():.3f}  max={neuron_coords[sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[sample_data_key].mean():.3f}  std={neuron_coords[sample_data_key].std():.3f}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "reshape_to = None\n",
    "for i in range(30, 150):\n",
    "    if resp.shape[-1] % i == 0:\n",
    "        reshape_to = (i, resp.shape[-1] // i)\n",
    "        break\n",
    "if reshape_to != None:\n",
    "    ax.imshow(resp[0].view(reshape_to).squeeze(0).unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data (1)\n",
    "- Using the same stimuli as in Sens22 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = \"test\"\n",
    "assert len(dataloaders[\"mouse_v1\"][data_part].data_keys) == 1,\\\n",
    "    \"Create synthetic datasets one by one.\"\n",
    "data_key = dataloaders[\"mouse_v1\"][data_part].data_keys[0]\n",
    "save_stats = False\n",
    "\n",
    "print(f\"{data_part=}  {data_key=}  {save_stats=}\")\n",
    "\n",
    "trans_to_apply = [\n",
    "    {\n",
    "        \"name\": \"original\",\n",
    "        \"stim\": lambda x: x,\n",
    "        \"resp\": lambda x: x,\n",
    "        \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, data_part),\n",
    "        \"sample_idx\": 0,\n",
    "        \"stats\": RunningStats(num_components=encoder.readout[data_key].outdims, lib=\"torch\", device=config[\"device\"]),\n",
    "    },\n",
    "    # { ### noise to resp\n",
    "    #     \"name\": \"01noise_resp\n",
    "    #     \"stim\": lambda x: x,\n",
    "    #     \"resp\": lambda x: x + torch.randn(x.shape) * 0.1,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_01noise_resp\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### flip stim\n",
    "    #     \"name\": \"flip_stim\",\n",
    "    #     \"stim\": lambda x: x.flip(2),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_flip_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"01rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 1, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_01rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"02rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 2, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_02rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"03rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 3, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_03rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "]\n",
    "\n",
    "### create dirs\n",
    "for tran_to_apply in trans_to_apply:\n",
    "    if os.path.exists(tran_to_apply[\"save_dir\"]) and len(os.listdir(tran_to_apply[\"save_dir\"])) > 0:\n",
    "        print(f\"[WARNING]: {tran_to_apply['save_dir']} already exists and is not empty.\")\n",
    "    os.makedirs(tran_to_apply[\"save_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(dataloaders[\"mouse_v1\"][data_part])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### run\n",
    "    for batch_idx, b in enumerate(dataloaders[\"mouse_v1\"][data_part]):\n",
    "        for _data_key, (stim, _, neuron_coords, pupil_center) in b.items():\n",
    "            assert _data_key == data_key, f\"Data key mismatch: {data_key} vs. {_data_key}\"\n",
    "            for tran_to_apply in trans_to_apply:\n",
    "                stim = tran_to_apply[\"stim\"](stim.to(config[\"device\"]))\n",
    "\n",
    "                ### forward\n",
    "                resp = encoder(stim, data_key=data_key)\n",
    "                resp = tran_to_apply[\"resp\"](resp)\n",
    "                if save_stats:\n",
    "                    tran_to_apply[\"stats\"].update(resp)\n",
    "\n",
    "                ### save\n",
    "                for i in range(stim.shape[0]):\n",
    "                    sample_path = os.path.join(tran_to_apply[\"save_dir\"], f\"{tran_to_apply['sample_idx']}.pickle\")\n",
    "                    with open(sample_path, \"wb\") as f:\n",
    "                        pickle.dump({\n",
    "                            \"stim\": stim[i].cpu(),\n",
    "                            \"resp\": resp[i].cpu(),\n",
    "                            \"pupil_center\": pupil_center[i].cpu(),\n",
    "                        }, f)\n",
    "                    tran_to_apply[\"sample_idx\"] += 1\n",
    "\n",
    "        ### log\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{n_batches}\")\n",
    "\n",
    "## save stats of responses\n",
    "if save_stats:\n",
    "    for tran_to_apply in trans_to_apply:\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_mean_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_mean().cpu().numpy(),\n",
    "        )\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_std_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_std().cpu().numpy(),\n",
    "        )\n",
    "\n",
    "## save neuron_coords\n",
    "p = os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"neuron_coords.npy\")\n",
    "np.save(p, neuron_coords.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "# resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", target_data_key, f\"responses_mean_original.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_std_original.npy\"))).float()\n",
    "\n",
    "div_by = resp_std.clone()\n",
    "thres = 0.01 * resp_std.mean()\n",
    "idx = resp_std <= thres\n",
    "div_by[idx] = thres\n",
    "\n",
    "dataset = PerSampleStoredDataset(\n",
    "    dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, data_part),\n",
    "    stim_transform=lambda x: x, # stim is already normalized\n",
    "    resp_transform=csng.utils.Normalize(\n",
    "        # mean=resp_mean,\n",
    "        mean=0,\n",
    "        std=div_by,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = RunningStats(num_components=resp.shape[-1], lib=\"torch\", device=\"cpu\")\n",
    "for b, (s, r, pc) in enumerate(dataloader):\n",
    "    # stats.update(s.view(s.shape[0], -1))\n",
    "    stats.update(r)\n",
    "    if b % 50 == 0:\n",
    "        print(f\"Batch {b} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_mean(), stats.get_std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix wrongly calculated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append synthetic data\n",
    "config[\"data\"][\"syn_dataset_config\"] = {\n",
    "    \"data_keys\": [\n",
    "        \"21067-10-18\",\n",
    "        # \"22846-10-16\",\n",
    "        # \"23343-5-17\",\n",
    "        # \"23656-14-22\",\n",
    "        # \"23964-4-22\",\n",
    "    ],\n",
    "    \"batch_size\": 32,\n",
    "    \"append_data_parts\": [\"train\"],\n",
    "    # \"data_key_prefix\": \"syn\",\n",
    "    \"data_key_prefix\": None, # the same data key as the original (real) data\n",
    "}\n",
    "\n",
    "dataloaders = append_syn_dataloaders(dataloaders, config=config[\"data\"][\"syn_dataset_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all = []\n",
    "for b in dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]:\n",
    "    resps_all.append(b.responses.cuda())\n",
    "    print(len(resps_all))\n",
    "resps_all = torch.cat(resps_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all.mean(0), resps_all.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", config[\"data\"][\"syn_dataset_config\"][\"data_keys\"][0], f\"responses_mean_original.npy\"),\n",
    "    resps_all.mean(0).cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", config[\"data\"][\"syn_dataset_config\"][\"data_keys\"][0], f\"responses_std_original.npy\"),\n",
    "    resps_all.std(0).cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data (2)\n",
    "- Using new stimuli (crops from ImageNet)\n",
    "- Get stimuli from Sens22 data, crop 256x144 into different patches, and use as input to the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get base dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prep data config\n",
    "filenames = [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "    # \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # mouse 1\n",
    "    # \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # sensorium+ (mouse 2)\n",
    "    # \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 3)\n",
    "    # \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 4)\n",
    "    # \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 5)\n",
    "    # \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 6)\n",
    "    \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 7)\n",
    "]\n",
    "for f_idx, f_name in enumerate(filenames):\n",
    "    filenames[f_idx] = os.path.join(DATA_PATH, f_name)\n",
    "\n",
    "config[\"data\"][\"mouse_v1\"] = {\n",
    "    \"paths\": filenames,\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": filenames,\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 1.0, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": False,\n",
    "        \"batch_size\": 1,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"normalize_neuron_coords\": True,\n",
    "    \"average_test_multitrial\": True,\n",
    "    \"save_test_multitrial\": True,\n",
    "    \"test_batch_size\": 1,\n",
    "    \"device\": config[\"device\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get dataloaders and cell coordinates\n",
    "dataloaders, neuron_coords = get_mouse_v1_data(config[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "sample_data_key = dataloaders[\"mouse_v1\"][\"train\"].data_keys[0]\n",
    "datapoint = next(iter(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[0]))\n",
    "stim, resp = datapoint.images, datapoint.responses\n",
    "pupil_center = datapoint.pupil_center\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {stim.shape}\"\n",
    "    f\"\\n  min={stim.min().item():.3f}  max={stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={stim.mean().item():.3f}  std={stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {resp.shape}\"\n",
    "    f\"\\n  min={resp.min().item():.3f}  max={resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={resp.mean().item():.3f}  std={resp.std().item():.3f}\"\n",
    "    \"\\nneuronal coordinates:\"\n",
    "    f\"\\n  {neuron_coords[sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[sample_data_key].min():.3f}  max={neuron_coords[sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[sample_data_key].mean():.3f}  std={neuron_coords[sample_data_key].std():.3f}\"\n",
    ")\n",
    "\n",
    "### plot sample data\n",
    "sample_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[sample_idx].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop(stim[sample_idx].cpu(), config[\"crop_win\"]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "### bin the neuronal responses based on their neuron coordinates and sum within each bin -> 2D grid of vals\n",
    "coords = neuron_coords[sample_data_key]\n",
    "H, W = stim.shape[-2:] # the size of the grid\n",
    "n_x_bins, n_y_bins = 32, 18 # number of bins in each dimension\n",
    "min_x, max_x, min_y, max_y = coords[:,0].min().item(), coords[:,0].max().item(), coords[:,1].min().item(), coords[:,1].max().item()\n",
    "x_bins = torch.linspace(min_x, max_x, n_x_bins + 1)\n",
    "y_bins = torch.linspace(min_y, max_y, n_y_bins + 1)\n",
    "binned_resp = torch.zeros(n_y_bins, n_x_bins)\n",
    "for i in range(n_y_bins):\n",
    "    for j in range(n_x_bins):\n",
    "        ### mask of the neurons in the bin\n",
    "        mask = (x_bins[j] <= coords[:,0]) &\\\n",
    "               (coords[:,0] < x_bins[j + 1]) &\\\n",
    "               (y_bins[i] <= coords[:,1]) &\\\n",
    "               (coords[:,1] < y_bins[i + 1])\n",
    "        binned_resp[i,j] = resp[sample_idx, mask.cpu()].sum(0)\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(binned_resp.squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\" Extracts patches from the given images and encodes them with a pretrained encoder (\"on the fly\"). \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dl,\n",
    "        data_key,\n",
    "        patch_shape,\n",
    "        overlap=(0, 0),\n",
    "        stim_transform=None,\n",
    "        resp_transform=None,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        assert base_dl.batch_size == 1, \"Batch size must be 1.\"\n",
    "        assert len(overlap) == 2, \"Overlap must be a tuple of 2 elements.\"\n",
    "        self.base_dl = base_dl\n",
    "        self.base_dl_iter = iter(base_dl)\n",
    "        self.data_key = data_key\n",
    "        self.patch_shape = patch_shape\n",
    "        self.overlap = overlap\n",
    "        self.stim_transform = stim_transform\n",
    "        self.resp_transform = resp_transform\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = self._load_encoder()\n",
    "\n",
    "        self.seen_idxs = set()\n",
    "\n",
    "    def _load_encoder(self):\n",
    "        \"\"\" Load pretrained encoder (predefined config) and return it. \"\"\"\n",
    "        print(\"Loading encoder...\")\n",
    "        encoder = get_encoder(device=self.device, eval_mode=True)\n",
    "        return encoder.eval()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.seen_idxs:\n",
    "            raise ValueError(f\"Index {idx} already seen.\")\n",
    "        self.seen_idxs.add(idx)\n",
    "\n",
    "        sample = next(self.base_dl_iter)\n",
    "        img, pupil_center = sample.images.to(self.device), sample.pupil_center.to(self.device)\n",
    "\n",
    "        patches, syn_resps = self.extract_patches(img=img, pupil_center=pupil_center)\n",
    "        return patches, syn_resps, pupil_center.repeat(patches.shape[0], 1)\n",
    "\n",
    "    # def _scale_for_encoder(self, patches):\n",
    "    #     ### scale to 0-100\n",
    "    #     p_min, p_max = patches.min(), patches.max()\n",
    "    #     return (patches - p_min) / (p_max - p_min) * 100\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_patches(self, img, pupil_center=None):\n",
    "        h, w = img.shape[-2:]\n",
    "        patches = []\n",
    "        patch_shape = self.patch_shape\n",
    "\n",
    "        for y in range(0, h - patch_shape[0] + 1, patch_shape[0] - self.overlap[0]):\n",
    "            for x in range(0, w - patch_shape[1] + 1, patch_shape[1] - self.overlap[1]):\n",
    "                # patch = img[:, y:y+patch_size, x:x+patch_size]\n",
    "                patch = img[..., y:y+patch_shape[0], x:x+patch_shape[1]]\n",
    "                patches.append(patch)\n",
    "\n",
    "        # patches = torch.from_numpy(np.stack(patches)).float().to(self.device)\n",
    "        # patches = torch.stack(patches).float().to(self.device)\n",
    "        ### merge into a batch\n",
    "        patches = torch.cat(patches, dim=0).to(self.device)\n",
    "\n",
    "        ### encode patches = get resps\n",
    "        # if self.expand_stim_for_encoder:\n",
    "        #     patches_for_encoder = F.interpolate(patches, size=self.patch_size, mode=\"bilinear\", align_corners=False)\n",
    "        #     ### take only the center of the patch - the encoder's resps cover only the center part\n",
    "        #     patches = patches[:, :, int(patch_size / 4):int(patch_size / 4) + self.patch_size,\n",
    "        #                 int(patch_size / 4):int(patch_size / 4) + self.patch_size]\n",
    "        # patches = self._scale_for_encoder(patches)\n",
    "        if self.encoder is not None:\n",
    "            if hasattr(self.encoder, \"shifter\") and self.encoder.shifter is not None:\n",
    "                resps = self.encoder(patches, data_key=self.data_key, pupil_center=pupil_center.expand(patches.shape[0], -1))\n",
    "            else:\n",
    "                resps = self.encoder(patches, data_key=self.data_key)\n",
    "\n",
    "        if self.resp_transform is not None:\n",
    "            resps = self.resp_transform(resps)\n",
    "\n",
    "        if self.stim_transform is not None:\n",
    "            patches = self.stim_transform(patches)\n",
    "\n",
    "        return patches, resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchPatchesDataLoader():\n",
    "    # dataloader that mixes patches from different images within a batch\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader_iter = iter(dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader_iter)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader_iter:\n",
    "            patches, resps, pupil_center = batch  # (B, N_patches, C, H, W)\n",
    "            patches = patches.view(-1, *patches.shape[2:])\n",
    "            resps = resps.view(-1, *resps.shape[2:])\n",
    "            pupil_center = pupil_center.view(-1, *pupil_center.shape[2:])\n",
    "\n",
    "            ### shuffle patch-resp pairs\n",
    "            idx = torch.randperm(patches.shape[0])\n",
    "            patches = patches[idx]\n",
    "            resps = resps[idx].float()\n",
    "            pupil_center = pupil_center[idx]\n",
    "\n",
    "            yield patches, resps, pupil_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_config = {\n",
    "    \"data_part\": \"train\",\n",
    "    \"max_samples\": 15000, # None or int\n",
    "    \"patch_dataset\": {\n",
    "        \"data_key\": None, # to be set\n",
    "        \"patch_shape\": (36, 64),\n",
    "        \"overlap\": (0, 0),\n",
    "        \"stim_transform\": None,\n",
    "        \"resp_transform\": None,\n",
    "        \"device\": config[\"device\"],\n",
    "    },\n",
    "    \"patch_dataloader\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"shuffle\": False,\n",
    "    },\n",
    "}\n",
    "syn_data_config[\"patch_dataset\"][\"data_key\"] = dataloaders[\"mouse_v1\"][syn_data_config[\"data_part\"]].data_keys[0]\n",
    "print(f\"data_part: {syn_data_config['data_part']}   data_key: {syn_data_config['patch_dataset']['data_key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataloader = dataloaders[\"mouse_v1\"][syn_data_config[\"data_part\"]].dataloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dataset = SyntheticDataset(base_dl=base_dataloader, **syn_data_config[\"patch_dataset\"])\n",
    "patch_dataloader = DataLoader(patch_dataset, **syn_data_config[\"patch_dataloader\"])\n",
    "dl = BatchPatchesDataLoader(dataloader=patch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "s = patch_dataset[0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "axes = fig.subplots(4, 4).flatten()\n",
    "for i in range(16):\n",
    "    axes[i].imshow(s[0][i].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "for b in dl:\n",
    "    break\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "axes = fig.subplots(4, 8).flatten()\n",
    "for i in range(32):\n",
    "    axes[i].imshow(b[0][i].squeeze().cpu(), cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "save_dir = os.path.join(\n",
    "    DATA_PATH,\n",
    "    \"synthetic_data_mouse_v1_encoder_new_stimuli\",\n",
    "    syn_data_config['patch_dataset']['data_key'],\n",
    "    syn_data_config[\"data_part\"],\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"{save_dir=}\")\n",
    "\n",
    "### save config to parent folder\n",
    "with open(os.path.join(os.path.dirname(save_dir), f\"config_{syn_data_config['data_part']}.json\"), \"w\") as f:\n",
    "    json.dump(syn_data_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save\n",
    "sample_idx = 0\n",
    "for b_idx, (patches, resps, pupil_center) in enumerate(dl):\n",
    "    for i in range(patches.shape[0]):\n",
    "        sample_path = os.path.join(save_dir, f\"{sample_idx}.pickle\")\n",
    "        with open(sample_path, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"stim\": patches[i].cpu(),\n",
    "                \"resp\": resps[i].cpu(),\n",
    "                \"pupil_center\": pupil_center[i].cpu(),\n",
    "            }, f)\n",
    "        sample_idx += 1\n",
    "        if sample_idx >= syn_data_config[\"max_samples\"]:\n",
    "            break\n",
    "    if sample_idx >= syn_data_config[\"max_samples\"]:\n",
    "        break\n",
    "\n",
    "    if b_idx % 50 == 0:\n",
    "        print(f\"Batch {b_idx} processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append synthetic data\n",
    "config[\"data\"][\"syn_dataset_config\"] = {\n",
    "    \"data_keys\": [\n",
    "        # \"21067-10-18\",\n",
    "        # \"22846-10-16\",\n",
    "        # \"23343-5-17\",\n",
    "        # \"23656-14-22\",\n",
    "        \"23964-4-22\",\n",
    "    ],\n",
    "    \"batch_size\": 32,\n",
    "    \"append_data_parts\": [\"train\"],\n",
    "    # \"data_key_prefix\": \"syn\",\n",
    "    \"data_key_prefix\": None, # the same data key as the original (real) data\n",
    "    \"dir_name\": \"synthetic_data_mouse_v1_encoder_new_stimuli\",\n",
    "}\n",
    "\n",
    "dataloaders = append_syn_dataloaders(dataloaders, config=config[\"data\"][\"syn_dataset_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples, samples = 40000, 0\n",
    "resps_all = []\n",
    "for i, b in enumerate(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]):\n",
    "    resps_all.append(b.responses.cuda())\n",
    "    samples += b.responses.shape[0]\n",
    "    if samples >= max_samples:\n",
    "        break\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(len(resps_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all = torch.cat(resps_all, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = torch.quantile(resps_all, 0.75, dim=0) - torch.quantile(resps_all, 0.25, dim=0)\n",
    "med = torch.median(resps_all, dim=0).values\n",
    "mean = resps_all.mean(dim=0)\n",
    "std = resps_all.std(dim=0)\n",
    "iqr, med, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(DATA_PATH, dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname, \"stats\")):\n",
    "    os.makedirs(os.path.join(DATA_PATH, dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname, \"stats\"))\n",
    "else:\n",
    "    print(\"[WARNING] stats directory already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_iqr.npy\"\n",
    "    ),\n",
    "    iqr.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_mean.npy\"\n",
    "    ),\n",
    "    mean.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_med.npy\"\n",
    "    ),\n",
    "    med.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_std.npy\"\n",
    "    ),\n",
    "    std.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp, syn_pupil_center = next(iter(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]))\n",
    "syn_sample_data_key = dataloaders[\"mouse_v1\"][\"train\"].data_keys[-1]\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {syn_stim.shape}\"\n",
    "    f\"\\n  min={syn_stim.min().item():.3f}  max={syn_stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_stim.mean().item():.3f}  std={syn_stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {syn_resp.shape}\"\n",
    "    f\"\\n  min={syn_resp.min().item():.3f}  max={syn_resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_resp.mean().item():.3f}  std={syn_resp.std().item():.3f}\"\n",
    "    \"\\nNeuron coordinates:\"\n",
    "    f\"\\n  {neuron_coords[syn_sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[syn_sample_data_key].min():.3f}  max={neuron_coords[syn_sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[syn_sample_data_key].mean():.3f}  std={neuron_coords[syn_sample_data_key].std():.3f}\"\n",
    "    \"\\nPupil center:\"\n",
    "    f\"\\n  {syn_pupil_center.shape}\"\n",
    "    f\"\\n  min={syn_pupil_center.min().item():.3f}  max={syn_pupil_center.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_pupil_center.mean().item():.3f}  std={syn_pupil_center.std().item():.3f}\"\n",
    ")\n",
    "\n",
    "### plot sample data\n",
    "sample_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim[sample_idx].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop(syn_stim[sample_idx].cpu(), config[\"crop_win\"]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "### bin the neuronal responses based on their neuron coordinates and sum within each bin -> 2D grid of vals\n",
    "coords = neuron_coords[syn_sample_data_key]\n",
    "H, W = syn_stim.shape[-2:] # the size of the grid\n",
    "n_x_bins, n_y_bins = 32, 18 # number of bins in each dimension\n",
    "min_x, max_x, min_y, max_y = coords[:,0].min().item(), coords[:,0].max().item(), coords[:,1].min().item(), coords[:,1].max().item()\n",
    "x_bins = torch.linspace(min_x, max_x, n_x_bins + 1)\n",
    "y_bins = torch.linspace(min_y, max_y, n_y_bins + 1)\n",
    "binned_resp = torch.zeros(n_y_bins, n_x_bins)\n",
    "for i in range(n_x_bins):\n",
    "    for j in range(n_y_bins):\n",
    "        ### mask of the neurons in the bin\n",
    "        mask = (x_bins[i] <= coords[:,0]) &\\\n",
    "               (coords[:,0] < x_bins[i + 1]) &\\\n",
    "               (y_bins[j] <= coords[:,1]) &\\\n",
    "               (coords[:,1] < y_bins[j + 1])\n",
    "        binned_resp[j,i] = syn_resp[sample_idx, mask.cpu()].sum(0)\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(binned_resp.squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
