{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH='/media/jsobotka/ext_ssd/csng_data/mouse_v1_sensorium22'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "import wandb\n",
    "from nnfabrik.builder import get_data\n",
    "from nnfabrik.builder import get_data\n",
    "from lurz2020.datasets.mouse_loaders import static_loaders\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "from lurz2020.training.trainers import standard_trainer as trainer\n",
    "from lurz2020.utility.measures import get_correlations, get_fraction_oracles\n",
    "\n",
    "import csng\n",
    "from csng.CNN_Decoder import CNN_Decoder\n",
    "from csng.utils import RunningStats, crop, plot_comparison, standardize, normalize, get_mean_and_std, count_parameters, plot_losses\n",
    "from csng.losses import SSIMLoss, MSELossWithCrop, Loss\n",
    "from csng.data import MixedBatchLoader\n",
    "from csng.readins import (\n",
    "    MultiReadIn,\n",
    "    HypernetReadIn,\n",
    "    ConvReadIn,\n",
    "    AttentionReadIn,\n",
    "    FCReadIn,\n",
    "    AutoEncoderReadIn,\n",
    "    Conv1dReadIn,\n",
    "    LocalizedFCReadIn,\n",
    ")\n",
    "\n",
    "from encoder import get_encoder\n",
    "from data_utils import (\n",
    "    get_mouse_v1_data,\n",
    "    append_syn_dataloaders,\n",
    "    append_data_aug_dataloaders,\n",
    "    RespGaussianNoise,\n",
    ")\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"mouse_v1_sensorium22\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Running on cuda ...\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"sequential\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "    \"crop_win\": (22, 36),\n",
    "}\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(device=config[\"device\"], eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "### load pretrained encoder ckpt\n",
    "encoder_ckpt = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"encoder.pt\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "\n",
    "### get temporary dataloaders for the encoder\n",
    "_dataloaders = get_data(\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_fn\"],\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_config\"]\n",
    ")\n",
    "\n",
    "### init encoder\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_ckpt[\"config\"][\"encoder\"][\"model_config\"],\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=encoder_ckpt[\"config\"][\"seed\"],\n",
    ").float()\n",
    "encoder.load_state_dict(encoder_ckpt[\"encoder_state\"], strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### validate encoder is working (corr on val set should be ~ 0.32)\n",
    "train_correlation = get_correlations(encoder, _dataloaders[\"train\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "validation_correlation = get_correlations(encoder, _dataloaders[\"validation\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "test_correlation = get_correlations(encoder, _dataloaders[\"test\"], device=config[\"device\"], as_dict=False, per_neuron=False)\n",
    "\n",
    "print(\n",
    "    f\"Correlation (train set):      {train_correlation:.3f}\"\n",
    "    f\"\\nCorrelation (validation set): {validation_correlation:.3f}\"\n",
    "    f\"\\nCorrelation (test set):       {test_correlation:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mouse V1 dataset (Sensorium 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prep data config\n",
    "filenames = [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "    # \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # mouse 1\n",
    "    # \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # sensorium+ (mouse 2)\n",
    "    \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 3)\n",
    "    # \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 4)\n",
    "    # \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 5)\n",
    "    # \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 6)\n",
    "    # \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 7)\n",
    "]\n",
    "for f_idx, f_name in enumerate(filenames):\n",
    "    filenames[f_idx] = os.path.join(DATA_PATH, f_name)\n",
    "\n",
    "config[\"data\"].update({\n",
    "    \"paths\": filenames,\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": filenames,\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 0.25, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": False,\n",
    "        \"batch_size\": 128,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"normalize_neuron_coords\": True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get dataloaders and cell coordinates\n",
    "dataloaders, neuron_coords = get_mouse_v1_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "sample_data_key = dataloaders[\"mouse_v1\"][\"val\"].data_keys[0]\n",
    "datapoint = next(iter(dataloaders[\"mouse_v1\"][\"val\"].dataloaders[0]))\n",
    "stim, resp = datapoint.images, datapoint.responses\n",
    "pupil_center = datapoint.pupil_center\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * config['data']['dataset_config']['batch_size'] for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {stim.shape}\"\n",
    "    f\"\\n  min={stim.min().item():.3f}  max={stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={stim.mean().item():.3f}  std={stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {resp.shape}\"\n",
    "    f\"\\n  min={resp.min().item():.3f}  max={resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={resp.mean().item():.3f}  std={resp.std().item():.3f}\"\n",
    "    \"\\nneuronal coordinates:\"\n",
    "    f\"\\n  {neuron_coords[sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[sample_data_key].min():.3f}  max={neuron_coords[sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[sample_data_key].mean():.3f}  std={neuron_coords[sample_data_key].std():.3f}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "reshape_to = None\n",
    "for i in range(30, 150):\n",
    "    if resp.shape[-1] % i == 0:\n",
    "        reshape_to = (i, resp.shape[-1] // i)\n",
    "        break\n",
    "if reshape_to != None:\n",
    "    ax.imshow(resp[0].view(reshape_to).squeeze(0).unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data (1)\n",
    "- Using the same stimuli as in Sens22 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part = \"test\"\n",
    "assert len(dataloaders[\"mouse_v1\"][data_part].data_keys) == 1,\\\n",
    "    \"Create synthetic datasets one by one.\"\n",
    "data_key = dataloaders[\"mouse_v1\"][data_part].data_keys[0]\n",
    "save_stats = False\n",
    "\n",
    "print(f\"{data_part=}  {data_key=}  {save_stats=}\")\n",
    "\n",
    "trans_to_apply = [\n",
    "    {\n",
    "        \"name\": \"original\",\n",
    "        \"stim\": lambda x: x,\n",
    "        \"resp\": lambda x: x,\n",
    "        \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, data_part),\n",
    "        \"sample_idx\": 0,\n",
    "        \"stats\": RunningStats(num_components=encoder.readout[data_key].outdims, lib=\"torch\", device=config[\"device\"]),\n",
    "    },\n",
    "    # { ### noise to resp\n",
    "    #     \"name\": \"01noise_resp\n",
    "    #     \"stim\": lambda x: x,\n",
    "    #     \"resp\": lambda x: x + torch.randn(x.shape) * 0.1,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_01noise_resp\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### flip stim\n",
    "    #     \"name\": \"flip_stim\",\n",
    "    #     \"stim\": lambda x: x.flip(2),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_flip_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"01rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 1, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_01rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"02rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 2, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_02rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "    # { ### rotate stim\n",
    "    #     \"name\": \"03rotate_stim\",\n",
    "    #     \"stim\": lambda x: torch.rot90(x, 3, (1, 2)),\n",
    "    #     \"resp\": lambda x: x,\n",
    "    #     \"save_dir\": os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_part + \"_03rotate_stim\"),\n",
    "    #     \"sample_idx\": 0,\n",
    "    #     \"stats\": RunningStats(num_components=10000, lib=\"torch\", device=config[\"device\"]),\n",
    "    # },\n",
    "]\n",
    "\n",
    "### create dirs\n",
    "for tran_to_apply in trans_to_apply:\n",
    "    if os.path.exists(tran_to_apply[\"save_dir\"]) and len(os.listdir(tran_to_apply[\"save_dir\"])) > 0:\n",
    "        print(f\"[WARNING]: {tran_to_apply['save_dir']} already exists and is not empty.\")\n",
    "    os.makedirs(tran_to_apply[\"save_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = len(dataloaders[\"mouse_v1\"][data_part])\n",
    "\n",
    "with torch.no_grad():\n",
    "    ### run\n",
    "    for batch_idx, b in enumerate(dataloaders[\"mouse_v1\"][data_part]):\n",
    "        for _data_key, (stim, _, neuron_coords, pupil_center) in b.items():\n",
    "            assert _data_key == data_key, f\"Data key mismatch: {data_key} vs. {_data_key}\"\n",
    "            for tran_to_apply in trans_to_apply:\n",
    "                stim = tran_to_apply[\"stim\"](stim.to(config[\"device\"]))\n",
    "\n",
    "                ### forward\n",
    "                resp = encoder(stim, data_key=data_key)\n",
    "                resp = tran_to_apply[\"resp\"](resp)\n",
    "                if save_stats:\n",
    "                    tran_to_apply[\"stats\"].update(resp)\n",
    "\n",
    "                ### save\n",
    "                for i in range(stim.shape[0]):\n",
    "                    sample_path = os.path.join(tran_to_apply[\"save_dir\"], f\"{tran_to_apply['sample_idx']}.pickle\")\n",
    "                    with open(sample_path, \"wb\") as f:\n",
    "                        pickle.dump({\n",
    "                            \"stim\": stim[i].cpu(),\n",
    "                            \"resp\": resp[i].cpu(),\n",
    "                            \"pupil_center\": pupil_center[i].cpu(),\n",
    "                        }, f)\n",
    "                    tran_to_apply[\"sample_idx\"] += 1\n",
    "\n",
    "        ### log\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{n_batches}\")\n",
    "\n",
    "## save stats of responses\n",
    "if save_stats:\n",
    "    for tran_to_apply in trans_to_apply:\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_mean_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_mean().cpu().numpy(),\n",
    "        )\n",
    "        np.save(\n",
    "            os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_std_{tran_to_apply['name']}.npy\"),\n",
    "            tran_to_apply[\"stats\"].get_std().cpu().numpy(),\n",
    "        )\n",
    "\n",
    "## save neuron_coords\n",
    "p = os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"neuron_coords.npy\")\n",
    "np.save(p, neuron_coords.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load\n",
    "# resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", target_data_key, f\"responses_mean_original.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, f\"responses_std_original.npy\"))).float()\n",
    "\n",
    "div_by = resp_std.clone()\n",
    "thres = 0.01 * resp_std.mean()\n",
    "idx = resp_std <= thres\n",
    "div_by[idx] = thres\n",
    "\n",
    "dataset = PerSampleStoredDataset(\n",
    "    dataset_dir=os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", data_key, data_part),\n",
    "    stim_transform=lambda x: x, # stim is already normalized\n",
    "    resp_transform=csng.utils.Normalize(\n",
    "        # mean=resp_mean,\n",
    "        mean=0,\n",
    "        std=div_by,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = RunningStats(num_components=resp.shape[-1], lib=\"torch\", device=\"cpu\")\n",
    "for b, (s, r, pc) in enumerate(dataloader):\n",
    "    # stats.update(s.view(s.shape[0], -1))\n",
    "    stats.update(r)\n",
    "    if b % 50 == 0:\n",
    "        print(f\"Batch {b} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.get_mean(), stats.get_std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix wrongly calculated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append synthetic data\n",
    "config[\"data\"][\"syn_dataset_config\"] = {\n",
    "    \"data_keys\": [\n",
    "        \"21067-10-18\",\n",
    "        # \"22846-10-16\",\n",
    "        # \"23343-5-17\",\n",
    "        # \"23656-14-22\",\n",
    "        # \"23964-4-22\",\n",
    "    ],\n",
    "    \"batch_size\": 32,\n",
    "    \"append_data_parts\": [\"train\"],\n",
    "    # \"data_key_prefix\": \"syn\",\n",
    "    \"data_key_prefix\": None, # the same data key as the original (real) data\n",
    "}\n",
    "\n",
    "dataloaders = append_syn_dataloaders(dataloaders, config=config[\"data\"][\"syn_dataset_config\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all = []\n",
    "for b in dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]:\n",
    "    resps_all.append(b.responses.cuda())\n",
    "    print(len(resps_all))\n",
    "resps_all = torch.cat(resps_all, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all.mean(0), resps_all.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", config[\"data\"][\"syn_dataset_config\"][\"data_keys\"][0], f\"responses_mean_original.npy\"),\n",
    "    resps_all.mean(0).cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(DATA_PATH, \"synthetic_data_mouse_v1_encoder\", config[\"data\"][\"syn_dataset_config\"][\"data_keys\"][0], f\"responses_std_original.npy\"),\n",
    "    resps_all.std(0).cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic data (2)\n",
    "- Using new stimuli (crops from ImageNet)\n",
    "- Get stimuli from Sens22 data, crop 256x144 into different patches, and use as input to the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get base dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prep data config\n",
    "filenames = [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "    # \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # mouse 1\n",
    "    # \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # sensorium+ (mouse 2)\n",
    "    \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 3)\n",
    "    # \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 4)\n",
    "    # \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 5)\n",
    "    # \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 6)\n",
    "    # \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\", # pretraining (mouse 7)\n",
    "]\n",
    "for f_idx, f_name in enumerate(filenames):\n",
    "    filenames[f_idx] = os.path.join(DATA_PATH, f_name)\n",
    "\n",
    "config[\"data\"][\"mouse_v1\"] = {\n",
    "    \"paths\": filenames,\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": filenames,\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 1.0, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": False,\n",
    "        \"batch_size\": 1,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"normalize_neuron_coords\": True,\n",
    "    \"average_test_multitrial\": True,\n",
    "    \"save_test_multitrial\": True,\n",
    "    \"test_batch_size\": 1,\n",
    "    \"device\": config[\"device\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get dataloaders and cell coordinates\n",
    "dataloaders, neuron_coords = get_mouse_v1_data(config[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "sample_data_key = dataloaders[\"mouse_v1\"][\"train\"].data_keys[0]\n",
    "datapoint = next(iter(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[0]))\n",
    "stim, resp = datapoint.images, datapoint.responses\n",
    "pupil_center = datapoint.pupil_center\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {stim.shape}\"\n",
    "    f\"\\n  min={stim.min().item():.3f}  max={stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={stim.mean().item():.3f}  std={stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {resp.shape}\"\n",
    "    f\"\\n  min={resp.min().item():.3f}  max={resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={resp.mean().item():.3f}  std={resp.std().item():.3f}\"\n",
    "    \"\\nneuronal coordinates:\"\n",
    "    f\"\\n  {neuron_coords[sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[sample_data_key].min():.3f}  max={neuron_coords[sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[sample_data_key].mean():.3f}  std={neuron_coords[sample_data_key].std():.3f}\"\n",
    ")\n",
    "\n",
    "### plot sample data\n",
    "sample_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[sample_idx].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop(stim[sample_idx].cpu(), config[\"crop_win\"]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "### bin the neuronal responses based on their neuron coordinates and sum within each bin -> 2D grid of vals\n",
    "coords = neuron_coords[sample_data_key]\n",
    "H, W = stim.shape[-2:] # the size of the grid\n",
    "n_x_bins, n_y_bins = 32, 18 # number of bins in each dimension\n",
    "min_x, max_x, min_y, max_y = coords[:,0].min().item(), coords[:,0].max().item(), coords[:,1].min().item(), coords[:,1].max().item()\n",
    "x_bins = torch.linspace(min_x, max_x, n_x_bins + 1)\n",
    "y_bins = torch.linspace(min_y, max_y, n_y_bins + 1)\n",
    "binned_resp = torch.zeros(n_y_bins, n_x_bins)\n",
    "for i in range(n_y_bins):\n",
    "    for j in range(n_x_bins):\n",
    "        ### mask of the neurons in the bin\n",
    "        mask = (x_bins[j] <= coords[:,0]) &\\\n",
    "               (coords[:,0] < x_bins[j + 1]) &\\\n",
    "               (y_bins[i] <= coords[:,1]) &\\\n",
    "               (coords[:,1] < y_bins[i + 1])\n",
    "        binned_resp[i,j] = resp[sample_idx, mask.cpu()].sum(0)\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(binned_resp.squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\" Extracts patches from the given images and encodes them with a pretrained encoder (\"on the fly\"). \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dl,\n",
    "        data_key,\n",
    "        patch_shape,\n",
    "        overlap=(0, 0),\n",
    "        stim_transform=None,\n",
    "        resp_transform=None,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        assert base_dl.batch_size == 1, \"Batch size must be 1.\"\n",
    "        assert len(overlap) == 2, \"Overlap must be a tuple of 2 elements.\"\n",
    "        self.base_dl = base_dl\n",
    "        self.base_dl_iter = iter(base_dl)\n",
    "        self.data_key = data_key\n",
    "        self.patch_shape = patch_shape\n",
    "        self.overlap = overlap\n",
    "        self.stim_transform = stim_transform\n",
    "        self.resp_transform = resp_transform\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = self._load_encoder()\n",
    "\n",
    "        self.seen_idxs = set()\n",
    "\n",
    "    def _load_encoder(self):\n",
    "        \"\"\" Load pretrained encoder (predefined config) and return it. \"\"\"\n",
    "        print(\"Loading encoder...\")\n",
    "        encoder = get_encoder(device=self.device, eval_mode=True)\n",
    "        return encoder.eval()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.seen_idxs:\n",
    "            raise ValueError(f\"Index {idx} already seen.\")\n",
    "        self.seen_idxs.add(idx)\n",
    "\n",
    "        sample = next(self.base_dl_iter)\n",
    "        img, pupil_center = sample.images.to(self.device), sample.pupil_center.to(self.device)\n",
    "\n",
    "        patches, syn_resps = self.extract_patches(img=img, pupil_center=pupil_center)\n",
    "        return patches, syn_resps, pupil_center.repeat(patches.shape[0], 1)\n",
    "\n",
    "    # def _scale_for_encoder(self, patches):\n",
    "    #     ### scale to 0-100\n",
    "    #     p_min, p_max = patches.min(), patches.max()\n",
    "    #     return (patches - p_min) / (p_max - p_min) * 100\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_patches(self, img, pupil_center=None):\n",
    "        h, w = img.shape[-2:]\n",
    "        patches = []\n",
    "        patch_shape = self.patch_shape\n",
    "\n",
    "        for y in range(0, h - patch_shape[0] + 1, patch_shape[0] - self.overlap[0]):\n",
    "            for x in range(0, w - patch_shape[1] + 1, patch_shape[1] - self.overlap[1]):\n",
    "                # patch = img[:, y:y+patch_size, x:x+patch_size]\n",
    "                patch = img[..., y:y+patch_shape[0], x:x+patch_shape[1]]\n",
    "                patches.append(patch)\n",
    "\n",
    "        # patches = torch.from_numpy(np.stack(patches)).float().to(self.device)\n",
    "        # patches = torch.stack(patches).float().to(self.device)\n",
    "        ### merge into a batch\n",
    "        patches = torch.cat(patches, dim=0).to(self.device)\n",
    "\n",
    "        ### encode patches = get resps\n",
    "        # if self.expand_stim_for_encoder:\n",
    "        #     patches_for_encoder = F.interpolate(patches, size=self.patch_size, mode=\"bilinear\", align_corners=False)\n",
    "        #     ### take only the center of the patch - the encoder's resps cover only the center part\n",
    "        #     patches = patches[:, :, int(patch_size / 4):int(patch_size / 4) + self.patch_size,\n",
    "        #                 int(patch_size / 4):int(patch_size / 4) + self.patch_size]\n",
    "        # patches = self._scale_for_encoder(patches)\n",
    "        if self.encoder is not None:\n",
    "            if hasattr(self.encoder, \"shifter\") and self.encoder.shifter is not None:\n",
    "                resps = self.encoder(patches, data_key=self.data_key, pupil_center=pupil_center.expand(patches.shape[0], -1))\n",
    "            else:\n",
    "                resps = self.encoder(patches, data_key=self.data_key)\n",
    "\n",
    "        if self.resp_transform is not None:\n",
    "            resps = self.resp_transform(resps)\n",
    "\n",
    "        if self.stim_transform is not None:\n",
    "            patches = self.stim_transform(patches)\n",
    "\n",
    "        return patches, resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchPatchesDataLoader():\n",
    "    # dataloader that mixes patches from different images within a batch\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader_iter = iter(dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader_iter)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader_iter:\n",
    "            patches, resps, pupil_center = batch  # (B, N_patches, C, H, W)\n",
    "            patches = patches.view(-1, *patches.shape[2:])\n",
    "            resps = resps.view(-1, *resps.shape[2:])\n",
    "            pupil_center = pupil_center.view(-1, *pupil_center.shape[2:])\n",
    "\n",
    "            ### shuffle patch-resp pairs\n",
    "            idx = torch.randperm(patches.shape[0])\n",
    "            patches = patches[idx]\n",
    "            resps = resps[idx].float()\n",
    "            pupil_center = pupil_center[idx]\n",
    "\n",
    "            yield patches, resps, pupil_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_config = {\n",
    "    \"data_part\": \"train\",\n",
    "    \"patch_dataset\": {\n",
    "        \"data_key\": None, # to be set\n",
    "        \"patch_shape\": (36, 64),\n",
    "        \"overlap\": (0, 0),\n",
    "        \"stim_transform\": None,\n",
    "        \"resp_transform\": None,\n",
    "        \"device\": config[\"device\"],\n",
    "    },\n",
    "    \"patch_dataloader\": {\n",
    "        \"batch_size\": 4,\n",
    "        \"shuffle\": False,\n",
    "    },\n",
    "}\n",
    "syn_data_config[\"patch_dataset\"][\"data_key\"] = dataloaders[\"mouse_v1\"][syn_data_config[\"data_part\"]].data_keys[0]\n",
    "print(f\"data_part: {syn_data_config['data_part']}   data_key: {syn_data_config['patch_dataset']['data_key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataloader = dataloaders[\"mouse_v1\"][syn_data_config[\"data_part\"]].dataloaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dataset = SyntheticDataset(base_dl=base_dataloader, **syn_data_config[\"patch_dataset\"])\n",
    "patch_dataloader = DataLoader(patch_dataset, **syn_data_config[\"patch_dataloader\"])\n",
    "dl = BatchPatchesDataLoader(dataloader=patch_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "s = patch_dataset[0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "axes = fig.subplots(4, 4).flatten()\n",
    "for i in range(16):\n",
    "    axes[i].imshow(s[0][i].squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "for b in dl:\n",
    "    break\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "axes = fig.subplots(4, 8).flatten()\n",
    "for i in range(32):\n",
    "    axes[i].imshow(b[0][i].squeeze().cpu(), cmap=\"gray\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "data_part = \"train\"\n",
    "save_dir = os.path.join(\n",
    "    DATA_PATH,\n",
    "    \"synthetic_data_mouse_v1_encoder_new_stimuli\",\n",
    "    data_key,\n",
    "    syn_data_config[\"data_part\"],\n",
    ")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"{save_dir=}\")\n",
    "\n",
    "### save config to parent folder\n",
    "with open(os.path.join(os.path.dirname(save_dir), f\"config_{syn_data_config['data_part']}.json\"), \"w\") as f:\n",
    "    json.dump(syn_data_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save\n",
    "sample_idx = 0\n",
    "for b_idx, (patches, resps, pupil_center) in enumerate(dl):\n",
    "    for i in range(patches.shape[0]):\n",
    "        sample_path = os.path.join(save_dir, f\"{sample_idx}.pickle\")\n",
    "        with open(sample_path, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"stim\": patches[i].cpu(),\n",
    "                \"resp\": resps[i].cpu(),\n",
    "                \"pupil_center\": pupil_center[i].cpu(),\n",
    "            }, f)\n",
    "        sample_idx += 1\n",
    "\n",
    "    if b_idx % 50 == 0:\n",
    "        print(f\"Batch {b_idx} processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append synthetic data\n",
    "config[\"data\"][\"syn_dataset_config\"] = {\n",
    "    \"data_keys\": [\n",
    "        \"21067-10-18\",\n",
    "        # \"22846-10-16\",\n",
    "        # \"23343-5-17\",\n",
    "        # \"23656-14-22\",\n",
    "        # \"23964-4-22\",\n",
    "    ],\n",
    "    \"batch_size\": 32,\n",
    "    \"append_data_parts\": [\"train\"],\n",
    "    # \"data_key_prefix\": \"syn\",\n",
    "    \"data_key_prefix\": None, # the same data key as the original (real) data\n",
    "    \"dir_name\": \"synthetic_data_mouse_v1_encoder_new_stimuli\",\n",
    "}\n",
    "\n",
    "dataloaders = append_syn_dataloaders(dataloaders, config=config[\"data\"][\"syn_dataset_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples, samples = 40000, 0\n",
    "resps_all = []\n",
    "for i, b in enumerate(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]):\n",
    "    resps_all.append(b.responses.cuda())\n",
    "    samples += b.responses.shape[0]\n",
    "    if samples >= max_samples:\n",
    "        break\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(len(resps_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps_all = torch.cat(resps_all, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = torch.quantile(resps_all, 0.75, dim=0) - torch.quantile(resps_all, 0.25, dim=0)\n",
    "med = torch.median(resps_all, dim=0).values\n",
    "mean = resps_all.mean(dim=0)\n",
    "std = resps_all.std(dim=0)\n",
    "iqr, med, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(DATA_PATH, dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname, \"stats\")):\n",
    "    os.makedirs(os.path.join(DATA_PATH, dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname, \"stats\"))\n",
    "else:\n",
    "    print(\"[WARNING] stats directory already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_iqr.npy\"\n",
    "    ),\n",
    "    iqr.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_mean.npy\"\n",
    "    ),\n",
    "    mean.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_med.npy\"\n",
    "    ),\n",
    "    med.cpu().numpy(),\n",
    ")\n",
    "np.save(\n",
    "    os.path.join(\n",
    "        DATA_PATH,\n",
    "        dataloaders[\"mouse_v1\"][\"train\"].datasets[-1].dirname,\n",
    "        \"stats\",\n",
    "        f\"responses_std.npy\"\n",
    "    ),\n",
    "    std.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\t 76057 samples\n",
      "Validation dataset:\t 523 samples\n",
      "Test dataset:\t\t 100 samples\n",
      "Test (no resp) dataset:\t 0 samples\n",
      "\n",
      "stimuli:\n",
      "  torch.Size([32, 1, 36, 64])\n",
      "  min=-1.750  max=2.198\n",
      "  mean=-0.016  std=0.781\n",
      "responses:\n",
      "  torch.Size([32, 8372])\n",
      "  min=-1.375  max=15.659\n",
      "  mean=0.223  std=0.895\n",
      "Neuron coordinates:\n",
      "  torch.Size([8372, 3])\n",
      "  min=-1.000  max=1.000\n",
      "  mean=0.002  std=0.593\n",
      "Pupil center:\n",
      "  torch.Size([32, 2])\n",
      "  min=-0.238  max=0.776\n",
      "  mean=0.233  std=0.371\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp, syn_pupil_center = next(iter(dataloaders[\"mouse_v1\"][\"train\"].dataloaders[-1]))\n",
    "syn_sample_data_key = dataloaders[\"mouse_v1\"][\"train\"].data_keys[-1]\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {syn_stim.shape}\"\n",
    "    f\"\\n  min={syn_stim.min().item():.3f}  max={syn_stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_stim.mean().item():.3f}  std={syn_stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {syn_resp.shape}\"\n",
    "    f\"\\n  min={syn_resp.min().item():.3f}  max={syn_resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_resp.mean().item():.3f}  std={syn_resp.std().item():.3f}\"\n",
    "    \"\\nNeuron coordinates:\"\n",
    "    f\"\\n  {neuron_coords[syn_sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[syn_sample_data_key].min():.3f}  max={neuron_coords[syn_sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[syn_sample_data_key].mean():.3f}  std={neuron_coords[syn_sample_data_key].std():.3f}\"\n",
    "    \"\\nPupil center:\"\n",
    "    f\"\\n  {syn_pupil_center.shape}\"\n",
    "    f\"\\n  min={syn_pupil_center.min().item():.3f}  max={syn_pupil_center.max().item():.3f}\"\n",
    "    f\"\\n  mean={syn_pupil_center.mean().item():.3f}  std={syn_pupil_center.std().item():.3f}\"\n",
    ")\n",
    "\n",
    "### plot sample data\n",
    "sample_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim[sample_idx].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop(syn_stim[sample_idx].cpu(), config[\"crop_win\"]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "### bin the neuronal responses based on their neuron coordinates and sum within each bin -> 2D grid of vals\n",
    "coords = neuron_coords[syn_sample_data_key]\n",
    "H, W = syn_stim.shape[-2:] # the size of the grid\n",
    "n_x_bins, n_y_bins = 32, 18 # number of bins in each dimension\n",
    "min_x, max_x, min_y, max_y = coords[:,0].min().item(), coords[:,0].max().item(), coords[:,1].min().item(), coords[:,1].max().item()\n",
    "x_bins = torch.linspace(min_x, max_x, n_x_bins + 1)\n",
    "y_bins = torch.linspace(min_y, max_y, n_y_bins + 1)\n",
    "binned_resp = torch.zeros(n_y_bins, n_x_bins)\n",
    "for i in range(n_x_bins):\n",
    "    for j in range(n_y_bins):\n",
    "        ### mask of the neurons in the bin\n",
    "        mask = (x_bins[i] <= coords[:,0]) &\\\n",
    "               (coords[:,0] < x_bins[i + 1]) &\\\n",
    "               (y_bins[j] <= coords[:,1]) &\\\n",
    "               (coords[:,1] < y_bins[j + 1])\n",
    "        binned_resp[j,i] = syn_resp[sample_idx, mask.cpu()].sum(0)\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(binned_resp.squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
