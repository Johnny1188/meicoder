{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "import csng\n",
    "from csng.utils.mix import RunningStats\n",
    "from csng.utils.data import crop, normalize\n",
    "from csng.cat_v1.data import get_cat_v1_dataloaders\n",
    "\n",
    "lt.monkey_patch()\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\", \"50K_single_trial_dataset\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"sequential\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "subdirs = [\"train\", \"val\"]\n",
    "train_ratio = 0.8\n",
    "all_samples = sorted(os.listdir(os.path.join(DATA_PATH, \"single_trial\")))\n",
    "total_samples = len(all_samples)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = total_samples - train_samples\n",
    "print(f\"{train_samples=}, {val_samples=}\")\n",
    "\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(DATA_PATH, \"datasets\", subdir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split into subfolders\n",
    "for sample_idx, sample_name in enumerate(all_samples):\n",
    "    if sample_idx < train_samples:\n",
    "        subdir = subdirs[0]\n",
    "    elif sample_idx < train_samples + val_samples:\n",
    "        subdir = subdirs[1]\n",
    "    else:\n",
    "        subdir = subdirs[2]\n",
    "    \n",
    "    ### move file\n",
    "    stim = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"stimulus.npy\"))\n",
    "    exc_resp = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"V1_Exc_L23.npy\"))\n",
    "    inh_resp = np.load(os.path.join(DATA_PATH, \"single_trial\", sample_name, \"V1_Inh_L23.npy\"))\n",
    "    # save as pickle\n",
    "    with open(os.path.join(DATA_PATH, \"datasets\", subdir, f\"{sample_name}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"stim\": stim,\n",
    "            \"exc_resp\": exc_resp,\n",
    "            \"inh_resp\": inh_resp,\n",
    "        }, f)\n",
    "\n",
    "### remove previous directory\n",
    "shutil.rmtree(os.path.join(DATA_PATH, \"single_trial\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move and preprocess multi-trial test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = os.path.join(DATA_PATH, \"datasets\", \"test\")\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "test_data_path = os.path.join(DATA_PATH, \"Dataset_multitrial\", \"Dic23data\", \"multitrial\")\n",
    "samples = sorted(os.listdir(test_data_path))\n",
    "print(f\"{len(samples)=},  {target_dir=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess test multi-trial data and save as pickle\n",
    "for sample_name in samples:\n",
    "    sample_dir = os.path.join(test_data_path, sample_name)\n",
    "\n",
    "    ### move files\n",
    "    stim = np.load(os.path.join(sample_dir, \"stimulus.npy\"))\n",
    "    all_exc_resp = []\n",
    "    all_inh_resp = []\n",
    "    for trial_dir_name in os.listdir(sample_dir):\n",
    "        if trial_dir_name == \"stimulus.npy\":\n",
    "            continue\n",
    "        exc_resp = np.load(os.path.join(sample_dir, trial_dir_name, \"V1_Exc_L23.npy\"))\n",
    "        inh_resp = np.load(os.path.join(sample_dir, trial_dir_name, \"V1_Inh_L23.npy\"))\n",
    "        all_exc_resp.append(exc_resp)\n",
    "        all_inh_resp.append(inh_resp)\n",
    "    exc_resp = np.stack(all_exc_resp, axis=0)\n",
    "    inh_resp = np.stack(all_inh_resp, axis=0)\n",
    "\n",
    "    ### save as pickle\n",
    "    with open(os.path.join(target_dir, f\"{sample_name}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"stim\": stim,\n",
    "            \"exc_resp\": exc_resp,\n",
    "            \"inh_resp\": inh_resp,\n",
    "        }, f)\n",
    "    \n",
    "    ### remove sample_name directory\n",
    "    shutil.rmtree(sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"datasets\", \"test\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 1000,\n",
    "    \"stim_keys\": (\"stim\",),\n",
    "    \"resp_keys\": (\"exc_resp\", \"inh_resp\"),\n",
    "    # \"stim_normalize_mean\": 46.143,\n",
    "    # \"stim_normalize_std\": 20.420,\n",
    "    # \"resp_normalize_mean\": torch.load(\n",
    "    #     os.path.join(DATA_PATH, \"responses_mean.pt\")\n",
    "    # ),\n",
    "    # \"resp_normalize_std\": torch.load(\n",
    "    #     os.path.join(DATA_PATH, \"responses_std.pt\")\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### image stimuli\n",
    "v1_dataloaders = get_cat_v1_dataloaders(**config[\"data\"][\"v1_data\"])\n",
    "dataloader = torch.utils.data.DataLoader(v1_dataloaders[\"train\"].dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "mean_inputs, std_inputs = torch.zeros(1), torch.zeros(1)\n",
    "for inp_idx, (inputs, targets) in enumerate(dataloader):\n",
    "    for c in range(inputs.size(1)):\n",
    "        mean_inputs[c] += inputs[:,c,:,:].mean((-1,-2)).mean()\n",
    "        std_inputs[c] += inputs[:,c,:,:].std((-1,-2)).mean()\n",
    "mean_inputs.div_(len(dataloader))\n",
    "std_inputs.div_(len(dataloader))\n",
    "mean_inputs, std_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### responses\n",
    "v1_dataloaders = get_cat_v1_dataloaders(**config[\"data\"][\"v1_data\"])\n",
    "dataloader = torch.utils.data.DataLoader(v1_dataloaders[\"train\"].dataset, batch_size=1000, shuffle=True)\n",
    "stats_all = RunningStats(num_components=46875, lib=\"torch\", device=\"cpu\")\n",
    "stats_exc = RunningStats(num_components=37500, lib=\"torch\", device=\"cpu\")\n",
    "stats_inh = RunningStats(num_components=9375, lib=\"torch\", device=\"cpu\")\n",
    "for i, (s, r) in enumerate(dataloader):\n",
    "    stats_all.update(r)\n",
    "    stats_exc.update(r[:,:37500])\n",
    "    stats_inh.update(r[:,37500:])\n",
    "    if i % 200 == 0:\n",
    "        print(f\"{i}: {r.mean()=} {r.std()=} {stats_all.get_mean()=} {stats_all.get_std()=}\")\n",
    "\n",
    "### save\n",
    "torch.save(stats_all.get_mean(), os.path.join(DATA_PATH, \"responses_mean.pt\"))\n",
    "torch.save(stats_all.get_std(), os.path.join(DATA_PATH, \"responses_std.pt\"))\n",
    "torch.save(stats_exc.get_mean(), os.path.join(DATA_PATH, \"responses_exc_mean.pt\"))\n",
    "torch.save(stats_exc.get_std(), os.path.join(DATA_PATH, \"responses_exc_std.pt\"))\n",
    "torch.save(stats_inh.get_mean(), os.path.join(DATA_PATH, \"responses_inh_mean.pt\"))\n",
    "torch.save(stats_inh.get_std(), os.path.join(DATA_PATH, \"responses_inh_std.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
