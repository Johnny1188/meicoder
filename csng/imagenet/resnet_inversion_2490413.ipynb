{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:05.430686Z",
     "iopub.status.busy": "2024-12-11T14:50:05.430536Z",
     "iopub.status.idle": "2024-12-11T14:50:12.471549Z",
     "shell.execute_reply": "2024-12-11T14:50:12.470905Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.nn.functional as batch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from random import random\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:12.475334Z",
     "iopub.status.busy": "2024-12-11T14:50:12.474431Z",
     "iopub.status.idle": "2024-12-11T14:50:12.502090Z",
     "shell.execute_reply": "2024-12-11T14:50:12.501641Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1235430919.py, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 44\u001b[0;36m\u001b[0m\n\u001b[0;31m    CachedImageNet(ImageNet):\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Define the image limit\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TRAIN_SIZE = 500 * BATCH_SIZE\n",
    "TEST_SIZE = 20 * BATCH_SIZE\n",
    "LOSS_EVERY = 5\n",
    "EPOCHS = 1\n",
    "SPLIT = 'train'\n",
    "WEIGHT_DECAY=5e-3\n",
    "LEARNING_RATE = .00005\n",
    "\n",
    "device = os.environ[\"DEVICE\"]\n",
    "\n",
    "DATA_DIR = os.environ['DATA_PATH']\n",
    "CACHE_DIR = join(DATA_DIR, 'imagenet_inversion')\n",
    "RESNET_SIZE = 224\n",
    "\n",
    "\n",
    "class ResnetExtractor:\n",
    "    def __init__(self):\n",
    "        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "        self.resnet50.eval().to(device)\n",
    "        self.resnet_layer = self.resnet50.layers[2][0].downsample[0]\n",
    "\n",
    "    def get_features(self, img):\n",
    "        IMGNET_NORM = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        imgnet_input = F.rgb_to_grayscale(IMGNET_NORM(img), num_output_channels=3).to(device).unsqueeze(0)\n",
    "\n",
    "        def assign_features(module, input, output):\n",
    "            nonlocal features\n",
    "            features = output\n",
    "        hook = self.resnet_layer.register_forward_hook(assign_features)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.resnet50(imgnet_input)\n",
    "        hook.remove()\n",
    "        assert features is not None\n",
    "        features = features.squeeze(0)\n",
    "        return features\n",
    "\n",
    "\n",
    "CachedImageNet(ImageNet):\n",
    "\n",
    "    def __init__(self, root, split, version='0.1'):\n",
    "        TRANSFORM = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        super().__init__(root=root, split=split, transform=TRANSFORM)\n",
    "        self.cache_dir = os.path.join(CACHE_DIR, version, split)\n",
    "\n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        self.resnet_extractor = None\n",
    "\n",
    "    def get_extractor(self):\n",
    "        if self.resnet_extractor is None:\n",
    "            self.resnet_extractor = ResnetExtractor()\n",
    "        return self.resnet_extractor\n",
    "\n",
    "    def _get_cache_path(self, index):\n",
    "        \"\"\"Get the path for the cached feature.\"\"\"\n",
    "        return os.path.join(self.cache_dir, f'feature_{index}.pt')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load the image and label\n",
    "        img, object_class = super().__getitem__(index)\n",
    "\n",
    "        cache_path = self._get_cache_path(index)\n",
    "        features = None\n",
    "        if not os.path.exists(cache_path):\n",
    "            # # Image transformations\n",
    "            # if os.path.exists(cache_path):\n",
    "            #     saved_features = torch.load(cache_path).to(device)\n",
    "            #     assert torch.equal(features,saved_features)\n",
    "            features = self.get_extractor().get_features(img)\n",
    "            self.save_atomic(features, cache_path)\n",
    "        else:\n",
    "            features = torch.load(cache_path).to(device)\n",
    "\n",
    "        target = F.rgb_to_grayscale(img, num_output_channels=1)\n",
    "        return features, target\n",
    "    def save_atomic(self, value, name):\n",
    "        tmp_name = name + '.tmp' + str(random())\n",
    "        torch.save(value, tmp_name)\n",
    "        os.replace(tmp_name, name)\n",
    "# Example feature extractor: compute mean/std per channel\n",
    "\n",
    "\n",
    "\n",
    "# Set the start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "# Initialize CachedImageNet\n",
    "train_dataset = CachedImageNet(\n",
    "    root=join(DATA_DIR, 'imagenet'),\n",
    "    split=SPLIT,\n",
    "    version='1.1'\n",
    ")\n",
    "\n",
    "\n",
    "assert(len(train_dataset) >= TRAIN_SIZE + TEST_SIZE)\n",
    "\n",
    "# Define the split sizes\n",
    "\n",
    "# Split the dataset within the limit\n",
    "train_subset_indices = list(range(TRAIN_SIZE))  # Indices for the training subset\n",
    "test_subset_indices = list(range(TRAIN_SIZE, TRAIN_SIZE + TEST_SIZE))  # Indices for the test subset\n",
    "\n",
    "# Create the subsets\n",
    "train_subset = Subset(train_dataset, train_subset_indices)\n",
    "test_subset = Subset(train_dataset, test_subset_indices)\n",
    "\n",
    "# Define the batch size\n",
    "\n",
    "# Create DataLoaders for train and test\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle can be enabled based on your training needs\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train loader contains {len(train_loader)} batches.\")\n",
    "print(f\"Test loader contains {len(test_loader)} batches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:12.530225Z",
     "iopub.status.busy": "2024-12-11T14:50:12.530053Z",
     "iopub.status.idle": "2024-12-11T14:50:12.710509Z",
     "shell.execute_reply": "2024-12-11T14:50:12.709970Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 9\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m)  \u001b[38;5;66;03m# Replace `train_loader` with your DataLoader\u001b[39;00m\n\u001b[1;32m     10\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(targets\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a batch from the DataLoader\n",
    "def show_image(img_tensor):\n",
    "    img = img_tensor.cpu().numpy().transpose((1, 2, 0))  # Convert to HWC\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data_iter = iter(train_loader)  # Replace `train_loader` with your DataLoader\n",
    "features, targets = next(data_iter)\n",
    "print(targets.shape)\n",
    "show_image(targets[0])  # Show the first image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:12.713521Z",
     "iopub.status.busy": "2024-12-11T14:50:12.712737Z",
     "iopub.status.idle": "2024-12-11T14:50:14.301722Z",
     "shell.execute_reply": "2024-12-11T14:50:14.301181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parametrs:  24511329\n",
      "Params in samples :  488.5070352359694\n"
     ]
    }
   ],
   "source": [
    "class UpsampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpsampleModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=1024, out_channels=768,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=768, out_channels=512,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=512, out_channels=384,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=384, out_channels=256,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = UpsampleModel()\n",
    "\n",
    "\n",
    "input_tensor = torch.randn(1, 1024, 14, 14)  # Batch size = 1\n",
    "output_tensor = model(input_tensor)\n",
    "assert output_tensor.shape == torch.Size([1,1,224,224])\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "params = count_parameters(model)\n",
    "print(\"Total parametrs: \", params)\n",
    "print(\"Params in samples : \", params / (224*224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.304838Z",
     "iopub.status.busy": "2024-12-11T14:50:14.304036Z",
     "iopub.status.idle": "2024-12-11T14:50:14.338096Z",
     "shell.execute_reply": "2024-12-11T14:50:14.337592Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LEARNING_RATE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mLEARNING_RATE\u001b[49m, weight_decay\u001b[38;5;241m=\u001b[39mWEIGHT_DECAY)\n\u001b[1;32m      4\u001b[0m T_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m*\u001b[39m EPOCHS  \u001b[38;5;66;03m# Total number of batches across all epochs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LEARNING_RATE' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "T_max = len(train_loader) * EPOCHS  # Total number of batches across all epochs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.340967Z",
     "iopub.status.busy": "2024-12-11T14:50:14.340206Z",
     "iopub.status.idle": "2024-12-11T14:50:14.375313Z",
     "shell.execute_reply": "2024-12-11T14:50:14.374952Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m      7\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "model.to(device)\n",
    "running_loss = 0.0\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (features, target) in enumerate(train_loader):\n",
    "        features = features.to(device)  # Move to device\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step the scheduler after each batch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log the running loss\n",
    "        running_loss += loss.item()\n",
    "        if i % LOSS_EVERY == LOSS_EVERY - 1:\n",
    "            avg_loss = running_loss / LOSS_EVERY\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}, LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {avg_loss:.4f}\")\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.378088Z",
     "iopub.status.busy": "2024-12-11T14:50:14.377328Z",
     "iopub.status.idle": "2024-12-11T14:50:14.406189Z",
     "shell.execute_reply": "2024-12-11T14:50:14.405474Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39myscale(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# set maximum y to \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "# set maximum y to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.409009Z",
     "iopub.status.busy": "2024-12-11T14:50:14.408187Z",
     "iopub.status.idle": "2024-12-11T14:50:14.446288Z",
     "shell.execute_reply": "2024-12-11T14:50:14.445754Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_loss, all_targets, all_predictions\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Evaluate on the first 10% of the test dataset\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m test_loss_partial, partial_targets, partial_predictions \u001b[38;5;241m=\u001b[39m evaluate_partial_model(model, \u001b[43mtest_loader\u001b[49m, device)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_partial\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_examples\u001b[39m(targets, predictions, num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_partial_model(model, test_loader, device, max_batches=10_000):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    num_batches = min(max_batches, len(test_loader)) \n",
    "    processed_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, target in tqdm(test_loader):\n",
    "            if processed_batches >= num_batches:\n",
    "                break\n",
    "\n",
    "            features = features.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Predict\n",
    "            predictions = model(features)\n",
    "\n",
    "            loss = criterion(predictions, target)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Store for plotting\n",
    "            all_targets.append(target.cpu())\n",
    "            all_predictions.append(predictions.cpu())\n",
    "\n",
    "            processed_batches += 1\n",
    "\n",
    "    test_loss /= processed_batches\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    return test_loss, all_targets, all_predictions\n",
    "\n",
    "# Evaluate on the first 10% of the test dataset\n",
    "test_loss_partial, partial_targets, partial_predictions = evaluate_partial_model(model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss_partial:.4f}\")\n",
    "\n",
    "def plot_examples(targets, predictions, num_examples=5):\n",
    "    plt.figure(figsize=(15, num_examples * 3))\n",
    "    for i in range(num_examples):\n",
    "        # Plot the actual target\n",
    "        plt.subplot(num_examples, 2, 2 * i + 1)\n",
    "        plt.imshow(targets[i].squeeze(), cmap='gray')  # Squeeze to remove extra dimension\n",
    "        plt.title(\"Actual Target\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot the predicted target\n",
    "        plt.subplot(num_examples, 2, 2 * i + 2)\n",
    "        plt.imshow(predictions[i].squeeze(), cmap='gray')  # Squeeze to remove extra dimension\n",
    "        plt.title(\"Predicted Target\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot Examples\n",
    "num_examples = 5  # Number of examples to visualize\n",
    "plot_examples(partial_targets[:num_examples].cpu().numpy(), partial_predictions[:num_examples].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.449009Z",
     "iopub.status.busy": "2024-12-11T14:50:14.448245Z",
     "iopub.status.idle": "2024-12-11T14:50:14.476476Z",
     "shell.execute_reply": "2024-12-11T14:50:14.476107Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpartial_predictions\u001b[49m\u001b[38;5;241m.\u001b[39mmin(), partial_predictions\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(partial_targets\u001b[38;5;241m.\u001b[39mmin(), partial_targets\u001b[38;5;241m.\u001b[39mmax())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "print(partial_predictions.min(), partial_predictions.max())\n",
    "print(partial_targets.min(), partial_targets.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T14:50:14.479173Z",
     "iopub.status.busy": "2024-12-11T14:50:14.478422Z",
     "iopub.status.idle": "2024-12-11T14:50:14.592280Z",
     "shell.execute_reply": "2024-12-11T14:50:14.591827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to  /scratch/izar/vanousek/cs-433-project/models/mlp2490413.pt\n"
     ]
    }
   ],
   "source": [
    "name_extra = os.environ['SLURM_JOB_ID'] if 'SLURM_JOB_ID' in os.environ else ''\n",
    "name = join(os.environ['MODELS_PATH'], \"mlp\" + name_extra + \".pt\")\n",
    "torch.save(model, name)\n",
    "print(\"Model saved to \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
