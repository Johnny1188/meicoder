{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import GaussianBlur\n",
    "import lovely_tensors as lt\n",
    "from nnfabrik.builder import get_data\n",
    "\n",
    "import csng\n",
    "from csng.InvertedEncoder import InvertedEncoder\n",
    "from csng.utils import crop, plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss, Loss\n",
    "from csng.data import MixedBatchLoader\n",
    "\n",
    "from csng.comparison import get_metrics, load_decoder_from_ckpt\n",
    "from csng.mouse_v1.encoder import get_encoder\n",
    "from csng.mouse_v1.data_utils import get_mouse_v1_data, PerSampleStoredDataset\n",
    "from csng.mouse_v1.cnn_decoder_utils import get_all_data\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"mouse_v1_sensorium22\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "    \"crop_win\": (22, 36),\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prep data config\n",
    "config[\"data\"][\"mouse_v1\"] = {\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "            # os.path.join(DATA_PATH, \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # mouse 1\n",
    "            # os.path.join(DATA_PATH, \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # sensorium+ (mouse 2)\n",
    "            os.path.join(DATA_PATH, \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 3)\n",
    "            # os.path.join(DATA_PATH, \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 4)\n",
    "            # os.path.join(DATA_PATH, \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 5)\n",
    "            # os.path.join(DATA_PATH, \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 6)\n",
    "            # os.path.join(DATA_PATH, \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 7)\n",
    "        ],\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 0.25, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": \"cuda\" in config[\"device\"],\n",
    "        \"batch_size\": 512,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"skip_train\": False,\n",
    "    \"skip_val\": False,\n",
    "    \"skip_test\": False,\n",
    "    \"normalize_neuron_coords\": True,\n",
    "    \"average_test_multitrial\": True,\n",
    "    \"save_test_multitrial\": True,\n",
    "    \"test_batch_size\": 7,\n",
    "    \"device\": config[\"device\"],\n",
    "}\n",
    "\n",
    "### get dataloaders and cell coordinates\n",
    "dataloaders, neuron_coords = get_mouse_v1_data(config[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "sample_data_key = dataloaders[\"mouse_v1\"][\"test\"].data_keys[0]\n",
    "datapoint = next(iter(dataloaders[\"mouse_v1\"][\"test\"].dataloaders[0]))\n",
    "stim, resp = datapoint.images, datapoint.responses\n",
    "pupil_center = datapoint.pupil_center\n",
    "print(\n",
    "    f\"Training dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['train'].dataloaders)} samples\"\n",
    "    f\"\\nValidation dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['val'].dataloaders)} samples\"\n",
    "    f\"\\nTest dataset:\\t\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test'].dataloaders)} samples\"\n",
    "    f\"\\nTest (no resp) dataset:\\t {sum(len(dl) * dl.batch_size for dl in dataloaders['mouse_v1']['test_no_resp'].dataloaders)} samples\"\n",
    "\n",
    "    \"\\n\\nstimuli:\"\n",
    "    f\"\\n  {stim.shape}\"\n",
    "    f\"\\n  min={stim.min().item():.3f}  max={stim.max().item():.3f}\"\n",
    "    f\"\\n  mean={stim.mean().item():.3f}  std={stim.std().item():.3f}\"\n",
    "    \"\\nresponses:\"\n",
    "    f\"\\n  {resp.shape}\"\n",
    "    f\"\\n  min={resp.min().item():.3f}  max={resp.max().item():.3f}\"\n",
    "    f\"\\n  mean={resp.mean().item():.3f}  std={resp.std().item():.3f}\"\n",
    "    \"\\nneuronal coordinates:\"\n",
    "    f\"\\n  {neuron_coords[sample_data_key].shape}\"\n",
    "    f\"\\n  min={neuron_coords[sample_data_key].min():.3f}  max={neuron_coords[sample_data_key].max():.3f}\"\n",
    "    f\"\\n  mean={neuron_coords[sample_data_key].mean():.3f}  std={neuron_coords[sample_data_key].std():.3f}\"\n",
    ")\n",
    "\n",
    "### plot sample data\n",
    "sample_idx = 0\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[sample_idx].squeeze().unsqueeze(-1).cpu(), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop(stim[sample_idx].cpu(), config[\"crop_win\"]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "### bin the neuronal responses based on their neuron coordinates and sum within each bin -> 2D grid of vals\n",
    "coords = neuron_coords[sample_data_key]\n",
    "H, W = stim.shape[-2:] # the size of the grid\n",
    "n_x_bins, n_y_bins = 32, 18 # number of bins in each dimension\n",
    "min_x, max_x, min_y, max_y = coords[:,0].min().item(), coords[:,0].max().item(), coords[:,1].min().item(), coords[:,1].max().item()\n",
    "x_bins = torch.linspace(min_x, max_x, n_x_bins + 1)\n",
    "y_bins = torch.linspace(min_y, max_y, n_y_bins + 1)\n",
    "binned_resp = torch.zeros(n_y_bins, n_x_bins)\n",
    "for i in range(n_y_bins):\n",
    "    for j in range(n_x_bins):\n",
    "        ### mask of the neurons in the bin\n",
    "        mask = (x_bins[j] <= coords[:,0]) &\\\n",
    "               (coords[:,0] < x_bins[j + 1]) &\\\n",
    "               (y_bins[i] <= coords[:,1]) &\\\n",
    "               (coords[:,1] < y_bins[i + 1])\n",
    "        binned_resp[i,j] = resp[sample_idx, mask.cpu()].sum(0)\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(binned_resp.squeeze().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(ckpt_path=os.path.join(DATA_PATH, \"models\", \"encoder_sens22.pth\"), device=config[\"device\"], eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "\n",
    "### load pretrained encoder ckpt\n",
    "encoder_ckpt = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"encoder.pt\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "\n",
    "### get temporary dataloaders for the encoder\n",
    "_dataloaders = get_data(\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_fn\"],\n",
    "    encoder_ckpt[\"config\"][\"data\"][\"dataset_config\"]\n",
    ")\n",
    "\n",
    "### init encoder\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_ckpt[\"config\"][\"encoder\"][\"model_config\"],\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=encoder_ckpt[\"config\"][\"seed\"],\n",
    ").float()\n",
    "encoder.load_state_dict(encoder_ckpt[\"encoder_state\"], strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()\n",
    "del _dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(history):\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(history[\"resp_loss\"])\n",
    "    ax.set_title(\"resp_loss\")\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(history[\"stim_loss\"])\n",
    "    ax.set_title(\"stim_loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set targets\n",
    "target_stim = stim.float().to(config[\"device\"])\n",
    "target_resp = resp.float().to(config[\"device\"])\n",
    "target_pupil_center = pupil_center.float().to(config[\"device\"])\n",
    "data_key = sample_data_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "config[\"enc_inv\"] = {\n",
    "    \"encoder\": encoder,\n",
    "    \"img_dims\": stim.shape[1:],\n",
    "    \"stim_pred_init\": \"zeros\",\n",
    "    \"opter_cls\": torch.optim.SGD,\n",
    "    \"opter_config\": {\"lr\": 1500, \"momentum\": 0.},\n",
    "    \"n_steps\": 400,\n",
    "    \"resp_loss_fn\": F.mse_loss,\n",
    "    \"stim_loss_fn\": SSIMLoss(\n",
    "        window=config[\"crop_win\"],\n",
    "        log_loss=True,\n",
    "        inp_normalized=True,\n",
    "        inp_standardized=False,\n",
    "    ),\n",
    "    \"img_gauss_blur_config\": None,\n",
    "    \"img_grad_gauss_blur_config\": {\"kernel_size\": 17, \"sigma\": 2},\n",
    "    \"device\": config[\"device\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"enc_inv\"][\"img_grad_gauss_blur_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InvertedEncoder(**config[\"enc_inv\"]).to(config[\"device\"])\n",
    "stim_pred, _, history = model(\n",
    "    resp_target=target_resp,\n",
    "    stim_target=target_stim,\n",
    "    additional_encoder_inp={\n",
    "        \"data_key\": data_key,\n",
    "        \"pupil_center\": target_pupil_center,\n",
    "    }\n",
    ")\n",
    "stim_pred_best = history[\"best\"][\"stim_pred\"]\n",
    "\n",
    "### show the results\n",
    "print(history[\"best\"][\"stim_loss\"])\n",
    "plot_loss_history(history)\n",
    "stim_pred_best = history[\"best\"][\"stim_pred\"]\n",
    "plot_comparison(target=crop(target_stim[:8], config[\"crop_win\"]).cpu(), pred=crop(stim_pred[:8], config[\"crop_win\"]).detach().cpu())\n",
    "plot_comparison(target=crop(target_stim[:8], config[\"crop_win\"]).cpu(), pred=crop(stim_pred_best[:8], config[\"crop_win\"]).detach().cpu(), pred_title=\"Reconstructed (best)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparam configs\n",
    "config_updates = [\n",
    "    {\"opter_config\": {\"lr\": 1000}, \"img_grad_gauss_blur_config\": dict(kernel_size=11, sigma=2.)},\n",
    "    {\"opter_config\": {\"lr\": 1000}, \"img_grad_gauss_blur_config\": dict(kernel_size=13, sigma=2.)},\n",
    "    {\"opter_config\": {\"lr\": 1000}, \"img_grad_gauss_blur_config\": dict(kernel_size=15, sigma=2.)},\n",
    "\n",
    "    {\"opter_config\": {\"lr\": 500}, \"img_grad_gauss_blur_config\": dict(kernel_size=13, sigma=2.5)},\n",
    "    {\"opter_config\": {\"lr\": 500}, \"img_grad_gauss_blur_config\": dict(kernel_size=15, sigma=2.5)},\n",
    "    {\"opter_config\": {\"lr\": 500}, \"img_grad_gauss_blur_config\": dict(kernel_size=17, sigma=2.5)},\n",
    "    {\"opter_config\": {\"lr\": 500}, \"img_grad_gauss_blur_config\": dict(kernel_size=21, sigma=2.5)},\n",
    "\n",
    "    {\"opter_config\": {\"lr\": 1000, \"momentum\": 0}, \"img_grad_gauss_blur_config\": dict(kernel_size=15, sigma=2.5)},\n",
    "    {\"opter_config\": {\"lr\": 1000, \"momentum\": 0}, \"img_grad_gauss_blur_config\": dict(kernel_size=17, sigma=2.5)},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run\n",
    "results = []\n",
    "for i, config_update in enumerate(config_updates):\n",
    "    print(f\"[{i+1}/{len(config_updates)}]\")\n",
    "    run_config = deepcopy(config)\n",
    "    run_config[\"enc_inv\"].update(config_update)\n",
    "    model = InvertedEncoder(**run_config[\"enc_inv\"]).to(config[\"device\"])\n",
    "    stim_pred, _, history = model(\n",
    "        resp_target=target_resp,\n",
    "        stim_target=target_stim,\n",
    "        additional_encoder_inp={\n",
    "            \"data_key\": data_key,\n",
    "            \"pupil_center\": target_pupil_center,\n",
    "        }\n",
    "    )\n",
    "    stim_pred_best = history[\"best\"][\"stim_pred\"]\n",
    "    results.append({\"run_config\": run_config, \"stim_pred\": stim_pred, \"history\": history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    print(\n",
    "        f\"lr={r['run_config']['enc_inv']['opter_config']['lr']}\"\n",
    "        f\", sigma={r['run_config']['enc_inv']['img_grad_gauss_blur_config']['sigma']}\"\n",
    "        f\", ker_sz={r['run_config']['enc_inv']['img_grad_gauss_blur_config']['kernel_size']}\"\n",
    "        f\"   {r['history']['best']['stim_loss']:.5f}\"\n",
    "    )\n",
    "\n",
    "    ### plot loss history\n",
    "    plot_loss_history(history=r[\"history\"])\n",
    "\n",
    "    ### plot best results\n",
    "    plot_comparison(target=crop(target_stim[:8], config[\"crop_win\"]).cpu(),\n",
    "                    pred=crop(r['history']['best']['stim_pred'][:8], config[\"crop_win\"]).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comparison_utils import get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder inversion config\n",
    "config[\"enc_inv\"] = {\n",
    "    \"model\": {\n",
    "        \"encoder\": get_encoder(\n",
    "            ckpt_path=os.path.join(DATA_PATH, \"models\", \"encoder_sens22.pth\"),\n",
    "            device=config[\"device\"],\n",
    "            eval_mode=True,\n",
    "            # ckpt_path=os.path.join(DATA_PATH, \"models\", \"encoder_sens22_no_shifter.pth\"),\n",
    "        ),\n",
    "        \"img_dims\": (1, 36, 64),\n",
    "        \"stim_pred_init\": \"zeros\",\n",
    "        \"opter_cls\": torch.optim.SGD,\n",
    "        \"opter_config\": {\"lr\": 150, \"momentum\": 0.},\n",
    "        \"n_steps\": 400,\n",
    "        \"resp_loss_fn\": lambda resp_pred, resp_target: F.mse_loss(resp_pred, resp_target, reduction=\"none\").mean(-1).sum(),\n",
    "        \"stim_loss_fn\": None, # set below\n",
    "        \"img_gauss_blur_config\": None,\n",
    "        \"img_grad_gauss_blur_config\": {\"kernel_size\": 13, \"sigma\": 2.},\n",
    "        \"device\": config[\"device\"],\n",
    "    },\n",
    "    \"loss_fns\": get_metrics(config=config),\n",
    "    \"save_dir\": os.path.join(DATA_PATH, \"models\", \"inverted_encoder\"),\n",
    "    # \"find_best_ckpt_according_to\": \"Perceptual Loss (VGG16)\",\n",
    "    \"find_best_ckpt_according_to\": \"SSIML + PSL\",\n",
    "}\n",
    "config[\"enc_inv\"][\"model\"][\"stim_loss_fn\"] = config[\"enc_inv\"][\"loss_fns\"][config[\"enc_inv\"][\"find_best_ckpt_according_to\"]]\n",
    "model = InvertedEncoder(**config[\"enc_inv\"][\"model\"]).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"enc_inv\"][\"model\"][\"n_steps\"] = 500\n",
    "config[\"enc_inv\"][\"model\"][\"img_grad_gauss_blur_config\"] = {\"kernel_size\": 13, \"sigma\": 2.}\n",
    "config[\"enc_inv\"][\"model\"][\"opter_config\"] = {\"lr\": 500, \"momentum\": 0.}\n",
    "model = InvertedEncoder(**config[\"enc_inv\"][\"model\"]).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "stim_pred, _, hist = model(\n",
    "    resp_target=resp[:128],\n",
    "    stim_target=stim[:128],\n",
    "    additional_encoder_inp={\n",
    "        \"data_key\": sample_data_key,\n",
    "        \"pupil_center\": pupil_center[:128],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csng.losses import SSIM\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = SSIM(size_average=False)\n",
    "ssim2 = SSIM2(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(standardize(stim_pred), standardize(stim)[:128]), ssim2(standardize(stim_pred), standardize(stim)[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(stim_pred, stim[:128]), ssim2(stim_pred, stim[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_comparison(target=crop(stim[:8], config[\"crop_win\"]).cpu(), pred=crop(stim_pred[:8], config[\"crop_win\"]).cpu())\n",
    "plt.plot(hist[\"stim_loss\"], label=\"stim_loss\")\n",
    "plt.plot(hist[\"resp_loss\"], label=\"resp_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code from EGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"crop_win\"] = (22, 36)\n",
    "config[\"data\"] = dict(mixing_strategy=\"sequential\")\n",
    "\n",
    "### mouse v1 data\n",
    "config[\"data\"][\"mouse_v1\"] = {\n",
    "    \"dataset_fn\": \"sensorium.datasets.static_loaders\",\n",
    "    \"dataset_config\": {\n",
    "        \"paths\": [ # from https://gin.g-node.org/cajal/Sensorium2022/src/master\n",
    "            # os.path.join(DATA_PATH, \"static26872-17-20-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # mouse 1\n",
    "            # os.path.join(DATA_PATH, \"static27204-5-13-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # sensorium+ (mouse 2)\n",
    "            os.path.join(DATA_PATH, \"static21067-10-18-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 3)\n",
    "            # os.path.join(DATA_PATH, \"static22846-10-16-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 4)\n",
    "            # os.path.join(DATA_PATH, \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 5)\n",
    "            # os.path.join(DATA_PATH, \"static23656-14-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 6)\n",
    "            # os.path.join(DATA_PATH, \"static23964-4-22-GrayImageNet-94c6ff995dac583098847cfecd43e7b6.zip\"), # pretraining (mouse 7)\n",
    "        ],\n",
    "        \"normalize\": True,\n",
    "        \"scale\": 0.25, # 256x144 -> 64x36\n",
    "        \"include_behavior\": False,\n",
    "        \"add_behavior_as_channels\": False,\n",
    "        \"include_eye_position\": True,\n",
    "        \"exclude\": None,\n",
    "        \"file_tree\": True,\n",
    "        \"cuda\": \"cuda\" in config[\"device\"],\n",
    "        \"batch_size\": 8,\n",
    "        \"seed\": config[\"seed\"],\n",
    "        \"use_cache\": False,\n",
    "    },\n",
    "    \"skip_train\": False,\n",
    "    \"skip_val\": False,\n",
    "    \"skip_test\": False,\n",
    "    \"normalize_neuron_coords\": True,\n",
    "    \"average_test_multitrial\": True,\n",
    "    \"save_test_multitrial\": True,\n",
    "    \"test_batch_size\": 7,\n",
    "    \"device\": config[\"device\"],\n",
    "}\n",
    "\n",
    "### sample data\n",
    "dls, neuron_coords = get_all_data(config=config)\n",
    "sample_data_key = dls[\"mouse_v1\"][\"val\"].data_keys[0]\n",
    "datapoint = next(iter(dls[\"mouse_v1\"][\"val\"].dataloaders[0]))\n",
    "stim, resp, pupil_center = datapoint.images, datapoint.responses, datapoint.pupil_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EGG imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import gc\n",
    "import sys\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as torch_models\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import time\n",
    "from egg.diffusion import EGG\n",
    "from egg.models import models\n",
    "\n",
    "class GaussianBlur:\n",
    "    \"\"\"Blur an image with a Gaussian window.\n",
    "    Arguments:\n",
    "        sigma (float or tuple): Standard deviation in y, x used for the gaussian blurring.\n",
    "        decay_factor (float): Compute sigma every iteration as `sigma + decay_factor *\n",
    "            (iteration - 1)`. Ignored if None.\n",
    "        truncate (float): Gaussian window is truncated after this number of standard\n",
    "            deviations to each side. Size of kernel = 8 * sigma + 1\n",
    "        pad_mode (string): Mode for the padding used for the blurring. Valid values are:\n",
    "            'constant', 'reflect' and 'replicate'\n",
    "        mei_only (True/False): for transparent mei, if True, no Gaussian blur for transparent channel:\n",
    "            default should be False (also for non transparent case)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, sigma, decay_factor=None, truncate=4, pad_mode=\"reflect\", mei_only=False\n",
    "    ):\n",
    "        self.sigma = sigma if isinstance(sigma, tuple) else (sigma,) * 2\n",
    "        self.decay_factor = decay_factor\n",
    "        self.truncate = truncate\n",
    "        self.pad_mode = pad_mode\n",
    "        self.mei_only = mei_only\n",
    "\n",
    "    def __call__(self, x, iteration=None):\n",
    "\n",
    "        # Update sigma if needed\n",
    "        if self.decay_factor is None:\n",
    "            sigma = self.sigma\n",
    "        else:\n",
    "            sigma = tuple(s + self.decay_factor * (iteration - 1) for s in self.sigma)\n",
    "\n",
    "        # Define 1-d kernels to use for blurring\n",
    "        y_halfsize = max(int(round(sigma[0] * self.truncate)), 1)\n",
    "        y_gaussian = signal.gaussian(2 * y_halfsize + 1, std=sigma[0])\n",
    "        x_halfsize = max(int(round(sigma[1] * self.truncate)), 1)\n",
    "        x_gaussian = signal.gaussian(2 * x_halfsize + 1, std=sigma[1])\n",
    "        y_gaussian = torch.as_tensor(y_gaussian, device=x.device, dtype=x.dtype)\n",
    "        x_gaussian = torch.as_tensor(x_gaussian, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # Blur\n",
    "        if self.mei_only:\n",
    "            num_channels = x.shape[1] - 1\n",
    "            padded_x = F.pad(\n",
    "                x[:, :-1, ...],\n",
    "                pad=(x_halfsize, x_halfsize, y_halfsize, y_halfsize),\n",
    "                mode=self.pad_mode,\n",
    "            )\n",
    "        else:  # also blur transparent channel\n",
    "            num_channels = x.shape[1]\n",
    "            padded_x = F.pad(\n",
    "                x,\n",
    "                pad=(x_halfsize, x_halfsize, y_halfsize, y_halfsize),\n",
    "                mode=self.pad_mode,\n",
    "            )\n",
    "        blurred_x = F.conv2d(\n",
    "            padded_x,\n",
    "            y_gaussian.repeat(num_channels, 1, 1)[..., None],\n",
    "            groups=num_channels,\n",
    "        )\n",
    "        blurred_x = F.conv2d(\n",
    "            blurred_x, x_gaussian.repeat(num_channels, 1, 1, 1), groups=num_channels\n",
    "        )\n",
    "        final_x = blurred_x / (y_gaussian.sum() * x_gaussian.sum())  # normalize\n",
    "        # print(final_x.shape)\n",
    "        if self.mei_only:\n",
    "            return torch.cat(\n",
    "                (final_x, x[:, -1, ...].view(x.shape[0], 1, x.shape[2], x.shape[3])),\n",
    "                dim=1,\n",
    "            )\n",
    "        else:\n",
    "            return final_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config[\"device\"]\n",
    "images, responses = stim, resp\n",
    "image_idxs = np.arange(images.shape[0])\n",
    "target_l2 = np.zeros(images.shape[0])\n",
    "model_type = \"task_driven\"  # 'task_driven' or 'v4_multihead_attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_fn(\n",
    "    x,\n",
    "    target_response=None,\n",
    "    val_response=None,\n",
    "    cross_val_response=None,\n",
    "    norm=100,\n",
    "    models=None,\n",
    "):\n",
    "    tar = x\n",
    "    tar = tar / torch.norm(tar) * norm  # 60\n",
    "    tar = tar.clip(-1.7, 1.9)\n",
    "\n",
    "    train_pred = models[\"train\"](tar, data_key=\"all_sessions\", multiplex=False)[0]\n",
    "    val_pred = models[\"val\"](tar, data_key=\"all_sessions\", multiplex=False)[0]\n",
    "    cross_val_pred = models[\"cross-val\"](\n",
    "        tar, data_key=\"all_sessions\", multiplex=False\n",
    "    )[0]\n",
    "\n",
    "    train_energy = torch.mean((train_pred - target_response) ** 2)\n",
    "    val_energy = torch.mean((val_pred - val_response) ** 2)\n",
    "    cross_val_energy = torch.mean((cross_val_pred - cross_val_response) ** 2)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_energy,\n",
    "        \"val\": val_energy,\n",
    "        \"cross-val\": cross_val_energy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(\n",
    "    ckpt_path=os.path.join(DATA_PATH, \"models\", \"encoder_sens22_mall_mean_activity.pth\"),\n",
    "    device=config[\"device\"],\n",
    "    eval_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 3\n",
    "gaussian_blur = GaussianBlur(sigma=1.)\n",
    "target_response = resp[img_idx].to(device)\n",
    "# target_response = encoder(stim[img_idx].unsqueeze(0), data_key=sample_data_key, pupil_center=pupil_center[img_idx].unsqueeze(0))[0].to(device)\n",
    "target_image = stim[img_idx].to(device)\n",
    "target_pupil_center = pupil_center[img_idx].unsqueeze(0).to(device)\n",
    "\n",
    "# optimize image to minimize energy using Adam\n",
    "class ImageGen(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image = torch.nn.Parameter(torch.randn_like(target_image).unsqueeze(0).to(device))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.image\n",
    "\n",
    "image_gen = ImageGen().to(device)\n",
    "optimizer = torch.optim.AdamW(image_gen.parameters(), lr=0.05)\n",
    "\n",
    "fig = plt.figure(figsize=(3,2))\n",
    "plt.imshow(crop(target_image, config[\"crop_win\"])[0].cpu().detach().numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "pbar = tqdm(range(5000))\n",
    "for i in pbar:\n",
    "    stim_pred = image_gen()\n",
    "    stim_pred = gaussian_blur(stim_pred)\n",
    "    stim_pred = stim_pred / torch.norm(stim_pred) * 30 #target_image.norm()  # 60\n",
    "\n",
    "    res = encoder(stim_pred, data_key=sample_data_key, pupil_center=target_pupil_center)\n",
    "\n",
    "    loss = torch.mean((res - target_response) ** 2)\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"loss: {loss.item()}\"\n",
    "    )\n",
    "\n",
    "    if i % 200 == 0:\n",
    "        fig = plt.figure(figsize=(3,2))\n",
    "        plt.imshow(crop(stim_pred, config[\"crop_win\"])[0,0].cpu().detach().numpy(), \"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(stim[0,0].cpu().detach().numpy(), \"gray\")\n",
    "plt.show()\n",
    "plt.imshow(image[0,0].cpu().detach().numpy(), \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder, ckpt = load_decoder_from_ckpt(\n",
    "    ckpt_path=os.path.join(DATA_PATH, \"models\", \"cnn\", \"2024-05-24_17-11-11\", \"decoder.pt\"),\n",
    "    load_best=False,\n",
    "    device=config[\"device\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = get_encoder(\n",
    "    ckpt_path=os.path.join(DATA_PATH, \"models\", \"encoder_sens22_mall_mean_activity.pth\"),\n",
    "    device=config[\"device\"],\n",
    "    eval_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### experiment settings\n",
    "num_timesteps = 1000\n",
    "energy_scale = 2  # 20\n",
    "seed = 0\n",
    "progressive = True\n",
    "device = config[\"device\"]\n",
    "\n",
    "img_idx = 5\n",
    "target_image, target_response = stim[img_idx].unsqueeze(0), resp[img_idx].unsqueeze(0)\n",
    "encoder_pred = partial(encoder, data_key=sample_data_key, pupil_center=pupil_center[img_idx].unsqueeze(0))\n",
    "x_zero_decoder_pred = crop(decoder(target_response, data_key=sample_data_key, pupil_center=pupil_center[img_idx].unsqueeze(0), neuron_coords=neuron_coords[sample_data_key]), config[\"crop_win\"]).detach()\n",
    "dm_loss_fn = get_metrics(crop_win=config[\"crop_win\"], device=config[\"device\"])[\"MSE-no-standardization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_response = encoder_pred(target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EGG(\n",
    "    num_steps=num_timesteps,\n",
    "    diffusion_artefact=\"/home/sobotj11/energy-guided-diffusion/models/256x256_diffusion_uncond.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_run(model, energy_fn, energy_scale, desc=\"progress\", grayscale=False):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    cur_t = num_timesteps - 1\n",
    "    imgs = []\n",
    "\n",
    "    samples = model.sample(energy_fn=energy_fn, energy_scale=energy_scale)\n",
    "\n",
    "    for j, sample in enumerate(samples):\n",
    "        cur_t -= 1\n",
    "        if (j % 10 == 0 and progressive) or cur_t == -1:\n",
    "            energy = energy_fn(sample[\"pred_xstart\"])\n",
    "            for k, image in enumerate(sample[\"pred_xstart\"]):\n",
    "                image = image.detach().cpu()\n",
    "                filename = f\"{desc}_{0:05}.png\"\n",
    "                if grayscale:\n",
    "                    image = image.mean(0, keepdim=True)\n",
    "                image = image.add(1).div(2)\n",
    "\n",
    "                image = image.clamp(0, 1)\n",
    "                TF.to_pil_image(image).save(filename)\n",
    "\n",
    "                # tqdm.write(\n",
    "                #     f'step {j} | train energy: {energy[\"train\"]:.4g}'\n",
    "                # )\n",
    "        imgs.append(image.detach().cpu().numpy())\n",
    "\n",
    "    return energy, image, imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_fn(\n",
    "    x,\n",
    "    encoder_model,\n",
    "    target_response=None,\n",
    "    val_response=None,\n",
    "    cross_val_response=None,\n",
    "    norm=100,\n",
    "    em_weight=1,\n",
    "    dm_weight=1,\n",
    "    dm_loss_fn=F.mse_loss,\n",
    "):\n",
    "    energy = 0\n",
    "\n",
    "    ### encoder matching\n",
    "    tar = F.interpolate(\n",
    "        x.clone(), size=(36, 64), mode=\"bilinear\", align_corners=False\n",
    "    ).mean(1, keepdim=True)\n",
    "    tar = tar / torch.norm(tar) * norm\n",
    "    resp_pred = encoder_model(tar)[0]\n",
    "    energy += em_weight * torch.mean((resp_pred - target_response) ** 2)\n",
    "\n",
    "    ### decoder matching\n",
    "    tar = F.interpolate(\n",
    "        x.clone(), size=config[\"crop_win\"], mode=\"bilinear\", align_corners=False\n",
    "    ).mean(1, keepdim=True)\n",
    "    tar = tar / torch.norm(tar) * norm\n",
    "    energy += dm_weight * dm_loss_fn(tar, x_zero_decoder_pred)\n",
    "\n",
    "    return {\"train\": energy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diffusion(target_image, imgs, energy_scale):\n",
    "    ### plot progression in one plot\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "    ### gt\n",
    "    ax = fig.add_subplot(2, 5, 1)\n",
    "    ax.imshow(crop(target_image, config[\"crop_win\"])[0].cpu().permute(1, 2, 0), \"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Target\", fontweight=\"bold\")\n",
    "\n",
    "    for t_idx, t in enumerate([0, 10, 100, 200, 300, 400, 600, 800, 999]):\n",
    "        ax = fig.add_subplot(2, 5, t_idx + 2)\n",
    "        rec = F.interpolate(\n",
    "            torch.from_numpy(imgs[t]).unsqueeze(0), size=(36, 64), mode=\"bilinear\", align_corners=False\n",
    "        )[0]\n",
    "        ax.imshow(rec.detach().permute(1,2,0).numpy(), \"gray\")\n",
    "        ax.set_title(f\"t={t}\" + r\" ($\\lambda=$\" + str(energy_scale) + \")\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(crop(target_image, config[\"crop_win\"])[0].cpu().detach().permute(1,2,0).numpy(), \"gray\")\n",
    "ax.axis(\"off\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(crop(x_zero_decoder_pred, config[\"crop_win\"])[0].cpu().detach().permute(1,2,0).numpy(), \"gray\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "for es in [5]:\n",
    "    for em_w in [1]:\n",
    "        for dm_w in [1]:\n",
    "            for dm_loss_fn_name in [\"MSE-no-standardization\"]:\n",
    "                _dm_loss_fn = get_metrics(crop_win=config[\"crop_win\"], device=config[\"device\"])[dm_loss_fn_name]\n",
    "                print(f\"energy-scale={es}  DM-loss-fn={dm_loss_fn_name}  DM-weight={dm_w}  EM-weight={em_w}\")\n",
    "                score, image, imgs = do_run(\n",
    "                    model=model,\n",
    "                    energy_fn=partial(\n",
    "                        energy_fn,\n",
    "                        encoder_model=encoder_pred,\n",
    "                        target_response=target_response,\n",
    "                        norm=target_image.norm(),\n",
    "                        em_weight=em_w,\n",
    "                        dm_weight=dm_w,\n",
    "                        dm_loss_fn=_dm_loss_fn,\n",
    "                    ),\n",
    "                    energy_scale=es,\n",
    "                    grayscale=True,\n",
    "                )\n",
    "                plot_diffusion(target_image, imgs, energy_scale=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0, 5, 10, 100, 200, 300, 400, 500, 600, 800, 999]:\n",
    "    fig = plt.figure(figsize=(5,2.5))\n",
    "    \n",
    "    ### gt\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.imshow(crop(target_image, config[\"crop_win\"])[0].cpu().permute(1, 2, 0), \"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    # ax = fig.add_subplot(1, 4, 2)\n",
    "    # ax.imshow(np.moveaxis(imgs[t], 0, 2), \"gray\")\n",
    "    # ax.axis(\"off\")\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    tar = F.interpolate(\n",
    "        torch.from_numpy(imgs[t]).unsqueeze(0), size=(36, 64), mode=\"bilinear\", align_corners=False\n",
    "    )[0]\n",
    "    ax.imshow(tar.detach().permute(1,2,0).numpy(), \"gray\")\n",
    "    ax.set_title(f\"t={t}\" + r\" ($\\lambda=$\" + str(energy_scale) + \")\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # ax = fig.add_subplot(1, 3, 3)\n",
    "    # ax.imshow(np.moveaxis(crop(imgs[t], config[\"crop_win\"]), 0, 2), \"gray\")\n",
    "    # ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,2.5))\n",
    "plt.imshow(crop(target_image, config[\"crop_win\"])[0].cpu().detach().permute(1,2,0).numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(3,2.5))\n",
    "plt.imshow(target_image[0].cpu().detach().permute(1,2,0).numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(image.detach().permute(1,2,0).numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "tar = F.interpolate(\n",
    "    image.unsqueeze(0), size=(36, 64), mode=\"bilinear\", align_corners=False\n",
    ")[0]\n",
    "fig = plt.figure(figsize=(3,2.5))\n",
    "plt.imshow(tar.detach().permute(1,2,0).numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(3,2.5))\n",
    "plt.imshow(crop(image, config[\"crop_win\"]).detach().permute(1,2,0).numpy(), \"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
