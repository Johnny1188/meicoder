{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "from csng.utils import get_corr, plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss\n",
    "\n",
    "from data_orig import prepare_spiking_data_loaders\n",
    "# from data import prepare_data_loaders\n",
    "from data import prepare_data_loaders, SyntheticDataset, BatchPatchesDataLoader\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "        \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "        \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "        \"image_size\": [50, 50],\n",
    "        \"crop\": False,\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = prepare_spiking_data_loaders(**config[\"data\"])\n",
    "oracle_dataloader = data_loaders[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "data_sample = next(iter(data_loaders[\"train\"][\"spiking\"]))\n",
    "stim, resp = data_sample[0].float(), data_sample[1]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(stim[0].cpu().squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(resp[0].cpu().view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_win = (slice(15, 35), slice(15, 35))\n",
    "crop_stim = lambda x: x[..., crop_win[0], crop_win[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_orig import prepare_spiking_data_loaders\n",
    "# from lurz2020.models.models import se2d_fullgaussian2d\n",
    "\n",
    "# spiking_data_loaders_config = {\n",
    "#     \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "#     \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "#     \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "#     \"image_size\": [50, 50],\n",
    "#     \"crop\": False,\n",
    "#     \"batch_size\": 32,\n",
    "# }\n",
    "# encoder_config = {\n",
    "#     \"init_mu_range\": 0.55,\n",
    "#     \"init_sigma\": 0.4,\n",
    "#     \"input_kern\": 19,\n",
    "#     \"hidden_kern\": 17,\n",
    "#     \"hidden_channels\": 32,\n",
    "#     \"gamma_input\": 1.0,\n",
    "#     \"gamma_readout\": 2.439,\n",
    "#     \"grid_mean_predictor\": None,\n",
    "#     \"layers\": 5\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### encoder\n",
    "# encoder = se2d_fullgaussian2d(\n",
    "#     **encoder_config,\n",
    "#     dataloaders=data_loaders,\n",
    "#     seed=2,\n",
    "# )\n",
    "\n",
    "# ### load pretrained core\n",
    "# pretrained_core = torch.load(\n",
    "#     os.path.join(DATA_PATH, \"models\", \"spiking_scratch_tunecore_68Y_model.pth\"),\n",
    "#     map_location=torch.device(\"cuda\")\n",
    "# )\n",
    "# encoder.load_state_dict(pretrained_core, strict=True)\n",
    "# encoder.to(config[\"device\"])\n",
    "# _ = encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_data_imgs_path = os.path.join(os.environ[\"DATA_PATH\"], \"sensorium22\", \"static23343-5-17-GrayImageNet-94c6ff995dac583098847cfecd43e7b6\", \"data\", \"images\")\n",
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_mean_from_syn_dataset.npy\"))).float().to(config[\"device\"])\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"responses_std_from_syn_dataset.npy\"))).float().to(config[\"device\"])\n",
    "syn_dataset = SyntheticDataset(\n",
    "    data_dir=syn_data_imgs_path,\n",
    "    patch_size=config[\"data\"][\"image_size\"][0],\n",
    "    overlap=15,\n",
    "    encoder=encoder,\n",
    "    expand_stim_for_encoder=False,\n",
    "    # stim_transform=transforms.Normalize(\n",
    "    #     mean=114.457,\n",
    "    #     std=51.356,\n",
    "    # ),\n",
    "    # resp_transform=transforms.Lambda(\n",
    "    #     lambda x: (x - resp_mean) / resp_std\n",
    "    # ),\n",
    "    stim_transform=None,\n",
    "    resp_transform=None,\n",
    "    device=config[\"device\"],\n",
    ")\n",
    "_dataloader = DataLoader(syn_dataset, batch_size=2, shuffle=True)\n",
    "syn_dataloader = BatchPatchesDataLoader(_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp = next(iter(syn_dataloader))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(syn_stim.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(syn_resp.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"encoder_inversion\"] = {\n",
    "    \"model\": {\n",
    "        \"init_mu_range\": 0.55,\n",
    "        \"init_sigma\": 0.4,\n",
    "        \"input_kern\": 19,\n",
    "        \"hidden_kern\": 17,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"gamma_input\": 1.0,\n",
    "        \"gamma_readout\": 2.439,\n",
    "        \"grid_mean_predictor\": None,\n",
    "        \"layers\": 5\n",
    "    },\n",
    "    \"n_inits\": 15,\n",
    "    \"n_steps\": 8000,\n",
    "    \"opter_cls\": torch.optim.Adam,\n",
    "    \"opter_kwargs\": {\n",
    "        \"lr\": 0.3,\n",
    "    },\n",
    "    \"loss_fn\": nn.MSELoss(),\n",
    "    # \"loss_fn\": nn.L1Loss(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoder\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **config[\"encoder_inversion\"][\"model\"],\n",
    "    dataloaders=data_loaders,\n",
    "    seed=2,\n",
    ")  # Use data loaders with 50x50 images\n",
    "\n",
    "### load pretrained core\n",
    "pretrained_core = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"spiking_scratch_tunecore_68Y_model.pth\"),\n",
    "    map_location=torch.device(\"cuda\")\n",
    ")\n",
    "encoder.load_state_dict(pretrained_core, strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "_ = encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_stim = syn_stim.float().to(config[\"device\"])\n",
    "target_resp = syn_resp.float().to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    target=target_stim[:8].cpu(),\n",
    "    pred=stim_pred[:8,:, 10:40, 10:40].detach().cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_preds = []\n",
    "\n",
    "for init_i in range(config[\"encoder_inversion\"][\"n_inits\"]):\n",
    "    print(f\"Init {init_i}\")\n",
    "\n",
    "    ### init decoded img \n",
    "    stim_pred = torch.rand_like(target_stim, device=config[\"device\"]) * 100.\n",
    "    # stim_pred = torch.zeros_like(stim, device=config[\"device\"])\n",
    "    stim_pred = stim_pred.requires_grad_(True)\n",
    "    opter = config[\"encoder_inversion\"][\"opter_cls\"](\n",
    "        [stim_pred], **config[\"encoder_inversion\"][\"opter_kwargs\"],\n",
    "    )\n",
    "\n",
    "    loss_history = []\n",
    "    for step_i in range(config[\"encoder_inversion\"][\"n_steps\"]):\n",
    "        opter.zero_grad()\n",
    "        resp_pred = encoder(stim_pred).float()\n",
    "        loss = config[\"encoder_inversion\"][\"loss_fn\"](resp_pred, target_resp)\n",
    "        loss.backward()\n",
    "        opter.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if step_i % 500 == 0:\n",
    "            print(f\"Step {step_i}: {loss.item():.3f}\")\n",
    "            ### plot reconstruction samples\n",
    "            plot_comparison(\n",
    "                target=crop_stim(target_stim[:8].cpu()),\n",
    "                pred=crop_stim(stim_pred[:8].detach().cpu()),\n",
    "            )\n",
    "            plot_comparison(\n",
    "                target=target_stim[:8].cpu(),\n",
    "                pred=stim_pred[:8].detach().cpu(),\n",
    "            )\n",
    "\n",
    "    ### plot loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.show()\n",
    "    \n",
    "    stim_preds.append(stim_pred.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_pred = torch.stack(stim_preds).mean(0)\n",
    "plot_comparison(\n",
    "    target=crop_stim(target_stim[:8].cpu()),\n",
    "    pred=crop_stim(stim_pred[:8].detach().cpu()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot reconstruction samples\n",
    "plot_comparison(\n",
    "    target=crop_stim(stim)[:8].cpu(),\n",
    "    pred=crop_stim(stim_pred)[:8].detach().cpu(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
