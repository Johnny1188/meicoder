{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lovely_tensors as lt\n",
    "\n",
    "import csng\n",
    "from csng.CNN_Decoder import CNN_Decoder\n",
    "from csng.utils import plot_comparison, standardize, normalize, get_mean_and_std, count_parameters\n",
    "from csng.losses import SSIMLoss, SSIMLossWithCrop, MSELossWithCrop\n",
    "\n",
    "from data import prepare_v1_dataloaders, SyntheticDataset, BatchPatchesDataLoader, MixedBatchLoader, PerSampleStoredDataset\n",
    "from L2O_Decoder import L2O_Decoder\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "DATA_PATH = os.path.join(os.environ[\"DATA_PATH\"], \"cat_V1_spiking_model\")\n",
    "print(f\"{DATA_PATH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"mixing_strategy\": \"parallel_min\", # needed only with multiple base dataloaders\n",
    "    },\n",
    "    \"stim_crop_win\": (slice(15, 35), slice(15, 35)),\n",
    "    \"only_v1_data_eval\": True,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "print(f\"... Running on {config['device']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"stim_crop_win\"] is not None:\n",
    "    crop_stim = lambda x: x[..., config[\"stim_crop_win\"][0], config[\"stim_crop_win\"][1]]\n",
    "else:\n",
    "    crop_stim = lambda x: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1 dataset (spiking model of cat V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"v1_data\"] = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    # \"crop\": True,\n",
    "    # \"batch_size\": 48,\n",
    "    \"batch_size\": 30,\n",
    "    \n",
    "    ### stim normalization\n",
    "    # \"stim_normalize_mean\": 46.236,\n",
    "    # \"stim_normalize_std\": 21.196,\n",
    "    ### stim standardization (important for setting the right SSIM loss args)\n",
    "    \"stim_normalize_mean\": 0,\n",
    "    \"stim_normalize_std\": 100,\n",
    "\n",
    "    \"resp_normalize_mean\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_mean_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "    \"resp_normalize_std\": torch.from_numpy(np.load(\n",
    "        os.path.join(DATA_PATH, \"responses_std_from_training_dataset.npy\")\n",
    "    )).float(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data statistics\n",
    "# data_loaders = prepare_data_loaders(**config[\"data\"])\n",
    "# data_stats = get_mean_and_std(dataset=data_loaders[\"train\"].dataset, verbose=True)\n",
    "# for k in data_stats:\n",
    "#     for ks in data_stats[k]:\n",
    "#         print(f\"{k}.{ks}: {data_stats[k][ks]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get data loaders\n",
    "dataloaders[\"v1_data\"] = prepare_v1_dataloaders(**config[\"data\"][\"v1_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "stim, resp = next(iter(dataloaders[\"v1_data\"][\"val\"]))\n",
    "print(\n",
    "    f\"{stim.shape=}, {resp.shape=}\"\n",
    "    f\"\\n{stim.min()=}, {stim.max()=}\"\n",
    "    f\"\\n{resp.min()=}, {resp.max()=}\"\n",
    "    f\"\\n{stim.mean()=}, {stim.std()=}\"\n",
    "    f\"\\n{resp.mean()=}, {resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(stim[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(stim[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(resp[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data (different stimuli dataset -> encoder -> neuronal responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_mean = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data\", \"responses_mean.npy\"))).float()\n",
    "resp_std = torch.from_numpy(np.load(os.path.join(DATA_PATH, \"synthetic_data\", \"responses_std.npy\"))).float()\n",
    "\n",
    "config[\"data\"][\"syn_data\"] = {\n",
    "    \"dataset\": {\n",
    "        ### stim normalization\n",
    "        # \"stim_transform\": transforms.Normalize(\n",
    "        #     mean=114.457,\n",
    "        #     std=51.356,\n",
    "        # ),\n",
    "        ### stim standardization (important to choose for SSIM loss)\n",
    "        \"stim_transform\": transforms.Normalize(\n",
    "            mean=0,\n",
    "            std=255,\n",
    "        ),\n",
    "\n",
    "        \"resp_transform\": csng.utils.Normalize(\n",
    "            mean=resp_mean,\n",
    "            std=resp_std,\n",
    "        ),\n",
    "    },\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 10,\n",
    "        \"shuffle\": True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets = {\n",
    "    \"train\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"train\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"val\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"val\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "    \"test\": PerSampleStoredDataset(\n",
    "        dataset_dir=os.path.join(DATA_PATH, \"synthetic_data\", \"processed\", \"test\"),\n",
    "        **config[\"data\"][\"syn_data\"][\"dataset\"]\n",
    "    ),\n",
    "}\n",
    "\n",
    "dataloaders[\"syn_data\"] = {\n",
    "    \"train\": DataLoader(\n",
    "        dataset=syn_datasets[\"train\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        dataset=syn_datasets[\"val\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        dataset=syn_datasets[\"test\"],\n",
    "        **config[\"data\"][\"syn_data\"][\"dataloader\"],\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate statistics\n",
    "\n",
    "### for stimuli\n",
    "# syn_stats = get_mean_and_std(dataset=syn_dataset, verbose=True)\n",
    "# syn_stats\n",
    "\n",
    "### for responses\n",
    "# from csng.utils import RunningStats\n",
    "\n",
    "# stats = RunningStats(num_components=10000, lib=\"torch\", device=\"cuda\")\n",
    "# for i, (s, r) in enumerate(syn_dataloader):\n",
    "#     stats.update(r)\n",
    "#     if i % 200 == 0:\n",
    "#         print(f\"{i}: {r.mean()=} {r.std()=} {stats.get_mean()=} {stats.get_std()=}\")\n",
    "\n",
    "### save\n",
    "# torch.save(stats.get_mean(), os.path.join(DATA_PATH, \"responses_mean_from_syn_dataset.pt\"))\n",
    "# torch.save(stats.get_std(), os.path.join(DATA_PATH, \"responses_std_from_syn_dataset.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show data\n",
    "syn_stim, syn_resp = next(iter(dataloaders[\"syn_data\"][\"val\"]))\n",
    "print(\n",
    "    f\"{syn_stim.shape=}, {syn_resp.shape=}\"\n",
    "    f\"\\n{syn_stim.min()=}, {syn_stim.max()=}\"\n",
    "    f\"\\n{syn_resp.min()=}, {syn_resp.max()=}\"\n",
    "    f\"\\n{syn_stim.mean()=}, {syn_stim.std()=}\"\n",
    "    f\"\\n{syn_resp.mean()=}, {syn_resp.std()=}\"\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.imshow(syn_stim.cpu()[0].squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.imshow(crop_stim(syn_stim.cpu()[0]).squeeze().unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.imshow(syn_resp.cpu()[0].view(100, 100).squeeze(0).unsqueeze(-1), cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, config, verbose=True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    n_batches = len(dataloader)\n",
    "\n",
    "    ### run\n",
    "    for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "        ### data\n",
    "        stim = stim.to(config[\"device\"])\n",
    "        resp = resp.to(config[\"device\"])\n",
    "        \n",
    "        ### train\n",
    "        stim_pred, stim_pred_history = model.run_batch(\n",
    "            train=True,\n",
    "            stim=stim,\n",
    "            resp=resp,\n",
    "            n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "            x_hat_history_iters=None,\n",
    "        )\n",
    "\n",
    "        ### log\n",
    "        loss = config[\"decoder\"][\"model\"][\"stim_loss_fn\"](stim_pred, stim)\n",
    "        train_loss += loss.item()\n",
    "        if verbose and batch_idx % 100 == 0:\n",
    "            print(f\"Training progress: [{batch_idx}/{n_batches} ({100. * batch_idx / n_batches:.0f}%)]\"\n",
    "                  f\"  Loss: {loss.item():.6f}\")\n",
    "        batch_idx += 1\n",
    "\n",
    "    train_loss /= n_batches\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, dataloader, loss_fn, config):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_idx, (stim, resp) in enumerate(dataloader):\n",
    "        ### data\n",
    "        stim = stim.to(config[\"device\"])\n",
    "        resp = resp.to(config[\"device\"])\n",
    "        \n",
    "        stim_pred, stim_pred_history = model.run_batch(\n",
    "            train=False,\n",
    "            stim=None,\n",
    "            resp=resp,\n",
    "            n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "            x_hat_history_iters=None,\n",
    "        )\n",
    "        loss = loss_fn(stim_pred, stim)\n",
    "\n",
    "        ### log\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(config, dataloaders, use_data_names, only_v1_data_eval=True):\n",
    "    if only_v1_data_eval:\n",
    "        val_dataloader = dataloaders[\"v1_data\"][\"val\"]\n",
    "\n",
    "    ### get dataloaders to mix\n",
    "    dataloaders_to_mix = []\n",
    "    for data_name in use_data_names:\n",
    "        dataloaders_to_mix.append(dataloaders[data_name])\n",
    "\n",
    "    if len(dataloaders_to_mix) > 1:\n",
    "        train_dataloader = MixedBatchLoader(\n",
    "            dataloaders=[dl[\"train\"] for dl in dataloaders_to_mix],\n",
    "            mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "            device=config[\"device\"],\n",
    "        )\n",
    "        if not only_v1_data_eval:\n",
    "            val_dataloader = MixedBatchLoader(\n",
    "                dataloaders=[dl[\"val\"] for dl in dataloaders_to_mix],\n",
    "                mixing_strategy=config[\"data\"][\"mixing_strategy\"],\n",
    "                device=config[\"device\"],\n",
    "            )\n",
    "    elif len(dataloaders_to_mix) == 1:\n",
    "        train_dataloader = dataloaders_to_mix[0][\"train\"]\n",
    "        if not only_v1_data_eval:\n",
    "            val_dataloader = dataloaders_to_mix[0][\"val\"]\n",
    "    else:\n",
    "        raise ValueError(\"No data to train on.\")\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load encoder\n",
    "from data_orig import prepare_spiking_data_loaders\n",
    "from lurz2020.models.models import se2d_fullgaussian2d\n",
    "\n",
    "print(\"Loading encoder...\")\n",
    "\n",
    "### config only for the encoder\n",
    "spiking_data_loaders_config = {\n",
    "    \"train_path\": os.path.join(DATA_PATH, \"datasets\", \"train\"),\n",
    "    \"val_path\": os.path.join(DATA_PATH, \"datasets\", \"val\"),\n",
    "    \"test_path\": os.path.join(DATA_PATH, \"orig\", \"raw\", \"test.pickle\"),\n",
    "    \"image_size\": [50, 50],\n",
    "    \"crop\": False,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "encoder_config = {\n",
    "    \"init_mu_range\": 0.55,\n",
    "    \"init_sigma\": 0.4,\n",
    "    \"input_kern\": 19,\n",
    "    \"hidden_kern\": 17,\n",
    "    \"hidden_channels\": 32,\n",
    "    \"gamma_input\": 1.0,\n",
    "    \"gamma_readout\": 2.439,\n",
    "    \"grid_mean_predictor\": None,\n",
    "    \"layers\": 5\n",
    "}\n",
    "\n",
    "### encoder\n",
    "_dataloaders = prepare_spiking_data_loaders(**spiking_data_loaders_config)\n",
    "encoder = se2d_fullgaussian2d(\n",
    "    **encoder_config,\n",
    "    dataloaders=_dataloaders,\n",
    "    seed=2,\n",
    ").float()\n",
    "del _dataloaders\n",
    "\n",
    "### load pretrained core\n",
    "pretrained_core = torch.load(\n",
    "    os.path.join(DATA_PATH, \"models\", \"spiking_scratch_tunecore_68Y_model.pth\"),\n",
    "    map_location=config[\"device\"],\n",
    ")\n",
    "encoder.load_state_dict(pretrained_core, strict=True)\n",
    "encoder.to(config[\"device\"])\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss = SSIMLoss(\n",
    "    window=config[\"stim_crop_win\"],\n",
    "    log_loss=True,\n",
    "    inp_normalized=True,\n",
    "    inp_standardized=False,\n",
    ")\n",
    "val_loss_fn = lambda stim_pred, stim: ssim_loss(normalize(stim_pred), normalize(stim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"decoder\"] = {\n",
    "    \"model\": {\n",
    "        \"encoder\": encoder.float(),\n",
    "        \"resp_shape\": (10000,),\n",
    "        \"stim_shape\": (1, 50, 50),\n",
    "        \"in_shape\": (4, 50, 50),\n",
    "        \"resp_layers_cfg\": {\n",
    "            \"layers\": [\n",
    "                (\"fc\", 300),\n",
    "                (\"unflatten\", 1, (3, 10, 10)),\n",
    "                (\"deconv\", 128, 7, 2, 1),\n",
    "                (\"deconv\", 64, 5, 2, 1),\n",
    "                (\"deconv\", 1, 4, 1, 0),\n",
    "            ],\n",
    "            \"act_fn\": nn.ReLU(),\n",
    "            \"out_act_fn\": nn.Sigmoid(),\n",
    "            \"dropout\": 0.2,\n",
    "            \"batch_norm\": True,\n",
    "        },\n",
    "        \"reconstruction_init_method\": \"resp_layers\",\n",
    "        \"act_fn\": nn.ReLU(),\n",
    "        # \"stim_loss_fn\": MSELossWithCrop(config[\"stim_crop_win\"]),\n",
    "        \"stim_loss_fn\": SSIMLoss(\n",
    "            window=config[\"stim_crop_win\"],\n",
    "            log_loss=True,\n",
    "            inp_normalized=False,\n",
    "            inp_standardized=True,\n",
    "        ),\n",
    "        # \"stim_loss_fn\": lambda x_hat, x: 0.9 * ssim_loss(x_hat, x) + 0.1 * F.mse_loss(x_hat, x),\n",
    "        \"resp_loss_fn\": nn.MSELoss(),\n",
    "        \"opter_cls\": torch.optim.Adam,\n",
    "        \"opter_kwargs\": {\n",
    "            \"lr\": 0.0005,\n",
    "        },\n",
    "        \"unroll\": 1,\n",
    "        \"preproc_grad\": True,\n",
    "        \"device\": config[\"device\"],\n",
    "    },\n",
    "    \"n_epochs\": 120,\n",
    "    \"n_steps\": 1,\n",
    "    \"save_run\": True,\n",
    "}\n",
    "\n",
    "decoder = L2O_Decoder(**config[\"decoder\"][\"model\"]).to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare checkpointing\n",
    "if config[\"decoder\"][\"save_run\"]:\n",
    "    ### save config\n",
    "    run_name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    config[\"dir\"] = os.path.join(DATA_PATH, \"models\", run_name)\n",
    "    os.makedirs(config[\"dir\"], exist_ok=True)\n",
    "    with open(os.path.join(config[\"dir\"], \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=4, default=str)\n",
    "    os.makedirs(os.path.join(config[\"dir\"], \"samples\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(config[\"dir\"], \"ckpt\"), exist_ok=True)\n",
    "    make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "        config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Run name: {run_name}\\nRun dir: {config['dir']}\")\n",
    "else:\n",
    "    make_sample_path = lambda epoch, prefix: None\n",
    "    print(\"WARNING: Not saving the run and the config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load ckpt\n",
    "run_name = \"2023-09-24_18-49-50\"\n",
    "ckpt = torch.load(os.path.join(DATA_PATH, \"models\", run_name, \"ckpt\", \"decoder_40.pt\"))\n",
    "\n",
    "history = ckpt[\"history\"]\n",
    "config = ckpt[\"config\"]\n",
    "best = ckpt[\"best\"]\n",
    "\n",
    "decoder = L2O_Decoder(**config[\"decoder\"][\"model\"]).to(config[\"device\"])\n",
    "decoder.load_state_dict(ckpt[\"decoder\"])\n",
    "\n",
    "make_sample_path = lambda epoch, prefix: os.path.join(\n",
    "    config[\"dir\"], \"samples\", f\"{prefix}stim_comparison_{epoch}e.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### print model\n",
    "print(decoder.run_batch(\n",
    "    stim=stim.to(config[\"device\"]),\n",
    "    resp=resp.to(config[\"device\"]),\n",
    "    n_steps=1,\n",
    "    train=False,\n",
    "    x_hat_history_iters=None,\n",
    ")[0].shape)\n",
    "print(f\"Number of parameters: {count_parameters(decoder)}\")\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, save_to=None):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(history[\"train_loss\"], label=\"train\")\n",
    "    ax.plot(history[\"val_loss\"], label=\"val\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    if save_to:\n",
    "        fig.savefig(save_to)\n",
    "    ### save fig\n",
    "    if config[\"decoder\"][\"save_run\"]:\n",
    "        fig.savefig(os.path.join(config[\"dir\"], f\"losses_{epoch}.png\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = list(dataloaders.keys())\n",
    "print(f\"{data_names=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "best = {\"val_loss\": np.inf, \"epoch\": 0, \"model\": None}\n",
    "s, e = len(history[\"train_loss\"]), len(history[\"train_loss\"]) + config[\"decoder\"][\"n_epochs\"]\n",
    "for epoch in range(s, e):\n",
    "    print(f\"[{epoch + 1}/{e}]\")\n",
    "\n",
    "    ### train and val\n",
    "    train_dataloader, val_dataloader = get_dataloaders(\n",
    "        config=config,\n",
    "        dataloaders=dataloaders,\n",
    "        use_data_names=data_names,\n",
    "        only_v1_data_eval=config[\"only_v1_data_eval\"],\n",
    "    )\n",
    "    train_loss = train(\n",
    "        model=decoder,\n",
    "        dataloader=train_dataloader,\n",
    "        config=config,\n",
    "    )\n",
    "    val_loss = val(\n",
    "        model=decoder,\n",
    "        dataloader=val_dataloader,\n",
    "        # loss_fn=config[\"decoder\"][\"model\"][\"stim_loss_fn\"],\n",
    "        loss_fn=val_loss_fn,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    ### save best model\n",
    "    if val_loss < best[\"val_loss\"]:\n",
    "        best[\"val_loss\"] = val_loss\n",
    "        best[\"epoch\"] = epoch\n",
    "        best[\"model\"] = deepcopy(decoder.state_dict())\n",
    "\n",
    "    ### log\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    print(f\"{train_loss=:.4f}, {val_loss=:.4f}\")\n",
    "\n",
    "    ### plot sample reconstructions\n",
    "    stim_pred = decoder.run_batch(\n",
    "        stim=stim.to(config[\"device\"]),\n",
    "        resp=resp.to(config[\"device\"]),\n",
    "        n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "        train=False,\n",
    "        x_hat_history_iters=None,\n",
    "    )[0].detach()\n",
    "    if \"v1_data\" in config[\"data\"] and config[\"data\"][\"v1_data\"][\"crop\"] == False:\n",
    "        plot_comparison(target=crop_stim(stim[:8]).cpu(), pred=crop_stim(stim_pred[:8]).cpu(), save_to=make_sample_path(epoch, \"\"))\n",
    "    else:\n",
    "        plot_comparison(target=stim[:8].cpu(), pred=stim_pred[:8].cpu(), save_to=make_sample_path(epoch, \"no_crop_\"))\n",
    "\n",
    "    ### plot losses\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        plot_losses(history=history)\n",
    "\n",
    "        ### ckpt\n",
    "        if config[\"decoder\"][\"save_run\"]:\n",
    "            torch.save({\n",
    "                \"decoder\": decoder.state_dict(),\n",
    "                \"opter\": decoder.opter.state_dict(),\n",
    "                \"history\": history,\n",
    "                \"config\": config,\n",
    "                \"best\": best,\n",
    "            }, os.path.join(config[\"dir\"], \"ckpt\", f\"decoder_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(\n",
    "    model=decoder,\n",
    "    dataloader=val_dataloader,\n",
    "    loss_fn=config[\"decoder\"][\"model\"][\"stim_loss_fn\"],\n",
    "    # loss_fn=ssim_loss,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best val loss: {best['val_loss']:.4f} at epoch {best['epoch']}\")\n",
    "\n",
    "### save final ckpt\n",
    "if config[\"decoder\"][\"save_run\"]:\n",
    "    torch.save({\n",
    "        \"decoder\": decoder.state_dict(),\n",
    "        \"opter\": decoder.opter.state_dict(),\n",
    "        \"history\": history,\n",
    "        \"config\": config,\n",
    "        \"best\": best,\n",
    "    }, os.path.join(config[\"dir\"], f\"decoder.pt\"))\n",
    "\n",
    "### plot reconstructions of the final model\n",
    "decoder.load_state_dict(best[\"model\"])\n",
    "stim_pred_best = decoder.run_batch(\n",
    "    stim=stim.to(config[\"device\"]),\n",
    "    resp=resp.to(config[\"device\"]),\n",
    "    n_steps=config[\"decoder\"][\"n_steps\"],\n",
    "    train=False,\n",
    "    x_hat_history_iters=None,\n",
    ")[0].detach()\n",
    "plot_comparison(\n",
    "    target=crop_stim(stim[:8]).cpu(),\n",
    "    pred=crop_stim(stim_pred_best[:8]).cpu(),\n",
    "    save_to=os.path.join(config[\"dir\"], \"stim_comparison_best.png\")\n",
    ")\n",
    "\n",
    "### plot losses\n",
    "plot_losses(\n",
    "    history=history,\n",
    "    save_to=None if not config[\"decoder\"][\"save_run\"] else os.path.join(config[\"dir\"], f\"losses_final.png\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
