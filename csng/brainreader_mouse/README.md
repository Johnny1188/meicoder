## Brainreader mouse data
We do not have permission to redistribute the Brainreader data. Please contact the [Andreas Tolias Lab](https://toliaslab.org/contact/).

Place the obtained data (files named `<session>_<data_part>_images` and `<session>_<data_part>_resp`) in the main data folder `<DATA_PATH>`. Finaly, run the `notebooks/preprocess_data.ipynb` notebook to rearrange the data and to compute the data statistics. These steps will create a folder structure like this:
```
<DATA_PATH>/brainreader
|   ├── data
|   │   ├── 1
|   │   │   ├── stats
|   |   │   │   ├── responses_iqr.npy
|   |   │   │   ├── responses_median.npy
|   |   │   │   ├── responses_mean.npy
|   |   │   │   ├── responses_std.npy
|   |   │   │   ├── stimuli_mean.npy
|   |   │   │   └── stimuli_std.npy
|   │   │   ├── train  ## contains individual data points as .pickle files
|   │   │   ├── test
|   │   │   └── val
|   │   ├── 2  ## same structure as session 1
|   │   ├── ...
|   │   └── 22 ## same structure as session 1
```


## Generating synthetic data
The synthetic data is generated using the `generate_synthetic_data.py` script. It works by loading 144x256 images from a base dataset and splitting them into smaller patches of size `config["syn_data"]["patch_dataset"]["patch_shape"]`. These patches are then passed through a trained encoder to get the predicted neuronal responses. The patches together with the predicted responses are saved as a new synthetic dataset to a folder
```
<DATA_PATH>/
    synthetic_data_<config["syn_data"]["data_key_src"]>_<config["syn_data"]["data_part_src"]>/
    <config["syn_data"]["patch_dataset"]["data_key"]>/
    <config["syn_data"]["data_part_target"]>
```
(split into lines for readability).

After the configuration which is described further below is set, the synthetic data can be generated by running the following command:
```bash
python generate_synthetic_data.py
```

### Configuration
#### 1. Global setup
```python
### config setup
config = {
    "data": {
        "mixing_strategy": "sequential", # needed only with multiple base dataloaders
    },
    "device": os.environ["DEVICE"],
    "seed": 0,
    "encoder_path": os.path.join(DATA_PATH, "models", "encoder_ball.pt"),
}
```
Here, only the `encoder_path` should be modified if you want to use a different encoder model. Seed will have only an effect on the ordering of datapoints in the generated synthetic dataset.

#### 2. Base dataloader config
```python
### base dataloader config
config["data"]["brainreader_mouse"] = {
    "device": config["device"],
    "mixing_strategy": config["data"]["mixing_strategy"],
    "max_batches": None,
    "data_dir": os.path.join(DATA_PATH_BRAINREADER, "data"),
    "batch_size": 1,
    "sessions": list(range(1, 23)),
    "resize_stim_to": None, # keep original size 144x256
    "normalize_stim": True,
    "normalize_resp": False,
    "div_resp_by_std": True,
    "clamp_neg_resp": False,
    "additional_keys": None,
    "avg_test_resp": True,
}
```
Nothing needs to be changed here. This part of the configuration serves as a base dataloader for the synthetic data generation.

```python
### synthetic data config
config["syn_data"] = {
    "data_part_src": "train", # from base_dls
    "data_part_target": "train", # to which folder to save the synthetic data
    "data_key_src": "1", # from base_dls
    "max_samples": 50000, # None or int
    "save_stats": True,
    "patch_dataset": {
        "data_key": "1", # data key on which the encoder was trained
        "patch_shape": (36, 64),
        "overlap": (0, 0),
        "stim_transform": None,
        "resp_transform": None,
        "device": config["device"],
    },
    "patch_dataloader": {
        "batch_size": 4,
        "shuffle": False,
    },
}
```
This section specifies from which data set the synthetic data should be generated and where it should be saved. More specifically:
- `data_part_src` specifies from which part of the base dataloader the synthetic data should be generated.
- `data_part_target` specifies to which folder the synthetic data should be saved.
- `data_key_src` specifies which data key of the base data set should be used for the generation of the synthetic data.
- `max_samples` specifies how many samples should be generated (sample here refers to pairs of image patches and predicted responses).
- `save_stats` specifies whether the statistics of the generated synthetic data should be saved.
- `patch_dataset` specifies the configuration of the patch dataset used for the generation of the synthetic data.
    - `data_key` specifies the data key to select the correct modules by the encoder model, i.e. the responses will be generated by the encoder trained on this data key and therefore correspond to the subject with this data key. This does not need to be the same as `data_key_src`, but along with `data_key_src` are the most important parameters to set.
    - `patch_shape` specifies the shape of the patches that should be generated.
    - `overlap` specifies the overlap between the patches - the higher the overlap, the more patches will be generated.
    - `stim_transform` and `resp_transform` can be used to apply transformations to the stimuli and responses.
    - `device` specifies the device on which the patches should be generated.
- `patch_dataloader` specifies the configuration of the dataloader used for the generation of the synthetic data. This is does not need to be changed in any way.
    - `batch_size` specifies the batch size of the dataloader.
    - `shuffle` specifies whether the dataloader should shuffle the data.