{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH_BRAINREADER='/scratch/izar/sobotka/csng/brainreader'\n",
      "DATA_PATH_MINDEYE='/scratch/izar/sobotka/csng/mindeye'\n",
      "DATA_PATH_MINDEYE_CACHE='/scratch/izar/sobotka/csng/mindeye/cache'\n",
      "Tue Feb 18 19:31:03 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   31C    P0              24W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "import sgm\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import pkgs.MindEyeV2.src.utils as utils\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from csng.data import get_dataloaders\n",
    "from csng.utils.mix import seed_all\n",
    "\n",
    "DATA_PATH_BRAINREADER = os.path.join(os.environ[\"DATA_PATH\"], \"brainreader\")\n",
    "DATA_PATH_MINDEYE = os.path.join(os.environ[\"DATA_PATH\"], \"mindeye\")\n",
    "DATA_PATH_MINDEYE_CACHE = os.path.join(DATA_PATH_MINDEYE, \"cache\")\n",
    "print(f\"{DATA_PATH_BRAINREADER=}\\n{DATA_PATH_MINDEYE=}\\n{DATA_PATH_MINDEYE_CACHE=}\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c0e47d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"device\": \"cuda\",\n",
    "    \"data_type\": torch.float16,\n",
    "    \"seed\": 0,\n",
    "    \"data\": {\"mixing_strategy\": \"parallel_min\", \"max_training_batches\": None},\n",
    "    \"wandb\": {\"project\": os.environ[\"WANDB_PROJECT\"], \"group\": \"mindeye\"},\n",
    "}\n",
    "\n",
    "### setup data config\n",
    "cfg[\"data\"][\"brainreader_mouse\"] = {\n",
    "    \"device\": cfg[\"device\"],\n",
    "    \"mixing_strategy\": cfg[\"data\"][\"mixing_strategy\"],\n",
    "    \"max_batches\": None,\n",
    "    \"data_dir\": os.path.join(DATA_PATH_BRAINREADER, \"data\"),\n",
    "    \"batch_size\": 2,\n",
    "    # \"sessions\": (subj_list := list(range(1, 23))),\n",
    "    \"sessions\": (subj_list := [1,2,3,4,5,6]),\n",
    "    \"resize_stim_to\": (36, 64),\n",
    "    \"normalize_stim\": True,\n",
    "    \"normalize_resp\": True,\n",
    "    \"div_resp_by_std\": True,\n",
    "    \"clamp_neg_resp\": False,\n",
    "    \"additional_keys\": None,\n",
    "    \"avg_test_resp\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "\n",
    "### setup model config\n",
    "cfg[\"model\"] = {\n",
    "    \"model_name\": (model_name := \"csng_18-02-25_19-31\"),\n",
    "    \"cache_dir\": DATA_PATH_MINDEYE_CACHE,\n",
    "    \"data_path\": DATA_PATH_BRAINREADER,\n",
    "    \"outdir\": f'{DATA_PATH_MINDEYE}/train_logs/{model_name}',\n",
    "    \"evalsdir\": f'{DATA_PATH_MINDEYE}/evals/{model_name}',\n",
    "    \"ckpt_saving\": True,\n",
    "    \"ckpt_interval\": 1,\n",
    "\n",
    "    # \"subj_list\": [6], # list(range(1, 23))\n",
    "    # \"num_voxels_list\": [8587],\n",
    "    # \"num_voxels\": {\n",
    "    #     f'subj06': 8587,\n",
    "    # },\n",
    "    \n",
    "    \"subj_list\": subj_list,\n",
    "    \"num_voxels_list\": (num_voxels_list := [dset.n_neurons for dset in get_dataloaders(config=cfg)[0][\"train\"][\"brainreader_mouse\"].datasets]),\n",
    "    \"num_voxels\": {\n",
    "        f\"subj{subj:02d}\": num_voxels\n",
    "        for subj, num_voxels in zip(subj_list, num_voxels_list)\n",
    "    },\n",
    "    \"hidden_dim\": 768,\n",
    "    \"n_blocks\": 4,\n",
    "    \"clip_scale\": 1.,\n",
    "    \"use_prior\": True,\n",
    "    \"prior_scale\": 30,\n",
    "    \"num_epochs\": 150,\n",
    "    \"num_iterations_per_epoch\": 500,\n",
    "    # \"mixup_pct\": 0.33,\n",
    "    \"mixup_pct\": 0.,\n",
    "    \"blurry_recon\": True,\n",
    "    \"blur_scale\": 0.54,\n",
    "    \"use_image_aug\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c63fc3e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/izar/sobotka/csng/mindeye/train_logs/csng_18-02-25_19-31\n"
     ]
    }
   ],
   "source": [
    "### create dirs\n",
    "os.makedirs(cfg[\"model\"][\"outdir\"], exist_ok=True)\n",
    "os.makedirs(cfg[\"model\"][\"cache_dir\"], exist_ok=True)\n",
    "print(cfg[\"model\"][\"outdir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e3feac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subj01': <torch.utils.data.dataloader.DataLoader at 0x7f3f4fb936d0>,\n",
       " 'subj02': <torch.utils.data.dataloader.DataLoader at 0x7f3f4ebcd7d0>,\n",
       " 'subj03': <torch.utils.data.dataloader.DataLoader at 0x7f3f4ebce610>,\n",
       " 'subj04': <torch.utils.data.dataloader.DataLoader at 0x7f3f4ebcf510>,\n",
       " 'subj05': <torch.utils.data.dataloader.DataLoader at 0x7f3f4ebc6610>,\n",
       " 'subj06': <torch.utils.data.dataloader.DataLoader at 0x7f3f4ebbab50>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n"
     ]
    }
   ],
   "source": [
    "seed_all(cfg[\"seed\"])\n",
    "dls, _ = get_dataloaders(config=cfg)\n",
    "train_dl, val_dl, test_dl = dls[\"train\"][\"brainreader_mouse\"], dls[\"val\"][\"brainreader_mouse\"], dls[\"test\"][\"brainreader_mouse\"]\n",
    "train_dls = {subj_name: dl for subj_name, dl in zip(cfg[\"model\"][\"num_voxels\"].keys(), train_dl.dataloaders)}\n",
    "display(train_dls)\n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "cfg[\"model\"][\"num_samples_per_epoch\"] = sum(len(dl) * dl.batch_size for dl in train_dls.values())\n",
    "cfg[\"model\"][\"num_iterations_per_epoch\"] = cfg[\"model\"][\"num_samples_per_epoch\"] // (cfg[\"data\"][\"brainreader_mouse\"][\"batch_size\"] * len(cfg[\"model\"][\"subj_list\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenOpenCLIPImageEmbedder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (patch_dropout): Identity()\n",
       "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-47): 48 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_1): Identity()\n",
       "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 1280)\n",
       "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg[\"model\"][\"clip_img_embedder_arch\"] = \"ViT-bigG-14\"\n",
    "cfg[\"model\"][\"clip_img_embedder_version\"] = \"laion2b_s39b_b160k\"\n",
    "cfg[\"model\"][\"clip_seq_dim\"] = 256\n",
    "cfg[\"model\"][\"clip_emb_dim\"] = 1664\n",
    "\n",
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=cfg[\"model\"][\"clip_img_embedder_arch\"],\n",
    "    version=cfg[\"model\"][\"clip_img_embedder_version\"],\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    "    cache_dir=cfg[\"model\"][\"cache_dir\"],\n",
    ")\n",
    "clip_img_embedder.to(cfg[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "param counts:\n",
      "349,687,808 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "if cfg[\"model\"][\"blurry_recon\"]:\n",
    "    ### SD VAE\n",
    "    from diffusers import AutoencoderKL\n",
    "    cfg[\"model\"][\"autoenc\"] = {\n",
    "        \"down_block_types\": ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        \"up_block_types\": ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        \"block_out_channels\": [128, 256, 512, 512],\n",
    "        \"layers_per_block\": 2,\n",
    "        \"sample_size\": 256,\n",
    "    }\n",
    "    autoenc = AutoencoderKL(**cfg[\"model\"][\"autoenc\"])\n",
    "    autoenc.load_state_dict(torch.load(f'{cfg[\"model\"][\"cache_dir\"]}/sd_image_var_autoenc.pth'))\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(cfg[\"device\"])\n",
    "    utils.count_params(autoenc)\n",
    "\n",
    "    ### VICRegL ConvNext-XL\n",
    "    from pkgs.MindEyeV2.src.autoencoder.convnext import ConvnextXL\n",
    "    cnx = ConvnextXL(f'{cfg[\"model\"][\"cache_dir\"]}/convnext_xlarge_alpha0.75_fullckpt.pth')\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(cfg[\"device\"])\n",
    "    utils.count_params(cnx)\n",
    "\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(cfg[\"device\"]).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(cfg[\"device\"]).reshape(1,3,1,1)\n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "37,705,728 total\n",
      "37,705,728 trainable\n",
      "param counts:\n",
      "37,705,728 total\n",
      "37,705,728 trainable\n",
      "torch.Size([2, 1, 9395]) torch.Size([2, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "### init model\n",
    "model = MindEyeModule()\n",
    "model.ridge = RidgeRegression(cfg[\"model\"][\"num_voxels_list\"], out_features=cfg[\"model\"][\"hidden_dim\"])\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,1,cfg[\"model\"][\"num_voxels_list\"][0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "345,356,284 total\n",
      "345,356,284 trainable\n",
      "param counts:\n",
      "383,062,012 total\n",
      "383,062,012 trainable\n",
      "b.shape torch.Size([2, 1, 768])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([2, 4, 28, 28]) torch.Size([2, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "from pkgs.MindEyeV2.src.models import BrainNetwork\n",
    "\n",
    "### backbone\n",
    "cfg[\"model\"][\"brainnetwork\"] = {\n",
    "    \"h\": cfg[\"model\"][\"hidden_dim\"],\n",
    "    \"in_dim\": cfg[\"model\"][\"hidden_dim\"],\n",
    "    \"seq_len\": 1,\n",
    "    \"n_blocks\": cfg[\"model\"][\"n_blocks\"],\n",
    "    \"clip_size\": cfg[\"model\"][\"clip_emb_dim\"],\n",
    "    \"out_dim\": cfg[\"model\"][\"clip_emb_dim\"] * cfg[\"model\"][\"clip_seq_dim\"],\n",
    "    \"blurry_recon\": cfg[\"model\"][\"blurry_recon\"],\n",
    "    \"clip_scale\": cfg[\"model\"][\"clip_scale\"],\n",
    "}\n",
    "model.backbone = BrainNetwork(**cfg[\"model\"][\"brainnetwork\"])\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,cfg[\"model\"][\"hidden_dim\"]))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "642,927,228 total\n",
      "642,927,212 trainable\n"
     ]
    }
   ],
   "source": [
    "if cfg[\"model\"][\"use_prior\"]:\n",
    "    from pkgs.MindEyeV2.src.models import *\n",
    "\n",
    "    ### setup diffusion prior network\n",
    "    cfg[\"model\"][\"out_dim\"] = cfg[\"model\"][\"clip_emb_dim\"]\n",
    "    cfg[\"model\"][\"depth\"] = 6\n",
    "    cfg[\"model\"][\"dim_head\"] = 52\n",
    "    cfg[\"model\"][\"heads\"] = cfg[\"model\"][\"clip_emb_dim\"] // cfg[\"model\"][\"dim_head\"]\n",
    "    cfg[\"model\"][\"timesteps\"] = 100\n",
    "    cfg[\"model\"][\"prior_network\"] = {\n",
    "        \"dim\": cfg[\"model\"][\"out_dim\"],\n",
    "        \"depth\": cfg[\"model\"][\"depth\"],\n",
    "        \"dim_head\": cfg[\"model\"][\"dim_head\"],\n",
    "        \"heads\": cfg[\"model\"][\"heads\"],\n",
    "        \"causal\": False,\n",
    "        \"num_tokens\": cfg[\"model\"][\"clip_seq_dim\"],\n",
    "        \"learned_query_mode\": \"pos_emb\",\n",
    "    }\n",
    "    cfg[\"model\"][\"brain_diffusion_prior\"] = {\n",
    "        \"image_embed_dim\": cfg[\"model\"][\"out_dim\"],\n",
    "        \"condition_on_text_encodings\": False,\n",
    "        \"timesteps\": cfg[\"model\"][\"timesteps\"],\n",
    "        \"cond_drop_prob\": 0.2,\n",
    "        \"image_embed_scale\": None,\n",
    "    }\n",
    "\n",
    "    prior_network = PriorNetwork(**cfg[\"model\"][\"prior_network\"])\n",
    "    model.diffusion_prior = BrainDiffusionPrior(net=prior_network, **cfg[\"model\"][\"brain_diffusion_prior\"])\n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 337500\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "642,927,228 total\n",
      "642,927,212 trainable\n"
     ]
    }
   ],
   "source": [
    "cfg[\"model\"][\"optimization\"] = {\n",
    "    \"no_decay\": ['bias', 'LayerNorm.bias', 'LayerNorm.weight'],\n",
    "    \"max_lr\": 3e-4,\n",
    "    \"lr_scheduler_type\": 'cycle',\n",
    "}\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in cfg[\"model\"][\"optimization\"][\"no_decay\"])], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in cfg[\"model\"][\"optimization\"][\"no_decay\"])], 'weight_decay': 0.0},\n",
    "]\n",
    "if cfg[\"model\"][\"use_prior\"]:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in cfg[\"model\"][\"optimization\"][\"no_decay\"])], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in cfg[\"model\"][\"optimization\"][\"no_decay\"])], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=cfg[\"model\"][\"optimization\"][\"max_lr\"])\n",
    "\n",
    "if cfg[\"model\"][\"optimization\"][\"lr_scheduler_type\"] == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=cfg[\"model\"][\"num_epochs\"]*cfg[\"model\"][\"num_iterations_per_epoch\"],\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif cfg[\"model\"][\"optimization\"][\"lr_scheduler_type\"] == 'cycle':\n",
    "    cfg[\"model\"][\"optimization\"][\"total_steps\"] = int(np.floor(cfg[\"model\"][\"num_epochs\"]*cfg[\"model\"][\"num_iterations_per_epoch\"]))\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=cfg[\"model\"][\"optimization\"][\"max_lr\"],\n",
    "        total_steps=cfg[\"model\"][\"optimization\"][\"total_steps\"],\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/cfg[\"model\"][\"num_epochs\"],\n",
    "    )\n",
    "    print(\"total_steps\", cfg[\"model\"][\"optimization\"][\"total_steps\"])\n",
    "\n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = cfg[\"model\"][\"outdir\"]+f'/{tag}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        'test_losses': test_losses,\n",
    "        'lrs': lrs,\n",
    "        \"cfg\": cfg,\n",
    "    }, ckpt_path)\n",
    "    print(f\"\\n---saved {cfg['model']['outdir']}/{tag} ckpt!---\\n\")\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab915f8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule(\n",
       "  (ridge): RidgeRegression(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=9395, out_features=768, bias=True)\n",
       "      (1): Linear(in_features=6721, out_features=768, bias=True)\n",
       "      (2): Linear(in_features=6864, out_features=768, bias=True)\n",
       "      (3): Linear(in_features=8784, out_features=768, bias=True)\n",
       "      (4): Linear(in_features=8739, out_features=768, bias=True)\n",
       "      (5): Linear(in_features=8587, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone): BrainNetwork(\n",
       "    (mixer_blocks1): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mixer_blocks2): ModuleList(\n",
       "      (0-3): 4 x Sequential(\n",
       "        (0): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.15, inplace=False)\n",
       "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (backbone_linear): Linear(in_features=768, out_features=425984, bias=True)\n",
       "    (clip_proj): Sequential(\n",
       "      (0): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      (3): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): GELU(approximate='none')\n",
       "      (5): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "      (6): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (7): GELU(approximate='none')\n",
       "      (8): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "    )\n",
       "    (blin1): Linear(in_features=768, out_features=3136, bias=True)\n",
       "    (bdropout): Dropout(p=0.3, inplace=False)\n",
       "    (bnorm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "    (bupsampler): Decoder(\n",
       "      (conv_in): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): LoRACompatibleConv(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): LoRACompatibleConv(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (conv1): LoRACompatibleConv(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): LoRACompatibleConv(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "              (conv1): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Attention(\n",
       "            (group_norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (to_q): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
       "            (to_k): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
       "            (to_v): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (b_maps_projector): Sequential(\n",
       "      (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (diffusion_prior): BrainDiffusionPrior(\n",
       "    (noise_scheduler): NoiseScheduler()\n",
       "    (net): PriorNetwork(\n",
       "      (to_time_embeds): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SinusoidalPosEmb()\n",
       "          (1): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=1664, out_features=3328, bias=True)\n",
       "                (1): SiLU()\n",
       "                (2): Identity()\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Linear(in_features=3328, out_features=3328, bias=True)\n",
       "                (1): SiLU()\n",
       "                (2): Identity()\n",
       "              )\n",
       "              (2): Linear(in_features=3328, out_features=1664, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Rearrange('b (n d) -> b n d', n=1)\n",
       "      )\n",
       "      (causal_transformer): FlaggedCausalTransformer(\n",
       "        (init_norm): Identity()\n",
       "        (rel_pos_bias): RelPosBias(\n",
       "          (relative_attention_bias): Embedding(32, 32)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x ModuleList(\n",
       "            (0): Attention(\n",
       "              (norm): LayerNorm()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_q): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "              (to_kv): Linear(in_features=1664, out_features=104, bias=False)\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "                (1): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): LayerNorm()\n",
       "              (1): Linear(in_features=1664, out_features=13312, bias=False)\n",
       "              (2): SwiGLU()\n",
       "              (3): Identity()\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "              (5): Linear(in_features=6656, out_features=1664, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "        (project_out): Linear(in_features=1664, out_features=1664, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnny1188\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sobotka/decoding-brain-activity/pkgs/MindEyeV2/wandb/run-20250218_193204-csng_18-02-25_19-31</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johnny1188/MindEye/runs/csng_18-02-25_19-31' target=\"_blank\">csng_18-02-25_19-31</a></strong> to <a href='https://wandb.ai/johnny1188/MindEye' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johnny1188/MindEye' target=\"_blank\">https://wandb.ai/johnny1188/MindEye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johnny1188/MindEye/runs/csng_18-02-25_19-31' target=\"_blank\">https://wandb.ai/johnny1188/MindEye/runs/csng_18-02-25_19-31</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if cfg.get(\"wandb\", None) is not None: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_log = wandb.init(\n",
    "        id=cfg[\"model\"][\"model_name\"],\n",
    "        name=cfg[\"model\"][\"model_name\"],\n",
    "        config=cfg,\n",
    "        resume=\"allow\",\n",
    "        **cfg[\"wandb\"],\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdbfdec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "### img transform (1x36x64 -> 3x224x224)\n",
    "img_tform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), antialias=True),\n",
    "    transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "if cfg[\"model\"][\"use_image_aug\"]:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E0/150 I0/2250]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loss 50.86752700805664\n",
      "[E0/150 I50/2250]\n",
      "  Loss 30.910066604614258\n",
      "[E0/150 I100/2250]\n",
      "  Loss 29.46784782409668\n",
      "[E0/150 I150/2250]\n",
      "  Loss 28.48337745666504\n",
      "[E0/150 I200/2250]\n",
      "  Loss 27.61124038696289\n",
      "[E0/150 I250/2250]\n",
      "  Loss 25.840059280395508\n",
      "[E0/150 I300/2250]\n",
      "  Loss 25.68180274963379\n",
      "[E0/150 I350/2250]\n",
      "  Loss 24.41274070739746\n",
      "[E0/150 I400/2250]\n",
      "  Loss 25.181795120239258\n",
      "[E0/150 I450/2250]\n",
      "  Loss 23.108753204345703\n"
     ]
    }
   ],
   "source": [
    "### tracking\n",
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best = {\"loss\": 1e9, \"epoch\": 0}\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "### optimization\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "cfg[\"model\"][\"cos_anneal_start\"] = 0.004\n",
    "cfg[\"model\"][\"cos_anneal_end\"] = 0.0075\n",
    "soft_loss_temps = utils.cosine_anneal(\n",
    "    cfg[\"model\"][\"cos_anneal_start\"],\n",
    "    cfg[\"model\"][\"cos_anneal_end\"],\n",
    "    cfg[\"model\"][\"num_epochs\"] - int(cfg[\"model\"][\"mixup_pct\"] * cfg[\"model\"][\"num_epochs\"])\n",
    ")\n",
    "\n",
    "### run\n",
    "model.to(cfg[\"device\"])\n",
    "progress_bar = tqdm(range(epoch, cfg[\"model\"][\"num_epochs\"]), ncols=1200, disable=False)\n",
    "seed_all(cfg[\"seed\"])\n",
    "for epoch in progress_bar:\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    loss_prior_total = 0.\n",
    "    blurry_pixcorr = 0.\n",
    "\n",
    "    train_dl = get_dataloaders(config=cfg)[0][\"train\"][\"brainreader_mouse\"]\n",
    "\n",
    "    ### train\n",
    "    model.train()\n",
    "    for train_i, batch in enumerate(train_dl):\n",
    "        if train_i % 50 == 0:\n",
    "            print(f\"[E{epoch}/{cfg['model']['num_epochs']} I{train_i}/{cfg['model']['num_iterations_per_epoch']}]\")\n",
    "        with torch.cuda.amp.autocast(dtype=cfg[\"data_type\"]):\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.\n",
    "\n",
    "            ### select data\n",
    "            voxel_list = [dp[\"resp\"].unsqueeze(1).to(cfg[\"device\"]) for dp in batch]  # (B, 1, num_voxels = num_of_neurons)\n",
    "            image = img_tform(torch.cat([dp[\"stim\"] for dp in batch], dim=0).to(cfg[\"device\"]))  # (B, 3, 224, 224)\n",
    "\n",
    "            ### augment image\n",
    "            if cfg[\"model\"][\"use_image_aug\"]: \n",
    "                image = img_augment(image)\n",
    "\n",
    "            if epoch < int(cfg[\"model\"][\"mixup_pct\"] * cfg[\"model\"][\"num_epochs\"]):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(cfg[\"device\"]) for s in cfg[\"model\"][\"subj_list\"]]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(cfg[\"device\"]) for s in cfg[\"model\"][\"subj_list\"]]\n",
    "                betas = torch.cat(betas_list, dim=0).to(cfg[\"data_type\"])\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(cfg[\"device\"]) for s in cfg[\"model\"][\"subj_list\"]]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "\n",
    "            ### map voxels to clip space\n",
    "            voxel_ridge = torch.cat([model.ridge(voxel_list[si], si) for si, s in enumerate(cfg[\"model\"][\"subj_list\"])], dim=0)\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "\n",
    "            ### map GT image to clip space\n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "            ### normalize clip embeddings\n",
    "            if cfg[\"model\"][\"clip_scale\"] > 0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if cfg[\"model\"][\"use_prior\"]:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                for i in range(3):\n",
    "                    if torch.isnan(loss_prior).any():\n",
    "                        print(f\"  Loss prior is NaN, trying again...\")\n",
    "                        loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= cfg[\"model\"][\"prior_scale\"]\n",
    "                loss += loss_prior\n",
    "\n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "\n",
    "            if cfg[\"model\"][\"clip_scale\"] > 0:\n",
    "                if epoch < int(cfg[\"model\"][\"mixup_pct\"] * cfg[\"model\"][\"num_epochs\"]):\n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch - int(cfg[\"model\"][\"mixup_pct\"] * cfg[\"model\"][\"num_epochs\"])]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp,\n",
    "                    )\n",
    "\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= cfg[\"model\"][\"clip_scale\"]\n",
    "                loss += loss_clip\n",
    "\n",
    "            if cfg[\"model\"][\"blurry_recon\"]:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "\n",
    "                # image_enc = autoenc.encode(2 * image - 1).latent_dist.mode() * 0.18215\n",
    "                image_enc = autoenc.encode(image).latent_dist.mode() * 0.18215  # already z-scored\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "\n",
    "                if epoch < int(cfg[\"model\"][\"mixup_pct\"] * cfg[\"model\"][\"num_epochs\"]):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "\n",
    "                # image_norm = (image - mean) / std\n",
    "                image_norm = image  # already z-scored\n",
    "                # print(f\"  Blurring Aug {image_norm.shape}\")\n",
    "                # image_aug = (blur_augs(image) - mean) / std\n",
    "                image_aug = blur_augs(image)  # already z-scored\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "\n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "\n",
    "                loss += (loss_blurry + 0.1 * cont_loss) * cfg[\"model\"][\"blur_scale\"] #/.18215\n",
    "\n",
    "            if cfg[\"model\"][\"clip_scale\"]>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            if cfg[\"model\"][\"blurry_recon\"]:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    # blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            if train_i % 50 == 0:\n",
    "                print(f\"  Loss {loss.item()}\")\n",
    "            utils.check_loss(loss)\n",
    "            # accelerator.backward(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            if cfg[\"model\"][\"optimization\"][\"lr_scheduler_type\"] is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    ### evaluate\n",
    "    print(\"\\n---Evaluating---\\n\")\n",
    "    model.eval()\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=cfg[\"data_type\"]):\n",
    "        n_minibatches = 0\n",
    "        test_loss=0.\n",
    "        test_loss_clip_total = 0.\n",
    "        test_loss_prior_total = 0.\n",
    "        test_blurry_pixcorr = 0.\n",
    "        test_fwd_percent_correct = 0.\n",
    "        test_bwd_percent_correct = 0.\n",
    "        eval_dl = get_dataloaders(config=cfg)[0][\"val\"][\"brainreader_mouse\"]\n",
    "        for batch in eval_dl:\n",
    "            voxel_list = [dp[\"resp\"].unsqueeze(1).to(cfg[\"device\"]) for dp in batch]  # (B, 1, num_voxels = num_of_neurons)\n",
    "            image = img_tform(torch.cat([dp[\"stim\"] for dp in batch], dim=0).to(cfg[\"device\"]))  # (B, 3, 224, 224)\n",
    "\n",
    "            voxel_ridge = torch.cat([model.ridge(voxel_list[si], si) for si, s in enumerate(cfg[\"model\"][\"subj_list\"])], dim=0)\n",
    "            backbone, clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "\n",
    "            clip_target = clip_img_embedder(image.float())\n",
    "            if cfg[\"model\"][\"clip_scale\"]>0:\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "\n",
    "            if cfg[\"model\"][\"use_prior\"]:\n",
    "                loss_prior, _ = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                test_loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= cfg[\"model\"][\"prior_scale\"]\n",
    "                test_loss += loss_prior\n",
    "\n",
    "            if cfg[\"model\"][\"clip_scale\"]>0:\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006,\n",
    "                )\n",
    "\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss_clip = loss_clip * cfg[\"model\"][\"clip_scale\"]\n",
    "                test_loss += loss_clip\n",
    "\n",
    "            if cfg[\"model\"][\"blurry_recon\"]:\n",
    "                image_enc_pred, _ = blurry_image_enc_\n",
    "                # blurry_recon_images = (autoenc.decode(image_enc_pred/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                blurry_recon_images = (autoenc.decode(image_enc_pred/0.18215).sample)\n",
    "                pixcorr = utils.pixcorr(image, blurry_recon_images)\n",
    "                test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            if cfg[\"model\"][\"clip_scale\"]>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "            n_minibatches += 1\n",
    "\n",
    "        test_loss /= n_minibatches\n",
    "        test_loss_clip_total /= n_minibatches\n",
    "        test_loss_prior_total /= n_minibatches\n",
    "        test_blurry_pixcorr /= n_minibatches\n",
    "        test_fwd_percent_correct /= n_minibatches\n",
    "        test_bwd_percent_correct /= n_minibatches\n",
    "\n",
    "        utils.check_loss(test_loss)                \n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        if test_loss.item() < best[\"loss\"]:\n",
    "            save_ckpt(f'best')\n",
    "            best[\"loss\"] = test_loss.item()\n",
    "            best[\"epoch\"] = epoch\n",
    "\n",
    "        logs = {\n",
    "            \"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "            \"test/loss\": test_loss.item(),\n",
    "            \"train/lr\": lrs[-1],\n",
    "            \"train/num_steps\": len(losses),\n",
    "            \"test/num_steps\": len(test_losses),\n",
    "            \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "            \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "            \"test/test_fwd_pct_correct\": test_fwd_percent_correct,\n",
    "            \"test/test_bwd_pct_correct\": test_bwd_percent_correct,\n",
    "            \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "            \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "            \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "            \"test/loss_clip_total\": test_loss_clip_total,\n",
    "            \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "            \"test/blurry_pixcorr\": test_blurry_pixcorr,\n",
    "            \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "            \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "            \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "            \"test/loss_prior\": test_loss_prior_total,\n",
    "        }\n",
    "\n",
    "        # if finished training, save jpg recons if they exist\n",
    "        if (epoch == cfg[\"model\"][\"num_epochs\"]-1) or (epoch % cfg[\"model\"][\"ckpt_interval\"] == 0):\n",
    "            if cfg[\"model\"][\"blurry_recon\"]:    \n",
    "                # image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                image_enc = autoenc.encode(image[:4]).latent_dist.mode() * 0.18215  # already z-scored\n",
    "                # transform blurry recon latents to images and plot it\n",
    "                fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                jj=-1\n",
    "                for j in [0,1,2,3]:\n",
    "                    jj+=1\n",
    "                    # axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    axes[jj].imshow(utils.torch_to_Image(autoenc.decode(image_enc[[j]]/0.18215).sample))\n",
    "                    axes[jj].axis('off')\n",
    "                    jj+=1\n",
    "                    # axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    axes[jj].imshow(utils.torch_to_Image(autoenc.decode(image_enc_pred[[j]]/0.18215).sample))\n",
    "                    axes[jj].axis('off')\n",
    "\n",
    "                if wandb_log:\n",
    "                    logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if wandb_log: wandb.log(logs)\n",
    "\n",
    "    # Save model checkpoint and reconstruct\n",
    "    # if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_ckpt(f'ep{epoch}')\n",
    "    save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    # accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Finished epoch {epoch} / {cfg['model']['num_epochs']}\")\n",
    "    \n",
    "    plt.imshow(blurry_recon_images[0].permute(1,2,0).cpu().detach().to(torch.float32))\n",
    "    plt.show()\n",
    "    plt.imshow(image[0].permute(1,2,0).cpu().detach().to(torch.float32))\n",
    "    plt.show()\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    plt.plot(test_losses)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "save_ckpt(f'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6533885",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "name": "",
   "version": ""
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
