{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18f6a3-cc4f-437e-9756-c99fc6a5fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "import dill\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "import sgm\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenCLIPEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import pkgs.MindEyeV2.src.utils\n",
    "from pkgs.MindEyeV2.src.models import *\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from csng.data import get_dataloaders\n",
    "from csng.utils.mix import seed_all\n",
    "from csng.utils.data import crop\n",
    "from csng.utils.data import crop, standardize, normalize\n",
    "from csng.data import get_dataloaders\n",
    "from csng.utils.comparison import plot_reconstructions\n",
    "\n",
    "DATA_PATH_BRAINREADER = os.path.join(os.environ[\"DATA_PATH\"], \"brainreader\")\n",
    "DATA_PATH_MINDEYE = os.path.join(os.environ[\"DATA_PATH\"], \"mindeye\")\n",
    "DATA_PATH_MINDEYE_CACHE = os.path.join(DATA_PATH_MINDEYE, \"cache\")\n",
    "print(f\"{DATA_PATH_BRAINREADER=}\\n{DATA_PATH_MINDEYE=}\\n{DATA_PATH_MINDEYE_CACHE=}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"csng_cat_v1__13-05-25_02-18\"\n",
    "data_path = DATA_PATH_BRAINREADER\n",
    "cache_dir = DATA_PATH_MINDEYE_CACHE\n",
    "evals_dir = f'{DATA_PATH_MINDEYE}/evals/{model_name}'\n",
    "assert os.path.exists(evals_dir)\n",
    "print(\"\\n\".join(os.listdir(evals_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "tag = \"best\"\n",
    "cfg = torch.load(f'{DATA_PATH_MINDEYE}/train_logs/{model_name}/{tag}.pth', map_location='cpu')['cfg']\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these files are downloadable from huggingface: https://huggingface.co/datasets/pscotti/mindeyev2/tree/main/evals\n",
    "# The others are obtained from running recon_inference.ipynb first with your desired model\n",
    "# subj_name = \"subj21067-10-18\"\n",
    "# subj_name = \"subj06\"\n",
    "subj_name = \"subjcat_v1\"\n",
    "data_tier = \"test\"\n",
    "\n",
    "# subj_list_idx = list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)\n",
    "all_images = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_images.pt\")\n",
    "all_recons = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_recons.pt\") # these are the unrefined MindEye2 recons!\n",
    "all_clipvoxels = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_clipvoxels.pt\")\n",
    "all_blurryrecons = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_blurryrecons.pt\")\n",
    "all_predcaptions = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_predcaptions.pt\")\n",
    "\n",
    "all_images = transforms.Resize((768,768))(all_images).float()\n",
    "if all_images.size(1):\n",
    "    all_images = all_images.repeat(1,3,1,1)\n",
    "all_recons = transforms.Resize((768,768))(all_recons).float()\n",
    "all_blurryrecons = transforms.Resize((768,768))(all_blurryrecons).float()\n",
    "\n",
    "print(model_name)\n",
    "print(\"\\n\".join([str(all_images), str(all_recons), str(all_clipvoxels), str(all_blurryrecons), str(len(all_predcaptions))]))\n",
    "seed_all(cfg[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdd667-0862-4561-b432-9fa7543df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load SDXL unCLIP model\n",
    "config = OmegaConf.load(\"src/generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "config = OmegaConf.load(\"src/generative_models/configs/inference/sd_xl_base.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "refiner_params = config[\"model\"][\"params\"]\n",
    "\n",
    "network_config = refiner_params[\"network_config\"]\n",
    "denoiser_config = refiner_params[\"denoiser_config\"]\n",
    "first_stage_config = refiner_params[\"first_stage_config\"]\n",
    "conditioner_config = refiner_params[\"conditioner_config\"]\n",
    "scale_factor = refiner_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = refiner_params[\"disable_first_stage_autocast\"]\n",
    "\n",
    "# base_ckpt_path = '/weka/robin/projects/stable-research/checkpoints/sd_xl_base_1.0.safetensors'\n",
    "base_ckpt_path = f'{cache_dir}/zavychromaxl_v30.safetensors'\n",
    "base_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config, # using the one defined by the unclip\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast,\n",
    "    ckpt_path=base_ckpt_path,\n",
    ")\n",
    "base_engine.eval().requires_grad_(False)\n",
    "base_engine.to(cfg[\"device\"])\n",
    "\n",
    "base_text_embedder1 = FrozenCLIPEmbedder(\n",
    "    layer=conditioner_config['params']['emb_models'][0]['params']['layer'],\n",
    "    layer_idx=conditioner_config['params']['emb_models'][0]['params']['layer_idx'],\n",
    ")\n",
    "base_text_embedder1.to(cfg[\"device\"])\n",
    "\n",
    "base_text_embedder2 = FrozenOpenCLIPEmbedder2(\n",
    "    arch=conditioner_config['params']['emb_models'][1]['params']['arch'],\n",
    "    version=conditioner_config['params']['emb_models'][1]['params']['version'],\n",
    "    freeze=conditioner_config['params']['emb_models'][1]['params']['freeze'],\n",
    "    layer=conditioner_config['params']['emb_models'][1]['params']['layer'],\n",
    "    always_return_pooled=conditioner_config['params']['emb_models'][1]['params']['always_return_pooled'],\n",
    "    legacy=conditioner_config['params']['emb_models'][1]['params']['legacy'],\n",
    ")\n",
    "base_text_embedder2.to(cfg[\"device\"])\n",
    "\n",
    "batch={\"txt\": \"\",\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(cfg[\"device\"]),\n",
    "      \"target_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 1024}\n",
    "out = base_engine.conditioner(batch)\n",
    "crossattn = out[\"crossattn\"].to(cfg[\"device\"])\n",
    "vector_suffix = out[\"vector\"][:,-1536:].to(cfg[\"device\"])\n",
    "print(\"crossattn\", crossattn.shape)\n",
    "print(\"vector_suffix\", vector_suffix.shape)\n",
    "print(\"---\")\n",
    "\n",
    "batch_uc={\"txt\": \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(cfg[\"device\"]),\n",
    "      \"target_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 1024}\n",
    "out = base_engine.conditioner(batch_uc)\n",
    "crossattn_uc = out[\"crossattn\"].to(cfg[\"device\"])\n",
    "vector_uc = out[\"vector\"].to(cfg[\"device\"])\n",
    "print(\"crossattn_uc\", crossattn_uc.shape)\n",
    "print(\"vector_uc\", vector_uc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f437d1-9b8e-4b13-85ad-d45062a5ce09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pkgs.MindEyeV2.src import utils\n",
    "if utils.is_interactive(): plotting=True\n",
    "\n",
    "num_samples = 1 # PS: I tried increasing this to 16 and picking highest cosine similarity like we did in MindEye1, it didnt seem to increase eval performance!\n",
    "img2img_timepoint = 13 # 9 # higher number means more reliance on prompt, less reliance on matching the conditioning image\n",
    "base_engine.sampler.guider.scale = 5 # 5 # cfg\n",
    "def denoiser(x, sigma, c):\n",
    "    return base_engine.denoiser(base_engine.model, x, sigma, c)\n",
    "\n",
    "if plotting or num_samples > 1:\n",
    "    clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "        arch=cfg[\"model\"][\"clip_img_embedder_arch\"],\n",
    "        version=cfg[\"model\"][\"clip_img_embedder_version\"],\n",
    "        output_tokens=True,\n",
    "        only_tokens=True,\n",
    "        cache_dir=cfg[\"model\"][\"cache_dir\"],\n",
    "    )\n",
    "    clip_img_embedder.to(cfg[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca49f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e1cbb-5836-48c2-87d8-3e493e950011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_enhancedrecons = None\n",
    "for img_idx in tqdm(range(len(all_recons))):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float32), base_engine.ema_scope():\n",
    "        base_engine.sampler.num_steps = 25\n",
    "\n",
    "        image = all_recons[[img_idx]]\n",
    "\n",
    "        if plotting:\n",
    "            print(\"blur pixcorr:\", utils.pixcorr(all_blurryrecons[[img_idx]].float(), all_images[[img_idx]].float()))\n",
    "            print(\"blur cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(all_blurryrecons[[img_idx]].float(),256).to(cfg[\"device\"])).flatten(1), \n",
    "                                                         clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(cfg[\"device\"])).flatten(1)))\n",
    "\n",
    "            print(\"recon pixcorr:\",utils.pixcorr(image,all_images[[img_idx]].float()))\n",
    "            print(\"recon cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(image,224).to(cfg[\"device\"])).flatten(1), \n",
    "                                                         clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(cfg[\"device\"])).flatten(1)))\n",
    "\n",
    "        image = image.to(cfg[\"device\"])\n",
    "        prompt = all_predcaptions[[img_idx]][0]\n",
    "        # prompt = \"\"\n",
    "        if plotting: \n",
    "            print(\"prompt:\",prompt)\n",
    "            # plt.imshow(transforms.ToPILImage()(all_blurryrecons[img_idx].float()))\n",
    "            plt.imshow(all_blurryrecons[img_idx].permute(1,2,0).float().cpu())\n",
    "            plt.title(\"blurry\")\n",
    "            plt.show()\n",
    "            # plt.imshow(transforms.ToPILImage()(all_recons[img_idx].float()))\n",
    "            plt.imshow(all_recons[img_idx].permute(1,2,0).float().cpu())\n",
    "            plt.title(\"recon\")\n",
    "            plt.show()\n",
    "            plt.imshow(all_images[img_idx].permute(1,2,0).cpu().numpy())\n",
    "            plt.title(\"GT\")\n",
    "            plt.show()\n",
    "\n",
    "        # z = torch.randn(num_samples,4,96,96).to(device)\n",
    "        assert image.shape[-1]==768\n",
    "        # z = base_engine.encode_first_stage(image*2-1).repeat(num_samples,1,1,1)\n",
    "        z = base_engine.encode_first_stage(image).repeat(num_samples,1,1,1) # already z-scored\n",
    "\n",
    "        openai_clip_text = base_text_embedder1(prompt)\n",
    "        clip_text_tokenized, clip_text_emb  = base_text_embedder2(prompt)\n",
    "        clip_text_emb = torch.hstack((clip_text_emb, vector_suffix))\n",
    "        clip_text_tokenized = torch.cat((openai_clip_text, clip_text_tokenized),dim=-1)\n",
    "        c = {\"crossattn\": clip_text_tokenized.repeat(num_samples,1,1), \"vector\": clip_text_emb.repeat(num_samples,1)}\n",
    "        uc = {\"crossattn\": crossattn_uc.repeat(num_samples,1,1), \"vector\": vector_uc.repeat(num_samples,1)}\n",
    "\n",
    "        noise = torch.randn_like(z)\n",
    "        sigmas = base_engine.sampler.discretization(base_engine.sampler.num_steps).to(cfg[\"device\"])\n",
    "        init_z = (z + noise * append_dims(sigmas[-img2img_timepoint], z.ndim)) / torch.sqrt(1.0 + sigmas[0] ** 2.0)\n",
    "        sigmas = sigmas[-img2img_timepoint:].repeat(num_samples,1)\n",
    "\n",
    "        base_engine.sampler.num_steps = sigmas.shape[-1] - 1\n",
    "        noised_z, _, _, _, c, uc = base_engine.sampler.prepare_sampling_loop(init_z, cond=c, uc=uc, \n",
    "                                                            num_steps=base_engine.sampler.num_steps)\n",
    "        for timestep in range(base_engine.sampler.num_steps):\n",
    "            noised_z = base_engine.sampler.sampler_step(sigmas[:,timestep],\n",
    "                                                        sigmas[:,timestep+1],\n",
    "                                                        denoiser, noised_z, cond=c, uc=uc, gamma=0)\n",
    "        samples_z_base = noised_z\n",
    "        samples_x = base_engine.decode_first_stage(samples_z_base)\n",
    "        assert (all_images.amin() < 0 or all_images.amax() > 1)\n",
    "        samples = samples_x\n",
    "        # samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "        # find best sample\n",
    "        if plotting==False and num_samples==1:\n",
    "            samples = samples[0]\n",
    "        else:\n",
    "            sample_cossim = nn.functional.cosine_similarity(clip_img_embedder(utils.resize(samples,224).to(cfg[\"device\"])).flatten(1), \n",
    "                                clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(cfg[\"device\"])).flatten(1))\n",
    "            which_sample = torch.argmax(sample_cossim)\n",
    "            best_cossim = torch.max(sample_cossim)\n",
    "\n",
    "            if plotting:\n",
    "                print(\"samples\", samples.shape)\n",
    "                for n in range(num_samples):\n",
    "                    # recon = transforms.ToPILImage()(samples[n])\n",
    "                    recon = samples[n].permute(1,2,0).float().cpu()\n",
    "                    plt.imshow(recon)\n",
    "                    plt.title(f\"enhanced recon {n}\")\n",
    "                    plt.show()\n",
    "                    if (n==which_sample).item(): print(\"CHOSEN ABOVE\")\n",
    "                    print(\"upsampled pixcorr:\",utils.pixcorr(samples[[n]].cpu(),all_images[[img_idx]].float()))\n",
    "                    print(\"upsampled cossim:\",nn.functional.cosine_similarity(clip_img_embedder(utils.resize(samples[[n]],224).to(cfg[\"device\"])).flatten(1), \n",
    "                                                         clip_img_embedder(utils.resize(all_images[[img_idx]].float(),224).to(cfg[\"device\"])).flatten(1)))\n",
    "            samples = samples[which_sample]\n",
    "\n",
    "        samples = samples.cpu()[None]\n",
    "        if all_enhancedrecons is None:\n",
    "            all_enhancedrecons = samples\n",
    "        else:\n",
    "            all_enhancedrecons = torch.vstack((all_enhancedrecons, samples))\n",
    "\n",
    "all_enhancedrecons = transforms.Resize((256,256))(all_enhancedrecons).float()\n",
    "print(\"all_enhancedrecons\", all_enhancedrecons.shape)\n",
    "torch.save(all_enhancedrecons,f\"{evals_dir}/{subj_name}_{data_tier}_all_enhancedrecons.pt\")\n",
    "print(f\"saved {evals_dir}/{subj_name}_{data_tier}_all_enhancedrecons.pt\")\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa35af",
   "metadata": {},
   "source": [
    "## Final plotting for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tier = \"test\"\n",
    "# subj_name = \"subj21067-10-18\"\n",
    "# subj_name = \"subj06\"\n",
    "subj_name = \"subjcat_v1\"\n",
    "loaded_all_images = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_images.pt\")\n",
    "loaded_all_blurryrecons = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_blurryrecons.pt\")\n",
    "loaded_all_enhancedrecons = torch.load(f\"{evals_dir}/{subj_name}_{data_tier}_all_enhancedrecons.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get image mean and std for inverting z-scoring\n",
    "_dls, _ = get_dataloaders(cfg)\n",
    "\n",
    "if \"data_name\" not in cfg:\n",
    "    if \"brainreader_mouse\" in cfg[\"data\"]:\n",
    "        cfg[\"data_name\"] = \"brainreader_mouse\"\n",
    "    elif \"mouse_v1\" in cfg[\"data\"]:\n",
    "        cfg[\"data_name\"] = \"mouse_v1\"\n",
    "    elif \"cat_v1\" in cfg[\"data\"]:\n",
    "        cfg[\"data_name\"] = \"cat_v1\"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "if cfg[\"data_name\"] == \"brainreader_mouse\":\n",
    "    zscore_trans = _dls[data_tier][cfg[\"data_name\"]].datasets[list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)].stim_transform.transforms[-1]\n",
    "    assert isinstance(zscore_trans, transforms.Normalize)\n",
    "    assert isinstance(zscore_trans.mean, float)\n",
    "\n",
    "    inv_zscore_trans = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=0.,\n",
    "            std=1/zscore_trans.std\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=-zscore_trans.mean,\n",
    "            std=1.\n",
    "        )\n",
    "    ])\n",
    "elif cfg[\"data_name\"] == \"mouse_v1\":\n",
    "    dset = _dls[\"train\"][cfg[\"data_name\"]].datasets[list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)]\n",
    "    assert dset.transforms[0]._inputs_mean > 50 # check that indeed the not z-scored data is 0-255\n",
    "    inv_zscore_trans = lambda x: dset.transforms[0]._itransforms[\"images\"](x) / 255\n",
    "elif cfg[\"data_name\"] == \"cat_v1\":\n",
    "    dset = _dls[\"train\"][cfg[\"data_name\"]].datasets[list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)]\n",
    "    mean, std = dset.stim_transform.transforms[-1].mean, dset.stim_transform.transforms[-1].std\n",
    "    inv_zscore_trans = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=0,\n",
    "            std=1/std\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=-mean,\n",
    "            std=1\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=0,\n",
    "            std=100\n",
    "        )\n",
    "    ])\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "plt.imshow(inv_zscore_trans(loaded_all_images[[0]])[0,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2865147",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data for visualization\n",
    "if cfg[\"data_name\"] == \"brainreader_mouse\":\n",
    "    target_shape = (36, 64)\n",
    "elif cfg[\"data_name\"] == \"mouse_v1\":\n",
    "    target_shape = (22, 36)\n",
    "elif cfg[\"data_name\"] == \"cat_v1\":\n",
    "    target_shape = (20, 20)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "resize_fn = transforms.Resize(target_shape, antialias=True)\n",
    "\n",
    "all_images = inv_zscore_trans(loaded_all_images).clip(0, 1)\n",
    "all_enhancedrecons = inv_zscore_trans(loaded_all_enhancedrecons).clip(0, 1)\n",
    "all_enhancedrecons_zscored = loaded_all_enhancedrecons\n",
    "all_blurryrecons = inv_zscore_trans(loaded_all_blurryrecons).clip(0, 1)\n",
    "all_blurryrecons_zscored = loaded_all_blurryrecons\n",
    "\n",
    "### as done in the original MindEye2 evals\n",
    "all_recons = all_enhancedrecons*.75 + all_blurryrecons*.25\n",
    "all_recons_zscored = all_enhancedrecons_zscored*.75 + all_blurryrecons_zscored*.25\n",
    "all_recons_orig = all_recons.clone()\n",
    "all_recons_zscored_orig = all_recons_zscored.clone()\n",
    "all_recons_zscored = resize_fn(all_recons_zscored)\n",
    "all_recons = resize_fn(all_recons)\n",
    "\n",
    "### channel matching\n",
    "if all_images.size(1) != all_enhancedrecons.size(1):\n",
    "    ### both in RGB\n",
    "    # all_images = all_images.repeat(1,3,1,1)\n",
    "\n",
    "    ### both in grayscale\n",
    "    all_recons = torch.tensor([\n",
    "        rgb2gray(all_recons[i].permute(1,2,0).numpy()) for i in range(len(all_recons))\n",
    "    ]).unsqueeze(1)\n",
    "    all_recons_zscored = torch.tensor([\n",
    "        rgb2gray(all_recons_zscored[i].permute(1,2,0).numpy()) for i in range(len(all_recons_zscored))\n",
    "    ]).unsqueeze(1)\n",
    "\n",
    "print(f\"{all_images=}\\n{all_recons=}\\n{all_recons_zscored=}\")\n",
    "plt.imshow(all_images[0].permute(1,2,0).numpy(), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(all_recons[0].squeeze(0), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(all_recons_zscored[0].squeeze(0), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot \n",
    "max_imgs = 40\n",
    "save_name = f\"reconstructions\"\n",
    "\n",
    "runs = {\n",
    "    \"MindEye2\": {\n",
    "        \"stim_pred_best\": [all_recons],\n",
    "    },\n",
    "}\n",
    "torch.save(all_recons, os.path.join(evals_dir, f\"{subj_name}_{data_tier}_{save_name}.pt\"))\n",
    "torch.save(all_recons_zscored, os.path.join(evals_dir, f\"{subj_name}_{data_tier}_{save_name}_zscored.pt\"))\n",
    "torch.save(all_recons_orig, os.path.join(evals_dir, f\"{subj_name}_{data_tier}_{save_name}_orig.pt\"))\n",
    "torch.save(all_recons_zscored_orig, os.path.join(evals_dir, f\"{subj_name}_{data_tier}_{save_name}_zscored_orig.pt\"))\n",
    "\n",
    "runs[\"MindEye2\"][\"stim_pred_best\"] = [runs[\"MindEye2\"][\"stim_pred_best\"][0][:max_imgs]]\n",
    "plot_reconstructions(\n",
    "    runs=runs,\n",
    "    stim=all_images[:max_imgs],\n",
    "    stim_label=\"Target\",\n",
    "    data_key=None,\n",
    "    manually_standardize=False,\n",
    "    crop_win=None,\n",
    "    save_to=os.path.join(evals_dir, f\"{save_name}_{data_tier}.pdf\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
