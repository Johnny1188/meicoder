{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH_BRAINREADER='/scratch/izar/sobotka/csng/brainreader'\n",
      "DATA_PATH_MINDEYE='/scratch/izar/sobotka/csng/mindeye'\n",
      "DATA_PATH_MINDEYE_CACHE='/scratch/izar/sobotka/csng/mindeye/cache'\n",
      "device: cuda\n",
      "Sun May  4 16:42:50 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  | 00000000:86:00.0 Off |                  Off |\n",
      "| N/A   36C    P0              26W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           On  | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   34C    P0              25W / 250W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "import sgm\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import pkgs.MindEyeV2.src.utils as utils\n",
    "from pkgs.MindEyeV2.src.models import *\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from csng.data import get_dataloaders\n",
    "from csng.utils.mix import seed_all\n",
    "from csng.utils.data import crop\n",
    "\n",
    "DATA_PATH_BRAINREADER = os.path.join(os.environ[\"DATA_PATH\"], \"brainreader\")\n",
    "DATA_PATH_MINDEYE = os.path.join(os.environ[\"DATA_PATH\"], \"mindeye\")\n",
    "DATA_PATH_MINDEYE_CACHE = os.path.join(DATA_PATH_MINDEYE, \"cache\")\n",
    "print(f\"{DATA_PATH_BRAINREADER=}\\n{DATA_PATH_MINDEYE=}\\n{DATA_PATH_MINDEYE_CACHE=}\")\n",
    "\n",
    "# accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "# device = accelerator.device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9be18c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9ee71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading csng_18-02-25_19-45:best ckpt---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'device': 'cuda',\n",
       " 'data_type': torch.float16,\n",
       " 'seed': 0,\n",
       " 'data': {'mixing_strategy': 'parallel_min',\n",
       "  'max_training_batches': None,\n",
       "  'brainreader_mouse': {'device': 'cuda',\n",
       "   'mixing_strategy': 'parallel_min',\n",
       "   'max_batches': None,\n",
       "   'data_dir': '/scratch/izar/sobotka/csng/brainreader/data',\n",
       "   'batch_size': 12,\n",
       "   'sessions': [6],\n",
       "   'resize_stim_to': (36, 64),\n",
       "   'normalize_stim': True,\n",
       "   'normalize_resp': True,\n",
       "   'div_resp_by_std': True,\n",
       "   'clamp_neg_resp': False,\n",
       "   'additional_keys': None,\n",
       "   'avg_test_resp': True,\n",
       "   'drop_last': True}},\n",
       " 'wandb': {'project': 'MindEye', 'group': 'mindeye'},\n",
       " 'model': {'model_name': 'csng_18-02-25_19-45',\n",
       "  'cache_dir': '/scratch/izar/sobotka/csng/mindeye/cache',\n",
       "  'data_path': '/scratch/izar/sobotka/csng/brainreader',\n",
       "  'outdir': '/scratch/izar/sobotka/csng/mindeye/train_logs/csng_18-02-25_19-45',\n",
       "  'evalsdir': '/scratch/izar/sobotka/csng/mindeye/evals/csng_18-02-25_19-45',\n",
       "  'ckpt_saving': True,\n",
       "  'ckpt_interval': 1,\n",
       "  'subj_list': [6],\n",
       "  'num_voxels_list': [8587],\n",
       "  'num_voxels': {'subj06': 8587},\n",
       "  'hidden_dim': 768,\n",
       "  'n_blocks': 4,\n",
       "  'clip_scale': 1.0,\n",
       "  'use_prior': True,\n",
       "  'prior_scale': 30,\n",
       "  'num_epochs': 150,\n",
       "  'num_iterations_per_epoch': 375,\n",
       "  'mixup_pct': 0.0,\n",
       "  'blurry_recon': True,\n",
       "  'blur_scale': 0.54,\n",
       "  'use_image_aug': False,\n",
       "  'num_samples_per_epoch': 4500,\n",
       "  'clip_img_embedder_arch': 'ViT-bigG-14',\n",
       "  'clip_img_embedder_version': 'laion2b_s39b_b160k',\n",
       "  'clip_seq_dim': 256,\n",
       "  'clip_emb_dim': 1664,\n",
       "  'autoenc': {'down_block_types': ['DownEncoderBlock2D',\n",
       "    'DownEncoderBlock2D',\n",
       "    'DownEncoderBlock2D',\n",
       "    'DownEncoderBlock2D'],\n",
       "   'up_block_types': ['UpDecoderBlock2D',\n",
       "    'UpDecoderBlock2D',\n",
       "    'UpDecoderBlock2D',\n",
       "    'UpDecoderBlock2D'],\n",
       "   'block_out_channels': [128, 256, 512, 512],\n",
       "   'layers_per_block': 2,\n",
       "   'sample_size': 256},\n",
       "  'brainnetwork': {'h': 768,\n",
       "   'in_dim': 768,\n",
       "   'seq_len': 1,\n",
       "   'n_blocks': 4,\n",
       "   'clip_size': 1664,\n",
       "   'out_dim': 425984,\n",
       "   'blurry_recon': True,\n",
       "   'clip_scale': 1.0},\n",
       "  'out_dim': 1664,\n",
       "  'depth': 6,\n",
       "  'dim_head': 52,\n",
       "  'heads': 32,\n",
       "  'timesteps': 100,\n",
       "  'prior_network': {'dim': 1664,\n",
       "   'depth': 6,\n",
       "   'dim_head': 52,\n",
       "   'heads': 32,\n",
       "   'causal': False,\n",
       "   'num_tokens': 256,\n",
       "   'learned_query_mode': 'pos_emb'},\n",
       "  'brain_diffusion_prior': {'image_embed_dim': 1664,\n",
       "   'condition_on_text_encodings': False,\n",
       "   'timesteps': 100,\n",
       "   'cond_drop_prob': 0.2,\n",
       "   'image_embed_scale': None},\n",
       "  'optimization': {'no_decay': ['bias', 'LayerNorm.bias', 'LayerNorm.weight'],\n",
       "   'max_lr': 0.0003,\n",
       "   'lr_scheduler_type': 'cycle',\n",
       "   'total_steps': 56250},\n",
       "  'cos_anneal_start': 0.004,\n",
       "  'cos_anneal_end': 0.0075}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model ckpt\n",
    "model_name = \"csng_18-02-25_19-45\"\n",
    "tag = \"best\"\n",
    "print(f\"\\n---loading {model_name}:{tag} ckpt---\\n\")\n",
    "checkpoint = torch.load(f\"{DATA_PATH_MINDEYE}/train_logs/{model_name}/{tag}.pth\", map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "cfg = checkpoint['cfg']\n",
    "evals_dir = cfg[\"model\"][\"evalsdir\"]\n",
    "outdir = cfg[\"model\"][\"outdir\"]\n",
    "assert os.path.exists(outdir)\n",
    "os.makedirs(evals_dir, exist_ok=True)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51805f86",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad027ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e595df733a9f48288983f9ef76745870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FrozenOpenCLIPImageEmbedder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisionTransformer(\n",
       "      (conv1): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (patch_dropout): Identity()\n",
       "      (ln_pre): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-47): 48 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_1): Identity()\n",
       "            (ln_2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (ls_2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 1280)\n",
       "    (ln_final): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=cfg[\"model\"][\"clip_img_embedder_arch\"],\n",
    "    version=cfg[\"model\"][\"clip_img_embedder_version\"],\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    "    cache_dir=cfg[\"model\"][\"cache_dir\"],\n",
    ")\n",
    "clip_img_embedder.to(cfg[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dae2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    }
   ],
   "source": [
    "if cfg[\"model\"][\"blurry_recon\"]:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(**cfg[\"model\"][\"autoenc\"])\n",
    "    ckpt = torch.load(f'{cfg[\"model\"][\"cache_dir\"]}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(cfg[\"device\"])\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051b2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    \n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "\n",
    "model = MindEyeModule()\n",
    "model.ridge = RidgeRegression(cfg[\"model\"][\"num_voxels_list\"], out_features=cfg[\"model\"][\"hidden_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7ad3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "6,595,584 total\n",
      "6,595,584 trainable\n",
      "param counts:\n",
      "345,356,284 total\n",
      "345,356,284 trainable\n",
      "param counts:\n",
      "351,951,868 total\n",
      "351,951,868 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "611,817,084 total\n",
      "611,817,068 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611817068"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pkgs.MindEyeV2.src.models import BrainNetwork\n",
    "\n",
    "model.backbone = BrainNetwork(**cfg[\"model\"][\"brainnetwork\"]) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "prior_network = PriorNetwork(**cfg[\"model\"][\"prior_network\"])\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(net=prior_network, **cfg[\"model\"][\"brain_diffusion_prior\"])\n",
    "model.to(cfg[\"device\"])\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from pkgs.MindEyeV2.src.modeling_git import GitForCausalLMClipEmb\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "# clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "cfg[\"model\"][\"clip_text_seq_dim\"] = 257\n",
    "cfg[\"model\"][\"clip_text_emb_dim\"] = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"model\"][\"clip_seq_dim\"], cfg[\"model\"][\"clip_text_seq_dim\"])\n",
    "        self.linear2 = nn.Linear(cfg[\"model\"][\"clip_emb_dim\"], cfg[\"model\"][\"clip_text_emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "clip_convert = CLIPConverter()\n",
    "state_dict = torch.load(f\"{cfg['model']['cache_dir']}/bigG_to_L_epoch8.pth\", map_location='cpu')['model_state_dict']\n",
    "clip_convert.load_state_dict(state_dict, strict=True)\n",
    "# clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_convert.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:pkgs.MindEyeV2.src.generative_models.sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# prep unCLIP\n",
    "from omegaconf import OmegaConf\n",
    "from copy import deepcopy\n",
    "\n",
    "config = OmegaConf.load(\"src/generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "cfg[\"model\"][\"unclip\"] = deepcopy(config)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config,\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast\n",
    ")\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(cfg[\"device\"])\n",
    "\n",
    "ckpt_path = f'{cfg[\"model\"][\"cache_dir\"]}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(cfg[\"device\"]), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(cfg[\"device\"])}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(cfg[\"device\"])\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ad484",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b1b8ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'crop_wins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_wins\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crop_wins'"
     ]
    }
   ],
   "source": [
    "cfg[\"crop_wins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_name='subj06'\n",
      "images=tensor[28, 1, 36, 64] n=64512 (0.2Mb) x∈[-1.575, 4.105] μ=-4.943e-08 σ=1.000 cuda:0\n",
      "voxels[subj_name]=tensor[28, 8587] n=240436 (0.9Mb) x∈[0.001, 6.323] μ=0.403 σ=0.406\n"
     ]
    }
   ],
   "source": [
    "### select a subject to test on\n",
    "save_to = os.path.join(evals_dir, f\"patterns_{subj_name}\")\n",
    "os.makedirs(save_to, exist_ok=True)\n",
    "subj_name = \"subj06\"\n",
    "cfg[\"data_name\"] = \"brainreader_mouse\"\n",
    "cfg[\"crop_wins\"] = {\n",
    "    \"mouse_v1\": (22, 36),\n",
    "    \"cat_v1\": (20, 20),\n",
    "    \"brainreader_mouse\": None,\n",
    "}\n",
    "data_tier = \"test\"\n",
    "subj_list_idx = list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)\n",
    "\n",
    "patterns_file = torch.load(f\"src/stim_resp_pairs_{subj_name}.pt\")\n",
    "images, voxels = patterns_file[\"stim\"], {subj_name: patterns_file[\"resp\"]}\n",
    "\n",
    "# voxels = {subj_name: []}\n",
    "# images = []\n",
    "# for b_i, batch in enumerate(test_dl):\n",
    "#     images.append(batch.images.cpu())\n",
    "#     voxels[subj_name].append(batch.responses.cpu())\n",
    "# images = torch.cat(images, dim=0)\n",
    "# voxels = {k: torch.cat(v, dim=0) for k,v in voxels.items()}\n",
    "print(f\"{subj_name=}\\n{images=}\\n{voxels[subj_name]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90aadc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: crop(x, cfg[\"crop_wins\"][cfg[\"data_name\"]])),\n",
    "    # transforms.Resize((224, 224), antialias=True),\n",
    "    # transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efab2f43928c4b2fbc552097765f9bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a black and white photo of a person.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "  4%|█                            | 1/28 [00:16<07:34, 16.83s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95cb5a01ac04b329afef1f1a1484b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large white and black object.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██                           | 2/28 [00:28<05:57, 13.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401924dc1d6544f19c880617555254be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a black and white photo of a person.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███                          | 3/28 [00:40<05:24, 12.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2537b566f7e4702888898d486020eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large black and white object.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████▏                        | 4/28 [00:52<04:57, 12.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c15375c43046368727210378404f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a view of a person.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████▏                       | 5/28 [01:03<04:36, 12.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca30c59c247451a9ce1d5e0314d4303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a person is standing up.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██████▏                      | 6/28 [01:14<04:19, 11.79s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d5d18bda7e49b5aafb3d0fe89c0f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a car is parked in front of a building.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████▎                     | 7/28 [01:26<04:11, 11.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d783b26212644718b3225eeab8a490e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a dog is standing in front of a camera.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████▎                    | 8/28 [01:39<04:01, 12.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c371b18346640fb8831b6f9c6e9d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man is standing in front of a building.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████▎                   | 9/28 [01:51<03:50, 12.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cbec8ea03c4d2aa6cdd649d0dda47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man is holding a piece of paper.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████                  | 10/28 [02:03<03:38, 12.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7cd2e7d4c84e8f813ee269368a27da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a car is parked in front of a building.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████                 | 11/28 [02:15<03:27, 12.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1384bb46cda7456e9db75fcf5af3bcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a group of people.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████                | 12/28 [02:27<03:09, 11.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54746f3e447347fbaab1c1947c6bc57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man is standing in front of a wall.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████               | 13/28 [02:39<02:59, 11.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df28b04a9254b6cb0c7f0d7c95b7ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a person is standing up.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████              | 14/28 [02:50<02:45, 11.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5e27747b7e434f9c79bf4cf3601054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large white and black truck.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████             | 15/28 [03:02<02:32, 11.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7d614a26d34a48b2749460532a98db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a person is standing up.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████            | 16/28 [03:13<02:19, 11.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b22f648ab34b988459ac7c4123c059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a man is standing in front of a wall.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████           | 17/28 [03:26<02:10, 11.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25007fbc69c441dfb38dc8096fe796b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a woman holding a cell phone.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████          | 18/28 [03:37<01:57, 11.77s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e8904bf9824279b1635c36d9657ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large black and white photo of a person.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|███████████████████         | 19/28 [03:50<01:47, 11.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbcaeb0e47a4adbb4ab62cf675618f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a woman holding a cell phone.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████        | 20/28 [04:01<01:34, 11.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7f6f6b410945299074dbdc4fcfa92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"a close up of a person's face\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████       | 21/28 [04:13<01:23, 11.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e647c2083049ce8c8dc069cdb44371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large group of people.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████      | 22/28 [04:25<01:10, 11.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c64572e77024bb2ad513e6a302780dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a car is parked in front of a truck.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████     | 23/28 [04:37<00:59, 11.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e842d83155949ac95f651c11dadd5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a group of people standing around.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████    | 24/28 [04:49<00:47, 11.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63094a42719e4144af68b67858e9810d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a group of people.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████   | 25/28 [05:00<00:34, 11.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c4d86983504953a3bf3800c8cf0659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a large group of people.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████  | 26/28 [05:11<00:23, 11.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1ee79b7e404981a3f0141d4d0c4727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a car is parked in front of a building.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|███████████████████████████ | 27/28 [05:23<00:11, 11.79s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f160d69ae78f4113851732bcb2b5b0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a group of people standing around each other.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 28/28 [05:36<00:00, 12.00s/it]\n",
      "/home/sobotka/miniconda3/envs/mindeye/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'crop_wins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# saving\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_recons\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 90\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(img_tform(images), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_tier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all_images.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(images, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_tier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all_images_before_transform.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(voxels,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_tier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_all_voxels.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.11/site-packages/torchvision/transforms/transforms.py:486\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambd(img)\n",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m img_tform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m----> 2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: crop(x, cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrop_wins\u001b[39m\u001b[38;5;124m\"\u001b[39m][cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]])),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# transforms.Resize((224, 224), antialias=True),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crop_wins'"
     ]
    }
   ],
   "source": [
    "# get all reconstructions\n",
    "model.to(cfg[\"device\"])\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "\n",
    "plotting = False\n",
    "\n",
    "seed_all(cfg[\"seed\"])\n",
    "with torch.no_grad():\n",
    "    for start_idx in tqdm(range(0,len(images),minibatch_size)):\n",
    "        voxel = voxels[subj_name][start_idx:start_idx + minibatch_size].unsqueeze(1).to(cfg[\"device\"])\n",
    "\n",
    "        # voxel_ridge = model.ridge(voxel, 0) # 0th index of subj_list\n",
    "        voxel_ridge = model.ridge(voxel, subj_list_idx)\n",
    "        torch.cuda.empty_cache()\n",
    "        backbone, clip_voxels, blurry_image_enc = model.backbone(voxel_ridge)\n",
    "        blurry_image_enc = blurry_image_enc[0]\n",
    "                \n",
    "        # Save retrieval submodule outputs\n",
    "        if all_clipvoxels is None:\n",
    "            all_clipvoxels = clip_voxels.cpu()\n",
    "        else:\n",
    "            all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "        \n",
    "        # Feed voxels through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)\n",
    "        \n",
    "        pred_caption_emb = clip_convert(prior_out.to(clip_convert.linear1.weight.device, clip_convert.linear1.weight.dtype))\n",
    "        generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(generated_caption)\n",
    "        \n",
    "        ### feed diffusion prior outputs through unCLIP\n",
    "        for i in range(len(voxel)):\n",
    "            assert images.amin() < 0 or images.amax() > 1\n",
    "            samples = utils.unclip_recon(\n",
    "                prior_out[[i]],\n",
    "                diffusion_engine,\n",
    "                vector_suffix,\n",
    "                num_samples=num_samples_per_image,\n",
    "                clamp=False, # to [0, 1]\n",
    "            )\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    # plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.imshow(samples[s].cpu().permute(1,2,0).to(torch.float32))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if cfg[\"model\"][\"blurry_recon\"]:\n",
    "            # blurred_image = (autoenc.decode(blurry_image_enc/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "            blurred_image = autoenc.decode(blurry_image_enc/0.18215).sample # already z-scored\n",
    "            \n",
    "            for i in range(len(voxel)):\n",
    "                im = torch.Tensor(blurred_image[i])\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im[None].cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im[None].cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    # plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.imshow(im.cpu().permute(1,2,0).to(torch.float32))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "# resize outputs before saving\n",
    "imsize = 256\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "if cfg[\"model\"][\"blurry_recon\"]: \n",
    "    all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "\n",
    "# saving\n",
    "print(all_recons.shape)\n",
    "torch.save(img_tform(images), f\"{save_to}/{subj_name}_{data_tier}_all_images.pt\")\n",
    "torch.save(images, f\"{save_to}/{subj_name}_{data_tier}_all_images_before_transform.pt\")\n",
    "torch.save(voxels,f\"{save_to}/{subj_name}_{data_tier}_all_voxels.pt\") \n",
    "if cfg[\"model\"][\"blurry_recon\"]:\n",
    "    torch.save(all_blurryrecons, f\"{save_to}/{subj_name}_{data_tier}_all_blurryrecons.pt\")\n",
    "torch.save(all_recons, f\"{save_to}/{subj_name}_{data_tier}_all_recons.pt\")\n",
    "torch.save(all_predcaptions, f\"{save_to}/{subj_name}_{data_tier}_all_predcaptions.pt\")\n",
    "torch.save(all_clipvoxels, f\"{save_to}/{subj_name}_{data_tier}_all_clipvoxels.pt\")\n",
    "torch.save(cfg, f\"{save_to}/cfg.pt\")\n",
    "print(f\"saved {cfg['model']['model_name']} outputs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
