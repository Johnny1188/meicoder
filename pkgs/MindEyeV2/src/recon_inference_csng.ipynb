{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "import sgm\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from pkgs.MindEyeV2.src.generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import pkgs.MindEyeV2.src.utils as utils\n",
    "from pkgs.MindEyeV2.src.models import *\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "from csng.data import get_dataloaders\n",
    "from csng.utils.mix import seed_all\n",
    "from csng.utils.data import crop\n",
    "\n",
    "DATA_PATH_BRAINREADER = os.path.join(os.environ[\"DATA_PATH\"], \"brainreader\")\n",
    "DATA_PATH_MINDEYE = os.path.join(os.environ[\"DATA_PATH\"], \"mindeye\")\n",
    "DATA_PATH_MINDEYE_CACHE = os.path.join(DATA_PATH_MINDEYE, \"cache\")\n",
    "print(f\"{DATA_PATH_BRAINREADER=}\\n{DATA_PATH_MINDEYE=}\\n{DATA_PATH_MINDEYE_CACHE=}\")\n",
    "\n",
    "# accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "# device = accelerator.device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9be18c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ee71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "model_name = \"csng_mouse_v1_all\"\n",
    "tag = \"best\"\n",
    "print(f\"\\n---loading {model_name}:{tag} ckpt---\\n\")\n",
    "checkpoint = torch.load(f\"{DATA_PATH_MINDEYE}/train_logs/{model_name}/{tag}.pth\", map_location='cpu')\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "cfg = checkpoint['cfg']\n",
    "evals_dir = cfg[\"model\"][\"evalsdir\"]\n",
    "outdir = cfg[\"model\"][\"outdir\"]\n",
    "assert os.path.exists(outdir)\n",
    "os.makedirs(evals_dir, exist_ok=True)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51805f86",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad027ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=cfg[\"model\"][\"clip_img_embedder_arch\"],\n",
    "    version=cfg[\"model\"][\"clip_img_embedder_version\"],\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    "    cache_dir=cfg[\"model\"][\"cache_dir\"],\n",
    ")\n",
    "clip_img_embedder.to(cfg[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dae2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"model\"][\"blurry_recon\"]:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL(**cfg[\"model\"][\"autoenc\"])\n",
    "    ckpt = torch.load(f'{cfg[\"model\"][\"cache_dir\"]}/sd_image_var_autoenc.pth')\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(cfg[\"device\"])\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    \n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "\n",
    "model = MindEyeModule()\n",
    "model.ridge = RidgeRegression(cfg[\"model\"][\"num_voxels_list\"], out_features=cfg[\"model\"][\"hidden_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ad3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkgs.MindEyeV2.src.models import BrainNetwork\n",
    "\n",
    "model.backbone = BrainNetwork(**cfg[\"model\"][\"brainnetwork\"]) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "prior_network = PriorNetwork(**cfg[\"model\"][\"prior_network\"])\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(net=prior_network, **cfg[\"model\"][\"brain_diffusion_prior\"])\n",
    "model.to(cfg[\"device\"])\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model ckpt\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup text caption networks\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from pkgs.MindEyeV2.src.modeling_git import GitForCausalLMClipEmb\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-large-coco\")\n",
    "clip_text_model = GitForCausalLMClipEmb.from_pretrained(\"microsoft/git-large-coco\")\n",
    "# clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "cfg[\"model\"][\"clip_text_seq_dim\"] = 257\n",
    "cfg[\"model\"][\"clip_text_emb_dim\"] = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(cfg[\"model\"][\"clip_seq_dim\"], cfg[\"model\"][\"clip_text_seq_dim\"])\n",
    "        self.linear2 = nn.Linear(cfg[\"model\"][\"clip_emb_dim\"], cfg[\"model\"][\"clip_text_emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "clip_convert = CLIPConverter()\n",
    "state_dict = torch.load(f\"{cfg['model']['cache_dir']}/bigG_to_L_epoch8.pth\", map_location='cpu')['model_state_dict']\n",
    "clip_convert.load_state_dict(state_dict, strict=True)\n",
    "# clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_convert.to(\"cpu\") # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep unCLIP\n",
    "from omegaconf import OmegaConf\n",
    "from copy import deepcopy\n",
    "\n",
    "config = OmegaConf.load(\"src/generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "cfg[\"model\"][\"unclip\"] = deepcopy(config)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(\n",
    "    network_config=network_config,\n",
    "    denoiser_config=denoiser_config,\n",
    "    first_stage_config=first_stage_config,\n",
    "    conditioner_config=conditioner_config,\n",
    "    sampler_config=sampler_config,\n",
    "    scale_factor=scale_factor,\n",
    "    disable_first_stage_autocast=disable_first_stage_autocast\n",
    ")\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(cfg[\"device\"])\n",
    "\n",
    "ckpt_path = f'{cfg[\"model\"][\"cache_dir\"]}/unclip6_epoch0_step110000.ckpt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(cfg[\"device\"]), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(cfg[\"device\"]) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(cfg[\"device\"])}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(cfg[\"device\"])\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### select a subject to test on\n",
    "subj_name = \"subj21067-10-18\"\n",
    "subj_list_idx = list(cfg[\"model\"][\"num_voxels\"].keys()).index(subj_name)\n",
    "\n",
    "seed_all(cfg[\"seed\"])\n",
    "dls, _ = get_dataloaders(config=cfg)\n",
    "test_dl = dls[\"test\"][cfg[\"data_name\"]].dataloaders[subj_list_idx]\n",
    "data_key = dls[\"test\"][cfg[\"data_name\"]].data_keys[subj_list_idx]\n",
    "assert (f'subj{data_key}' == subj_name or f'subj0{data_key}' == subj_name)\n",
    "\n",
    "voxels = {subj_name: []}\n",
    "images = []\n",
    "for b_i, batch in enumerate(test_dl):\n",
    "    images.append(batch.images.cpu())\n",
    "    voxels[subj_name].append(batch.responses.cpu())\n",
    "images = torch.cat(images, dim=0)\n",
    "voxels = {k: torch.cat(v, dim=0) for k,v in voxels.items()}\n",
    "print(f\"{subj_name=}\\n{images=}\\n{voxels[subj_name]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aadc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: crop(x, cfg[\"crop_wins\"][cfg[\"data_name\"]])),\n",
    "    # transforms.Resize((224, 224), antialias=True),\n",
    "    # transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all reconstructions\n",
    "model.to(cfg[\"device\"])\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_clipvoxels = None\n",
    "\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "\n",
    "if utils.is_interactive(): plotting=True\n",
    "\n",
    "seed_all(cfg[\"seed\"])\n",
    "# with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float32):\n",
    "with torch.no_grad():\n",
    "    # for batch in test_dl:\n",
    "    #     with torch.cuda.amp.autocast(dtype=cfg[\"data_type\"]):\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss = 0.\n",
    "\n",
    "    #         ### select data\n",
    "    #         voxel_list = [dp[\"resp\"].unsqueeze(1).to(cfg[\"device\"]) for dp in batch]  # (B, 1, num_voxels = num_of_neurons)\n",
    "    #         image = img_tform(torch.cat([dp[\"stim\"] for dp in batch], dim=0).to(cfg[\"device\"]))  # (B, 3, 224, 224)\n",
    "\n",
    "    for start_idx in tqdm(range(0,len(images),minibatch_size)):\n",
    "        voxel = voxels[subj_name][start_idx:start_idx + minibatch_size].unsqueeze(1).to(cfg[\"device\"])\n",
    "\n",
    "        # voxel_ridge = model.ridge(voxel, 0) # 0th index of subj_list\n",
    "        voxel_ridge = model.ridge(voxel, subj_list_idx)\n",
    "        torch.cuda.empty_cache()\n",
    "        backbone, clip_voxels, blurry_image_enc = model.backbone(voxel_ridge)\n",
    "        blurry_image_enc = blurry_image_enc[0]\n",
    "                \n",
    "        # Save retrieval submodule outputs\n",
    "        if all_clipvoxels is None:\n",
    "            all_clipvoxels = clip_voxels.cpu()\n",
    "        else:\n",
    "            all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))\n",
    "        \n",
    "        # Feed voxels through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)\n",
    "        \n",
    "        pred_caption_emb = clip_convert(prior_out.to(clip_convert.linear1.weight.device, clip_convert.linear1.weight.dtype))\n",
    "        generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(generated_caption)\n",
    "        \n",
    "        ### feed diffusion prior outputs through unCLIP\n",
    "        for i in range(len(voxel)):\n",
    "            assert images.amin() < 0 or images.amax() > 1\n",
    "            samples = utils.unclip_recon(\n",
    "                prior_out[[i]],\n",
    "                diffusion_engine,\n",
    "                vector_suffix,\n",
    "                num_samples=num_samples_per_image,\n",
    "                clamp=False, # to [0, 1]\n",
    "            )\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    # plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.imshow(samples[s].cpu().permute(1,2,0).to(torch.float32))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if cfg[\"model\"][\"blurry_recon\"]:\n",
    "            # blurred_image = (autoenc.decode(blurry_image_enc/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "            blurred_image = autoenc.decode(blurry_image_enc/0.18215).sample # already z-scored\n",
    "            \n",
    "            for i in range(len(voxel)):\n",
    "                im = torch.Tensor(blurred_image[i])\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im[None].cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im[None].cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    # plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.imshow(im.cpu().permute(1,2,0).to(torch.float32))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "# resize outputs before saving\n",
    "imsize = 256\n",
    "all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "if cfg[\"model\"][\"blurry_recon\"]: \n",
    "    all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    " \n",
    "# saving\n",
    "print(all_recons.shape)\n",
    "torch.save(img_tform(images), f\"{cfg['model']['evalsdir']}/{subj_name}_all_images.pt\")\n",
    "torch.save(images, f\"{cfg['model']['evalsdir']}/{subj_name}_all_images_before_transform.pt\")\n",
    "torch.save(voxels,f\"{cfg['model']['evalsdir']}/{subj_name}_all_voxels.pt\") \n",
    "if cfg[\"model\"][\"blurry_recon\"]:\n",
    "    torch.save(all_blurryrecons, f\"{cfg['model']['evalsdir']}/{subj_name}_all_blurryrecons.pt\")\n",
    "torch.save(all_recons, f\"{cfg['model']['evalsdir']}/{subj_name}_all_recons.pt\")\n",
    "torch.save(all_predcaptions, f\"{cfg['model']['evalsdir']}/{subj_name}_all_predcaptions.pt\")\n",
    "torch.save(all_clipvoxels, f\"{cfg['model']['evalsdir']}/{subj_name}_all_clipvoxels.pt\")\n",
    "torch.save(cfg, f\"{cfg['model']['evalsdir']}/cfg.pt\")\n",
    "print(f\"saved {cfg['model']['model_name']} outputs!\")\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "mindeye"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
